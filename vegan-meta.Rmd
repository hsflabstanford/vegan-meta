---
title: "Vegan meta-analysis "
author: "Seth Green & Benny Smith"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  word_document: default
editor_options:
  chunk_output_type: console
---

**libraries, functions, options, and data**

```{r setup, message=F}
#' libraries
library(metafor, quietly = T)
library(dplyr, warn.conflicts = F)
library(ggforestplot) # https://nightingalehealth.github.io/ggforestplot
library(ggplot2, warn.conflicts = F)
library(knitr)
library(purrr)
library(sessioninfo)
library(stringr)

#' functions
source('./functions/d_calc.R')
source('./functions/map_robust.R')
source('./functions/study_count.R')
source('./functions/sum_lm.R')
source('./functions/sum_tab.R')
source('./functions/var_d_calc.R')

#' options and reproducibility
options(scipen = 99)
set.seed(11111988)

#' count_and_robust helper function
#' necessary when you ` filter |> split` for some reason
  count_and_robust <- function(data) {
      bind_cols(study_count(data), map_robust(data)) |>
        kable('markdown')
   }
```

**load data, add more descriptive vars, and add d, var_d, and se_d**

```{r load_data}

dat <- read.csv('./data/vegan-meta.csv')  |>
  group_by(title) |>
  mutate(unique_paper_id = cur_group_id())  |> # create unique paper id
  ungroup() |> group_by(unique_paper_id, intervention_condition) |> 
  mutate(unique_study_id = cur_group_id()) |> ### create unique study id
  ungroup() |>
  mutate(
         decade = as.factor(
      case_when(
        year >= 2000 & year <= 2009 ~ "2000s",
        year >= 2010 & year <= 2019  ~ "2010s",
        year >= 2020 ~ "2020s")),
      self_report = as.factor(self_report),
      cluster_assigned = as.factor(cluster_assigned),
      d = mapply(
        FUN = d_calc,
        stat_type = eff_type,
        stat =  u_s_d,
        sample_sd = ctrl_sd,
        n_t = n_t_post,
        n_c = n_c_post),
      var_d = mapply(
        FUN = var_d_calc,
        d = d,
        n_c = n_c_post,
        n_t = n_t_post),
      se_d = sqrt(var_d))

```

### 3.3 descriptive stats
mean N, country table, and population

```{r descriptive_stats}
dat |> group_by(cluster_assigned) |> 
  summarise(pop_total = round(mean(n_c_post) + mean(n_t_post)),
            median_pop = round(median(n_c_post) + median(n_t_post)))
            
sort(table(dat$country))
table(dat$population)
```

# Meta-analysis

### 4.1 Overall effect

```{r avg_eff}
dat |> map_robust()
```

Sign test (how many studies were positive, negative, or neutral in their authors' own words)

```{r sign_test}
sign_table <- table(dat$neg_null_pos)
binom.test(sign_table[[3]], sum(sign_table[1:3]))
table(dat$neg_null_pos)
```

Fig 1: forest plot

```{r forest_plot}
dat |> 
  mutate(study_name = paste0(author, " ", year)) |> 
           arrange(var_d) |> ggforestplot::forestplot(study_name,
                         estimate = d, se = se_d, 
                         colour = theory, 
                         xlab = expression(paste("Glass's", " ", Delta)), 
                         ylab = "Study",
                         title = "Vegan meta forest plot") +
    geom_vline(xintercept = (dat |> map_robust())$Delta, 
                            lty = 'dashed') +  # color = '#00BFC4'
  theme(plot.title = element_text(hjust = 0.5))
```


### 4.2 publication bias
```{r pub_bias}
dat |> sum_lm()

dat |> ggplot(aes(d, se_d)) + 
  geom_point(aes(color = theory)) + 
  stat_smooth(method = 'lm', se = F, color = 'grey', 
              lty = 'dashed', linewidth = 1) + 
  theme_minimal() +
  ggtitle('relationship between effect size and standard erors')

#' split by published/unpublished (DOI = published)

dat |> 
  mutate(published_unpublished = 
    if_else(str_detect(dat$doi_or_url, "10\\."), T, F)) |> 
  sum_lm(d, published_unpublished)
  
dat |> split(~str_detect(dat$doi_or_url, "10\\.")) |> 
  imap(~count_and_robust(.x))

# funnel plot and eggers test
st_model <- rma(yi = d, vi = var_d, data = dat) # n/A for metafor::robust()
funnel(st_model)
regtest(st_model)
```

### 4.3 differences by clustering, theory, persuasion theory, and self-report? 

**cluster**
```{r cluster}
dat |> split(~cluster_assigned) |>   
  imap(~count_and_robust(.x)) 

# is this just a self-report difference
dat |> group_by(cluster_assigned, self_report) |> study_count()
```

**theory differences**
```{r theory_differences}

dat |> filter(theory != 'economics') |> 
  # economics has one study so we have to report it separately
  split(~theory) |> 
  imap(~count_and_robust(.x))

dat |> filter(theory == 'economics') |> select(author, year, d, se_d)

# I have no idea why this was so complicated to do but it works
setNames(
  lapply(c('animal welfare', 'environment', 'health'), function(term) {
    dat %>%
      filter(str_detect(persuasion_category, term)) %>%
      count_and_robust()
  }),
  c('animal', 'environment', 'health')
)

```

**self report or not**
```{r self_report}
dat |> split(~self_report) |> 
   imap(~count_and_robust(.x))

# a couple zoom ins on the self-report vs not

dat |>
  split(~self_report) |>
  imap(~sum_tab(.x, theory))

dat |>
  split(~self_report) |>
  imap(~sum_tab(.x, decade))
```

In light of this, I think the check for social desirability bias we outlined in the pre-analysis plan no longer makes sense, so we're omitting it.

### 4.4 do leaflet studies work? 

```{r leaflet}
dat |>
  filter(str_detect(intervention_condition, "leaflet") |
           str_detect(brief_description, "leaflet")) |>
  count_and_robust()

```

### 4.5 How do studies administered online compare to in-person studies?

```{r online_vs_in_person}
# effect size comparison
dat |> split(~str_detect(country, "internet")) |> map(count_and_robust)

# simple null vs not comparison (not simple to code, thank you chatGPT)
dat |>
  mutate(country_group = ifelse(grepl("internet", country), "Internet", "Other"))|>
  group_by(country_group) |>
  do({
    test_result <- binom.test(sum(.$neg_null_pos == 1), length(.$neg_null_pos), conf.level = 0.95)
    tibble(
      estimate = test_result$estimate,
      conf_low = test_result$conf.int[1],
      conf_high = test_result$conf.int[2],
      p_value = round(test_result$p.value, 5)
    )
  })
```

### 4.6 Any relationship between delay and outcome?

```{r delay}
dat |> sum_lm(d, delay)
dat |> ggplot(aes(d, delay)) + geom_point() + 
  stat_smooth(method = 'lm', se = F) + theme_minimal()
```

### 4.7 any relationship between publication date and effect size or validity?

```{r publication_date}

dat |> sum_lm(d, year) #...what is happening with year and d in this dataset

dat |> split(~decade) |>  
   imap(~count_and_robust(.x))
# wow! This is one of the most interesting results

dat |> ggplot(aes(year, d)) + geom_point() + 
  stat_smooth(method = 'lm', se = F) + theme_minimal()
```

Everything from hereonout is speculative.

### Effects on college students vs. adults vs. everyone

```{r population_tests}
dat |> mutate(pop_categories = case_when(
  str_detect(population, "adults") ~ 'adults',
  str_detect(population, "students") ~ 'college students',
  TRUE ~ 'other')) |>
  split(~pop_categories) |> 
  imap(~count_and_robust(.x))

```

### Effects by county/region
Grouping studies into US, UK, Italy & other because you need at least three clusters to do meta-analysis at all

```{r country_effects}
dat |> mutate(country_categories = case_when(
  str_detect(country, "(US)|United States") ~ 'United States',
  str_detect(country, "United Kingdom") ~ 'United Kingdom',
  str_detect(country, "Italy") ~ "Italy",
  TRUE ~ 'Other')) |>
  split(~country_categories) |> 
  imap(~count_and_robust(.x))
```


### 4.9 record computational environment
```{r reproducibility}
session_info()
```