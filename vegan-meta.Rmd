---
title: "Nudges, norms, and persuasion approaches to reducing consumption of meat and animal products: a meta-analysis and theoretical review"
shorttitle: "vegan-meta"
author:
  - name: "Seth A. Green"
    affiliation: "1"
    address: "Kahneman-Treisman Center, Princeton University"
    email: "sag2212@columbia.edu"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Writing - Review & Editing"
  - name: "Maya Mathur"
    affiliation: "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name: "Benny Smith"
    affiliation: "3"
    role:

affiliation:
  - id: "1"
    institution: "Kahneman-Treisman Center, Princeton University"
  - id: "2"
    institution: "Stanford University"
  - id: "3"
    institution: "Allied Scholars for Animal Protection "
abstract: |
  This paper meta-analyzes interventions intended to reduce consumption of meat and animal products (MAP), focusing on the most rigorous, policy-ready studies. Extant efforts generally embody one of three theoretical perspectives: appeals to social norms, nudges to make MAP less salient, and information-based approaches that attempt to change attitudes towards MAP itself on health, environmental, and animal welfare grounds. We find that direct appeals to the environment and personal health, along with some norm messages in cafeterias and text-based nudges, can reduce MAP consumption. Appeals to animal welfare, online studies, and leafletting studies have no apparent overall effect. However, even the very best studies in this literature typically have concerning measurement limitations stemming from the predominance of self-reported outcomes and a lack of post-intervention follow-up. These limitations raise concerns about social desirability bias and `regression to the meat:' the possibility that someone who is nudged into eating less MAP at one meal will compensate by eating more at the next. We address these concerns with a variety of statistical corrections as well as highlighting the studies whose designs and measurement strategies convincingly address both issues. We conclude with concrete, shovel-ready suggestions for future researchers that take the lessons of this analysis into account. 
authornote: |
 This work was generously supported by the Food System Research Fund and NIH award 1R01LM013866-01.
keywords: "meta-analysis, meta-science, meat-and-animal-products, evaluation"
wordcount: "3213"
bibliography: "./manuscript/vegan-refs.bib"
floatsintext: no
linenumbers: no
draft: no
mask: no
figurelist: no
tablelist: no
footnotelist: no
output: papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(dplyr)

dat <- read.csv('./data/vegan-meta.csv')  |>
  group_by(title) |>
  mutate(unique_paper_id = cur_group_id())  |> 
  ungroup() |> group_by(unique_paper_id, intervention_condition) |> 
  mutate(unique_study_id = cur_group_id()) |>
  ungroup() |>
  mutate(
         decade = as.factor(
      case_when(
        year >= 2000 & year <= 2009 ~ "2000s",
        year >= 2010 & year <= 2019  ~ "2010s",
        year >= 2020 ~ "2020s")),
      total_sample = n_c_post + n_c_post) |>
  select(author, year, title, unique_paper_id, unique_study_id, everything())
  
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
```

## 1. Introduction: a multitude of perspectives on changing dietary behavior

Reducing consumption of meat and animal products (MAP) is vital to many policy goals. Animal agriculture is a major driver of climate change [@scarborough2023; @koneswaran2008; @goodland2009] as well as more localized environmental and public health harms [@horrigan2002; @slingenbergh2004; @graham2008; @greger2010]. Excess MAP consumption is a leading cause of premature deaths [@willett2019; @landry2023]; finally, the conditions in which farmed animals live and die are increasingly recognized as a policy matter in their own right [@yeates2011; @webster2001; @kuruc2023].

Policymakers have a broad array of tools at their disposal for attempting to change eating behavior. For example, they can ban certain kinds of food [@caro2009] or practices [@bursey2018] perceived to be unusually cruel; campaign for vegetarianism [@trewern2022]; or change eating environments to make non-meat options more salient or appealing [@guthrie2015; @bianchi2018restructuring]. However, any policy lever which focuses on supply rather than reducing consumer demand risks backsliding through political correction [@michielsen2022]. It is essential, therefore, to assess which theoretical approaches most effectively and durably alter consumer eating behavior. This paper approaches that question with a theoretically comprehensive review and methodologically focused meta-analysis.

A previous review called on future MAP research to feature more “direct behavioral outcomes” and “long-term follow-up" [@mathur2021meta, p. 1]. Our paper builds on that one, and thus restricts its meta-analysis to the very most rigorous, policy-ready research: randomized controlled trials with at least 25 subjects in treatment and control (or at least 10 clusters in cluster-assigned studies) that measure actual MAP consumption at least a single day after treatment begins. We identified `r num_studies` such interventions published in `r num_papers` papers or technical reports.

Scholars have approached MAP reduction from many disciplines, including social psychology [@rosenfeld2018; @dhont2019], economics [@lusk2009], choice architecture [@bianchi2018restructuring; @mertens2022]; and environmental studies [@costello2016]. The central theoretical divide we observe among policy-ready studies is between **information-based approaches** that attempt to change ideas about MAP consumption, **nudges** that make MAP consumption less salient, alter the defaults, and expensive, and **norms-based approaches** that make MAP seem socially undesirable. Which of these approaches is more effective at changing real world behavior has broad implications for behavioral scientists and policymakers.

Our main quantitative finding is that appeals to personal health and the environment are the most broadly effective MAP reduction strategies. We also see some evidence of change resulting from a handful of salience- and norms-based changes to some university dining halls, as well as text-based nudges to reduce MAP consumption. We find overall null results for appeals to animal welfare, studies conducted online, and leafletting studies.

However, we place low confidence in the generalizability of these results due to two concerns about measurement. The first is social desirability bias arising from self-reported outcomes [@mathur2021effectiveness], which are predominant in the information-based literature. The second is the place-based literature's general lack of engagement with the possibility of compensatory/backlash effects: that someone who is nudged into eating less MAP at one meal may perceive a moral license [@merritt2010; @blanken2015] to eat more MAP later. If we limit our analysis to studies with deal convincingly with both threats to inference, we still conclude that appeals to the environment and health, as well as a handful of nudges and norms messages, reduce reduce MAP consumption, but in the highly specialized context of dining halls at elite American universities. For this literature to truly become shovel-ready for policymakers, it urgently needs extension and replication, along with careful attention to measurement validity.

Our paper builds on two literatures. The first is systematic reviews of attempts to reduce MAP consumption, of which there have been 22 by our count, with two others that we know of forthcoming. (See appendix A for a complete overview). Our paper makes four contributions on top of this already rich set. First, our database is current as of December 2023, which has significance in an emerging literature whose most credible estimates and best designs tend to be in recent publications. Second, our review is quantitative, while most previous reviews are narrative or systematic reviews but do not offer meta-analysis. Third, among papers with a meta-analytic component, ours is (so far) unique for being theoretically comprehensive rather than focusing on the effects of a single conceptual approach. Fourth, ours is the only review to our knowledge to set strict inclusion criteria that attempt to identify the most rigorous, policy-ready research.[^1]

[^1]: We'd also note that some of these reviews err strongly on the side of inclusion, and thus meta-analyze studies with serious threats to internal validity. For example, prior reviews included studies that use differential screening procedures for treatment and control [@rees2018] or that alter the treatment after assignment based on subjects' prior diets [@morren2021]; these procedures violate the 'all else equal in expectation' condition of a randomized controlled trial. As @simonsohn2022 put it, combining "studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts" with high-quality studies creates averages that are "virtually guaranteed to lack a meaningful interpretation" (p. 551).

Second, our paper contributes to a growing literature of meta-analyses, reviews, and "megastudies" [@doell2023; @milkman2021; @milkman2021megastudy] that attempt to compare the efficacy of different approaches to solving social problems. In social psychology, @paluck2021 and @porat2024 apply meta-analytic methods to testing the efficacy of different theories at reducing prejudice and sexual violence, respectively, while @vlasceanu2024 test 11 theoretically diverse interventions aimed at four "climate mitigation outcomes" (p. 1).[^2] In a related vein, @bergquist2023 provide a meta-analysis of meta-analyses for climate mitigation behaviors, and find that social comparison and financial incentives were generally effective, while information and feedback generally were not. We also build on prior reviews that that holistically assess the efficacy of choice architecture approaches versus, e.g., taxes [@list2023] or subsidies [@campos2021]. Finally, a paper from @bergman2024 compares information-based approaches to "short-term financial assistance, customized assistance during the housing search process, and connections to landlords" (p. XXX) to encourage families to move to high-opportunity areas. Our paper's approach is similar to all of these but distinct, to the best of our knowledge, for the stringency and policy focus of our meta-analytic inclusion criteria.

[^2]: This is distinct from the "Metaketa" initiatives from Evidence on Governance and Politics, which run many field experiments simultaneously to test the effect of one theoretical approach on a common outcome [@slough2021; @blair2021]. Metaketas are concerned primarily with external validity, and thus keep the independent variable approximately constant, while megastudies are aimed at comparative efficacy, and so test many distinct treatments.

The remainder of the paper proceeds as follows. Section 2 details and motivatess the inclusion criteria and search process by which we assembled our meta-analytic database. Section 3 describes the database. We provide some descriptive statistics about the database, and then present its major theoretical approaches \textemdash environmental, health, and animal welfare appeals to MAP reduction comprising information-based strategies, norms manipulations, and nudges \textemdash and representative literature from each. For each subset, we also highlight some frequent design or measurement limitations that led us to exclude otherwise promising results. Section 4 provides an overview of our meta-analytic methods and procedures.

Section 5 presents our quantitative results. We provide a mixture of pooled averages, tests for publication biases, and comparisons of different approaches. We also offer some attempts to estimate the magnitude of different biases in this literature drawn from related literatures, and to use those estimates to offer effect size corrections. These analyses are tentative, and offer readers a broad range of possibilities intended to match different priors about the severity and importance of the measurement issues we enumerate Section 6 concludes with some concrete suggestions for future MAP reduction researchers based on the results of our analyses.

## 2 Assembling the meta-analytic database

### 2.1 Selecting inclusion criteria

Meta-analysis is a powerful, flexible procedure for pooling results from many studies into a singular estimate, or cluster of estimates, denoting the relationship between a treatment and an outcome across contexts. For this to be an unbiased estimate of the true causal relationship between the two requires several additional assumptions. First, the pooled studies must each furnish an unbiased causal estimate. Second, the set of studies must be a random sample of the universe of possible studies, and not, for instance, truncated by publication bias [@thornton2000]. Third, the assembled outcomes must have a persistent, known relationship with the true outcome of interest.

Each of these propositions is, in theory, testable and amenable to statistical correction (see @mathur2022 and @green2024 for suggestions and strategies). However, the most fundamental challenge to meta-analysis is whether the underlying data are coherently integrable. A recent paper by @slough2023 makes this point forcefully. In their view, for studies to have "target equivalence" \textemdash the property of identifying "the same estimand" (p. 1) \textemdash they must first achieve harmonized contrasts and measurements, meaning that the "substantive comparison across studies is the same" and "the outcome of interest is the same and it is measured in the same way" (p.2). These properties are necessary for meta-analytic results to be "meaningful and interpretable" (p.2). Crucially, they cannot be achieved "solely with statistical techniques" and are instead a product of tailored "design or inclusion criteria" (p.2).

We share this paper's concerns, and we address them via the following inclusion criteria.

First, we only look at randomized controlled trials for historically well-understood reasons [@cook2002; see @simonsohn2022 for discussion specific to meta-analysis]. This meant both that treatment was randomized and that there was a true, no-treatment control group for comparison.

Second, studies needed to measure MAP consumption directly. This included self-reported outcomes. Although this is a potential source of bias [@hebert1995social; @hebert1997gender; @cerri2019], it is a well-understood, widely studied problem, which means that we have reasonable priors about its magnitude, and therefore can account for it in our sensitivity analyses.

Third, studies needed to measure MAP consumption at least a single day after treatment. For information-based studies, this was straightforward: essentially every treatment took under an hour to administer, and we looked only at studies where there was a delay of at least 24 hours before outcomes were collected. For place-based interventions, measuring this was a little trickier. Most studies in this category measured outcomes while treatment was being actively administered, e.g. a dining hall with a dynamic norms message on display counting the amount of meat sold while the sign was up. For an individual subject of such a study, there was no delay between treatment and outcome. We decided to count studies that took place for more than one day and measured outcomes continuously (e.g. if they displayed the dynamic norms message at many lunches consecutively). Arguably these studies' results will capture any adaptation to treatment, and an enduring effect over many days therefore represents an enduring effect. However, we remain concern about what happens to treated subjects once the treatment ends, and we return to this point in our quantitative results.

Finally, we required that studies have at least 25 subjects in both treatment and control, or, for cluster-assigned studies, at least 10 clusters in total. @paluck2021 found that studies with fewer than 25 subjects per arm, which constituted the smallest quintile of studies in the prejudice reduction literature, showed systematically larger effects than their larger peers. That paper also argued that fewer than 10 clusters would be too few to calculate meaningful standard errors. In practice, we excluded very few studies for having fewer than 25 subjects, but quite a few for having fewer than 10 clusters; many studies describing themselves as experiments had just one unit assigned to treatment and one to control, but recorded results at the level of individuals, thereby ignoring clustered standard errors. We treat these studies as, effectively, quasi-experiments and did not include them in our database.

We also required that the full papers be available on the internet, rather than just a summary or abstract, and written in English.

### 2.2 Our search process

Our cutoff date for papers was December 2023.

```{r where_do_studies_come_from }
source("./code/functions/sum_tab.R")
paper_sources <- dat |> group_by(unique_paper_id) |> slice(1) |> sum_tab(source)
paper_sources

prior_knowledge <- as.numeric(paper_sources["prior knowledge"])
initial_review_papers <- as.numeric(paper_sources["Bianchi (2018a)"]) + 
  as.numeric(paper_sources["Mathur (2021)"])

second_review_papers <- as.numeric(paper_sources["Chang (2023)"]) + 
  as.numeric(paper_sources["Harguess (2019)"]) +  as.numeric(paper_sources["Wynes (2018)"])

cv_papers <- as.numeric(paper_sources["CV (Sparkman)"]) + 
  as.numeric(paper_sources["CV (Jalil)"])

snowball_papers <- as.numeric(paper_sources["snowball search"])

systematic_search_papers <- as.numeric(paper_sources["systematic search"])
```

First, we read all qualifying studies that two authors (SAG and BS) knew of at the outset, which yielded `r prior_knowledge` papers.

Second, we located and read XX previous systematic reviews, starting with @mathur2021effectiveness as well as @bianchi2018restructuring and @bianchi2018conscious. Those three reviews yielded `r initial_review_papers` additional papers. We then read and categorized papers from XX additional reviews (assisted greatly by @grundy2022, a review of reviews); From @harguess2020, @chang2023 and @wynes2018,  we learned of `r second_review_papers` additional papers.

Third, We checked the CVs of prominent researchers in the field, which yielded `r cv_papers` additional papers.

Fourth, we conducted a snowball search where we read papers our assembled papers had cited, papers suggested to us by article homepages, and papers that cited articles already in our database. This yielded `r snowball_papers` papers.

Finally, we conducted a systematic search of Google Scholar for the terms [OUR SEARCH TERMS ONCE WE ARE DONE]. This yielded `r systematic_search_papers` more papers, bringing our total sample size to `r num_papers` papers and `r num_studies` interventions.[^3]

[^3]: In four cases, we condensed multiple interventions into one statistical result based on how they were reported in the paper: @abrahamse2007, @dijkstra2022, @lacroix2020 and @peacock2017. These studies all present substantively null results and, in their results sections, group multiple treatment arms into singular statistical results, which we recorded. 

## 3 The meta-analytic database

We first offer some descriptive statistics of our database and then provide a theoretical review.

### 3.1 Descriptive overview of meta-analytic database

```{r descriptive_stats, echo = F, include=F}
library(dplyr)
library(stringr)
library(purrr)

# Data preparation and basic statistics
paper_dat <- dat |> 
  group_by(unique_paper_id) |> 
  slice(1) |>
  mutate(pub_status = case_when(
    str_detect(doi_or_url, "osf\\.io") ~ "preprint",
    str_detect(doi_or_url, "10\\.") ~ "publication",
    TRUE ~ "dissertation_or_tech_report"),
    decade = as.character(cut(year, breaks = c(1999, 2009, 2019, 2029), labels = c("2000s", "2010s", "2020s")))) |>
  ungroup()

# Decade Analysis
decade_tab <- paper_dat |> count(decade)
first_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2000s"])
second_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2010s"])
third_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2020s"])

# Publication Type Analysis
doi_tab <- paper_dat |> count(pub_status)
published_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "publication"])
preprint_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "preprint"])
other_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "dissertation_or_tech_report"])

# Non-DOI Paper Analysis
non_doi_paper_stats <- paper_dat |> filter(pub_status != "publication") |> count(venue)
dissertation_papers <- as.numeric(non_doi_paper_stats$`n`[non_doi_paper_stats$venue == "dissertation"])
advocacy_papers <- as.numeric(non_doi_paper_stats$`n`[non_doi_paper_stats$venue == "advocacy org publication"])

# Venue Analysis
venue_counts <- table(paper_dat$venue)
num_venues <- length(venue_counts)
most_common_venue <- names(sort(venue_counts, decreasing = TRUE)[1])
most_common_venue_count <- max(venue_counts)

# Correcting Journal Article Counts
journal_article_counts <- paper_dat |> 
  filter(pub_status == "publication") |> 
  count(venue) |> 
  arrange(desc(n))

# Cluster Assigned Analysis
cluster_sample_stats <- paper_dat |> 
  filter(cluster_assigned == "Y") |> 
  summarise(median = median(total_sample, na.rm = TRUE), 
            min = min(total_sample, na.rm = TRUE), 
            max = max(total_sample, na.rm = TRUE))

# Non-Clustered Sample Size Statistics
non_cluster_sample_stats <- paper_dat |> 
  filter(cluster_assigned == "N") |> 
  summarise(median = median(total_sample, na.rm = TRUE), 
            min = min(total_sample, na.rm = TRUE), 
            max = max(total_sample, na.rm = TRUE))

# Intervention Types Count
intervention_types_counts <- paper_dat |> 
  summarise(cafeteria_or_restaurant = sum(cafeteria_or_restaurant_based == "Y", na.rm = TRUE),
            multi_component = sum(multi_component == "Y", na.rm = TRUE),
            leaflet_count = sum(leaflet == "Y", na.rm = TRUE),
            video_count = sum(video == "Y", na.rm = TRUE),
            internet_based = sum(internet == "Y", na.rm = TRUE))

# Emotional Activation
emotional_activation_stats <- paper_dat |> 
  summarise(emotional_activated = sum(emotional_activation == "Y", na.rm = TRUE), 
            emotional_not_activated = sum(emotional_activation == "N", na.rm = TRUE))

# Pre-Analysis Plan Stats
pre_analysis_plan_stats <- paper_dat |> 
  summarise(pre_analysis_yes = sum(public_pre_analysis_plan != "N", na.rm = TRUE),
            pre_analysis_no = sum(public_pre_analysis_plan == "N", na.rm = TRUE))

# Open Data Stats
open_data_stats <- paper_dat |> 
  summarise(open_data_yes = sum(open_data != "N", na.rm = TRUE),
            open_data_no = sum(open_data == "N", na.rm = TRUE))
open_data_stats
```

The earliest paper in our sample is @allen2002, and the latest is @jalil2023. Remarkably, a majority of these papers have been published since 2020: `r first_decade_papers` in the 2000s, `r second_decade_papers` in the 2010s, and `r third_decade_papers` in the 2020s.

Out of `r num_papers` total papers, `r published_papers` were published in peer-reviewed journals. `r dissertation_papers` are dissertations, one is a preprint, and four were published by advocacy organizations. Among journal articles, `r num_venues` different journals are represented. The most common venue is *`r most_common_venue`,* with `r most_common_venue_count` papers, followed by three in _Journal of Environmental Psychology_, and two apiece in _American Journal of Public Health_, and _Frontiers in Psychology_. The remainder were published in field-specific journals for food and nutrition, sustainability, psychology, and public health, with psychology appearing to be a slight majority.

The methods of intervention in the studies are varied: `r intervention_types_counts$cafeteria_or_restaurant` were cafeteria or restaurant-based, `r intervention_types_counts$multi_component` used multi-component strategies, while `r intervention_types_counts$leaflet_count` employed leaflets. Digital mediums were also notable, with `r intervention_types_counts$video_count` using videos and `r intervention_types_counts$internet_based` being internet-based.

In terms of study designs, `r cluster_sample_stats$n` studies were assigned to clusters, with a median sample size of `r cluster_sample_stats$median`, ranging from `r cluster_sample_stats$min` to `r cluster_sample_stats$max`. Non-clustered studies had a similar range in sample sizes. Non-clustered studies showed a median sample size of `r non_cluster_sample_stats$median`, ranging from `r non_cluster_sample_stats$min` to `r non_cluster_sample_stats$max`.

Emotional activation was a factor in `r emotional_activation_stats$emotional_activated` studies Furthermore, `r open_data_stats$open_data_yes` studies made their data openly available, and `r pre_analysis_plan_stats$pre_analysis_yes` studies had a pre-analysis plan.

### 3.2 A diverse array of theoretical approaches

### Health

@lohmann2022, @klockner2017 recommended people eat other kinds of MAP...

### Animal welfare

#### Social desirability bias..

...Consider [Fehrenbach (2015)](https://www.proquest.com/docview/1712399091?fromopenview=true&pq-origsite=gscholar), a 2015 dissertation that tested the “effectiveness of two video messages designed to encourage Americans to reduce their meat consumption.” The study had two treatment arms and a control. Both treatment videos sought to induce a feeling of “high threat” by informing viewers of the “the negative health effects of high meat consumption;” one video also sought to induce feelings of “high efficacy” by suggesting “easy ways to reduce their meat consumption,” while the “low efficacy” group’s video “only included a very minor efficacy component in the conclusion.” The videos were 7 and 4 minutes long, respectively. Before the study, on the day of the study, and again a week later, participants were asked about their attitudes and intentions towards eating meat, as well as how much meat they’d eaten in the past 7 days.

Overall, the high threat/high efficacy group reported that they ate an average of 3.16 fewer meals involving meat in the week following the intervention than the one before it, compared to 2.11 for the high threat/low efficacy group and 1.92 for the control group. As a benchmark, the population ate meat at an average of 13.64 meals per week before the intervention (SD = 4.21).

This study strikes us as having a high risk of social desirability bias for three reasons.

First, the study is designed to make people feel a sense of “high threat” from eating meat, and then asks them a week later about how much meat they ate. There are grounds for doubting how much respondents would accurately recount their eating habits. This problem is typical of this literature.

Second, the study asks participants to recall a week’s worth of meals; previous research has found that [daily food diaries lead to more accurate reports](https://pubmed.ncbi.nlm.nih.gov/7635601/). As [Mathur et al. (2021a)](https://www.sciencedirect.com/science/article/pii/S0195666321001847) put it:

> Many existing studies measure meat consumption in terms of, for example, Likert-type items that categorize the number of weekly meals containing meat (e.g., “none”, “1–5 meals”, etc.) or in terms of reductions from one’s previous consumption. When possible, using finer-grained absolute measures, such as the number of servings of poultry, beef, pork, lamb, fish, etc., would enable effect sizes to be translated into direct measures of societal impact.

Third, the decline in meat-eating among the control group suggests that the intended direction of the experiment might have been crystal clear to everyone, whether they watched the video or not.

In sum, using broad stroke, self-reported outcomes in a context where meat is being presented as bad for you seems like a high-risk environment for [experimenter demand effects](https://www.elgaronline.com/display/edcoll/9781788110556/9781788110556.00031.xml).

### Health

However, a tricky case emerged in studies that attempted to reduce red or processed meat consumption rather than MAP consumption as a whole. Some of these studies recommended that people substitute to other MAP, such as chicken or fish, but none to our knowledge measured any outcome besides red and/or processed meat. We decided to include studies focused on red and/or processed meat so long as they did not specifically advise inter-MAP substitution.

### Nudges

are texts nudges? these articles don't frame it that way but they're very similar to articles that do we've got the megastudy but also <https://jamanetwork.com/journals/jamapediatrics/fullarticle/2801662> and <https://www.nature.com/articles/s41586-021-03843-2> and <https://www.hbs.edu/ris/Publication%20Files/text_messages_6294f05d-ef00-420d-9a44-24dfc8989d01.pdf>

## 4 Meta-analytic Methods

## 5 Meta-analysis

```{r echo=F, include=F}
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(knitr)
library(purrr)
library(sessioninfo)
library(stringr)

source('./code/functions/d_calc.R')
source('./code/functions/var_d_calc.R')
source('./code/functions/study_count.R')
source('./code/functions/map_robust.R')

#' count_and_robust helper function;
#' necessary b/c `filter |> split |> imap()` didn't work for some reason

  count_and_robust <- function(data) {
      bind_cols(study_count(data), map_robust(data)) |>
        kable('markdown')
  }

dat <- dat |> mutate(d = mapply(
        FUN = d_calc,
        stat_type = eff_type,
        stat =  u_s_d,
        sample_sd = ctrl_sd,
        n_t = n_t_post,
        n_c = n_c_post),
      var_d = mapply(
        FUN = var_d_calc,
        d = d,
        n_c = n_c_post,
        n_t = n_t_post),
      se_d = sqrt(var_d))
dat |> count_and_robust()

dat |> mutate(has_pap = if_else(public_pre_analysis_plan == 'N', 0, 1)) |> 
  split(~has_pap) |>  
  map(count_and_robust)

```

### Bibliography
