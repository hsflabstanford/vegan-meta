---
title: "Nudges, norms, and persuasion approaches to reducing consumption of meat and animal products: a meta-analysis and theoretical review"
shorttitle: "vegan-meta"
author:
  - name: "Seth A. Green"
    affiliation: "1"
    address: "Kahneman-Treisman Center, Princeton University"
    email: "sag2212@columbia.edu"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Writing - Review & Editing"
  - name: "Maya Mathur"
    affiliation: "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name: "Benny Smith"
    affiliation: "3"
    role:

affiliation:
  - id: "1"
    institution: "Kahneman-Treisman Center, Princeton University"
  - id: "2"
    institution: "Stanford University"
  - id: "3"
    institution: "Allied Scholars for Animal Protection "
abstract: |
  This paper meta-analyzes interventions intended to reduce consumption of meat and animal products (MAP), focusing on the most rigorous, policy-ready studies. Extant efforts generally embody one of three theoretical perspectives: appeals to social norms, nudges to make MAP less salient, and information-based approaches that attempt to change attitudes towards MAP itself on health, environmental, and animal welfare grounds. We find that direct appeals to the environment and personal health, along with some norm messages in cafeterias. Appeals to animal welfare, online studies, video treatments, and leafletting studies have no apparent overall effect. However, even the very best studies in this literature typically have concerning measurement limitations arising from self-reported outcomes and a lack of post-intervention follow-up. These limitations raise concerns about social desirability bias and `regression to the meat:' the possibility that someone who is nudged into eating less MAP at one meal will compensate by eating more at the next. We address these concerns with a variety of statistical corrections as well as highlighting the studies whose designs and measurement strategies convincingly address both issues. We conclude with concrete, shovel-ready suggestions for researchers and policymakers. 
  
authornote: |
 This work was generously supported by the Food System Research Fund and NIH award 1R01LM013866-01. Thanks to Alex Berke, Alix Winter, Anson Berns, Hari Dandapani, Adin Richards, and Matt Lerner for comments on an early draft. Thanks to Jacob Peacock, Andrew Jalil, Gregg Sparkman, Joshua Tasoff, Lucius Caviola, and Emma Garnett for helping assemble the database and providing guidance on their studies. Thanks to Jeff Shrader, Andrey Fradkin, Sofia Verduzco, and Daniel Waldinger for helpful suggestions.

keywords: "meta-analysis, meta-science, meat-and-animal-products, evaluation"
wordcount: "6244"
bibliography: "./manuscript/vegan-refs.bib"
floatsintext: no
linenumbers: no
draft: no
mask: no
figurelist: no
tablelist: no
footnotelist: no
output: 
  papaja::apa6_pdf:
    keep_tex: no
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(dplyr, warn.conflicts = F)
knitr::opts_chunk$set(fig.path = "results/figures/")

dat <- read.csv('./data/vegan-meta.csv')  |>
  group_by(title) |>
  mutate(unique_paper_id = cur_group_id())  |> 
  ungroup() |> 
  group_by(unique_paper_id, intervention_condition) |> 
  mutate(unique_study_id = cur_group_id()) |>
  ungroup() |>
  mutate(decade = as.factor(case_when(year >= 2000 & year <= 2009 ~ "2000s",
                                      year >= 2010 & year <= 2019  ~ "2010s",
                                      year >= 2020 ~ "2020s")
                            ),
         total_sample = n_c_post + n_c_post) |>
  select(author, year, title, unique_paper_id, unique_study_id, everything())
  
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
```

```{r redownload_data_if_missing, echo=F, include=F, eval=F}
library(googledrive)

# drive_auth() # only if needed
drive_download(as_id('1mPCt7HuK7URvuWcsMQokQCOGnSold-TS0NyC1EZniJk'), 
               path = './data/vegan-meta.csv',
               overwrite= TRUE)
```

## 1. Introduction: the three leading perspectives on changing dietary behavior

Reducing consumption of meat and animal products (MAP) would advance numerous policy goals. Animal agriculture is a major driver of climate change [@scarborough2023; @koneswaran2008; @goodland2009] as well as more localized environmental and public health harms [@horrigan2002; @slingenbergh2004; @graham2008; @greger2010]. Excess MAP consumption is a leading cause of premature deaths [@willett2019; @landry2023]; finally, the conditions in which farmed animals live and die are increasingly recognized as a policy matter in its own right [@yeates2011; @webster2001; @kuruc2023].

Policymakers have many tools availabe for reducing MAP consumption. They can can ban certain foods [@caro2009] or practices [@bursey2018] deemed especially cruel; campaign for vegetarianism [@trewern2022]; raise taxes on MAP with high externalities [@springmann2018]; or change eating environments to make MAP alternatives more salient or appealing [@guthrie2015; @bianchi2018restructuring]. However, any lever which focuses on reducing supply rather than demand risks backsliding through political correction [@michielsen2022]. It is essential, therefore, to assess which theoretical approaches most effectively and durably reduce demand for MAP. This paper approaches that question with a theoretically comprehensive review and methodologically focused meta-analysis.

A previous review called on future MAP research to feature more “direct behavioral outcomes” and “long-term follow-up" [@mathur2021meta, p. 1]. Building on this, our paper restricts its quantitative synthesis to the most rigorous, policy-ready research: randomized controlled trials with at least 25 subjects in treatment and control (or at least 10 clusters in cluster-assigned studies) that measure actual MAP consumption at least a single day after treatment begins. We identified `r num_studies` such interventions published in `r num_papers` papers or technical reports.

Scholars have approached MAP reduction from many disciplines, including social psychology [@rosenfeld2018; @dhont2019], economics [@lusk2009], choice architecture [@bianchi2018restructuring; @mertens2022]; and environmental studies [@costello2016]. Among the studies that meet our inclusion criteria, three approaches prevail: **norms-based approaches** that make MAP seem socially undesirable, **nudges** that make MAP alternatives more salient, and **persuasion-based approaches** that attempt to change ideas about the desirability of MAP itself. Which approach is most effective at changing real world behavior has broad implications for both this particular issue and policy in general.

Our main quantitative finding is that appeals to personal health and the environment are the most effective MAP reduction strategies. We also see some evidence of change resulting from a handful of salience- and norms-based changes to some university dining halls. We find overall null results for appeals to animal welfare, studies conducted online, video treatments, and leafletting studies.

However, we place low confidence in the generalizability of these findings because of two potential soruces of measurement error: social desirability bias arising from self-reported outcomes [@mathur2021effectiveness]: and, for interventions that alter a particular eating environment, a general lack of engagement with the possibility of compensatory/backlash effects down the line. We think of this as the "regression to the meat" problem: that someone who is nudged into eating less MAP at one meal may perceive a moral license [@merritt2010; @blanken2015] to eat more MAP later. If we limit our analysis to studies with deal convincingly with both threats to inference, we still conclude that appeals to the environment and health, as well as a handful of nudges and norms messages, reduce reduce MAP consumption, but in the highly specialized context of dining halls at elite American universities. For this literature to truly become shovel-ready for policymakers, it urgently needs extension and replication, along with careful attention to measurement validity.

Our paper builds on two literatures. The first is systematic reviews of attempts to reduce MAP consumption, of which there have been 22 by our count, with two others that we know of forthcoming. (See appendix A for a complete overview). Our paper makes four contributions on top of this already rich set. First, our database is current as of December 2023, which has significance in an emerging literature whose most credible estimates and best designs tend to be in recent publications. Second, our review is quantitative, while most previous reviews are narrative or systematic reviews but do not offer meta-analysis. Third, among papers with a meta-analytic component, ours is (so far) unique for being theoretically comprehensive rather than focusing on the effects of a single conceptual approach. Fourth, ours is the only review to our knowledge to set strict inclusion criteria that attempt to identify the most rigorous, policy-ready research.[^1]

[^1]: We'd also note that some of these reviews err strongly on the side of inclusion, and thus meta-analyze studies with serious threats to internal validity. For example, prior reviews included studies that use differential screening procedures for treatment and control [@rees2018] or that alter the treatment after assignment based on subjects' prior diets [@morren2021]; these procedures violate the 'all else equal in expectation' condition of a randomized controlled trial. As @simonsohn2022 put it, combining "studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts" with high-quality studies creates averages that are "virtually guaranteed to lack a meaningful interpretation" (p. 551).

Second, our paper contributes to a growing literature of meta-analyses, reviews, and "megastudies" [@doell2023; @milkman2021; @milkman2021megastudy] that attempt to compare the efficacy of different approaches to solving social problems. In social psychology, @paluck2021 and @porat2024 apply meta-analytic methods to testing the efficacy of different theories at reducing prejudice and sexual violence, respectively, while @vlasceanu2024 test 11 theoretically diverse interventions aimed at four "climate mitigation outcomes" (p. 1). In a related vein, @bergquist2023 provide a meta-analysis of meta-analyses for climate mitigation behaviors, and find that social comparison and financial incentives were generally effective, while information and feedback generally were not. We also build on prior reviews that that holistically assess the efficacy of choice architecture approaches versus, e.g., taxes [@list2023] or subsidies [@campos2021]. Finally, a paper from @bergman2024 compares information-based approaches to "short-term financial assistance, customized assistance during the housing search process, and connections to landlords" (p. XXX) to encourage families to move to high-opportunity areas. Our paper's approach is similar to all of these but distinct, to the best of our knowledge, for the stringency and policy focus of our meta-analytic inclusion criteria.

The remainder of the paper proceeds as follows. Section 2 details and motivatess the inclusion criteria and search process by which we assembled our meta-analytic database. Section 3 describes the database. We provide some descriptive statistics about the database, and then present its major theoretical approaches \textemdash environmental, health, and animal welfare appeals to MAP reduction comprising information-based strategies, norms manipulations, and nudges \textemdash and representative literature from each. For each subset, we also highlight some frequent design or measurement limitations that led us to exclude otherwise promising results. Section 4 provides an overview of our meta-analytic methods and procedures.

Section 5 presents our quantitative results. We provide a mixture of pooled averages, tests for publication biases, and comparisons of different approaches. We also offer some attempts to estimate the magnitude of different biases in this literature drawn from related literatures, and to use those estimates to offer effect size corrections. These analyses are tentative, and offer readers a broad range of possibilities intended to match different priors about the severity and importance of the measurement issues we enumerate Section 6 concludes with some concrete suggestions for future MAP reduction researchers based on the results of our analyses.

## 2 Assembling the meta-analytic database

### 2.1 Selecting inclusion criteria

Meta-analysis is a powerful, flexible procedure for pooling results from many studies into a singular estimate, or cluster of estimates, denoting the relationship between a treatment and an outcome across contexts. For this to be an unbiased estimate of the true causal relationship between the two requires several additional assumptions. First, the pooled studies must each furnish an unbiased causal estimate. Second, the set of studies must be a random sample of the universe of possible studies, and not, for instance, truncated by publication bias [@thornton2000]. Third, the assembled outcomes must have a persistent, known relationship with the true outcome of interest.

Each of these propositions is, in theory, testable and amenable to statistical correction (see @mathur2022 and @green2024 for suggestions and strategies). However, the most fundamental challenge to meta-analysis is whether the underlying data are coherently integrable. A recent paper by @slough2023 makes this point forcefully. In their view, for studies to have "target equivalence" \textemdash the property of identifying "the same estimand" (p. 1) \textemdash they must first achieve harmonized contrasts and measurements, meaning that the "substantive comparison across studies is the same" and "the outcome of interest is the same and it is measured in the same way" (p.2). These properties are necessary for meta-analytic results to be "meaningful and interpretable" (p.2). Crucially, they cannot be achieved "solely with statistical techniques" and are instead a product of tailored "design or inclusion criteria" (p.2).

We share this paper's concerns, and we address them via the following inclusion criteria.

First, we only look at randomized controlled trials for historically well-understood reasons [@cook2002; see @simonsohn2022 for discussion specific to meta-analysis]. This meant both that treatment was randomized and that there was a true, no-treatment control group for comparison.

Second, studies needed to measure MAP consumption directly, whether indirectly or through self-report. Although this self-report is a potential source of bias [@hebert1995social; @hebert1997gender; @cerri2019], it is a well-understood, widely studied problem, which means that we have reasonable priors about its magnitude, and therefore can account for it in our sensitivity analyses. However, we excluded studies that only measured consumption of particular meat products, typically red and/or processed meat. If subjects reduce their red and/or processed meat consumption by switching to chicken or fish, this might be an improvement on environmental or health grounds but a step backwards for animal welfare [@mathur2022ethical]. Moreover, such measures are likely to overestimate overall MAP reduction, but not in a way that allows for well-validated estiamtes of the magnitude of bias. We return to this point, and analyze some studies that evaluate red and/or processed meat consumption, in an appendix.

Third, studies needed to measure MAP consumption at least a single day after treatment. For information-based studies, this was straightforward: essentially every treatment took under an hour to administer, and we looked only at studies where there was a delay of at least 24 hours before outcomes were collected. For place-based interventions, measuring this was a little trickier. Most studies in this category measured outcomes while treatment was being actively administered, e.g. a dining hall with a dynamic norms message on display counting the amount of meat sold while the sign was up. For an individual subject of such a study, there was no delay between treatment and outcome. We decided to count studies that took place for more than one day and measured outcomes continuously (e.g. if they displayed the dynamic norms message at many lunches consecutively). Arguably these studies' results will capture any adaptation to treatment, and an enduring effect over many days therefore represents an enduring effect. However, we remain concern about what happens to treated subjects once the treatment ends, and we return to this point in our quantitative results.

Finally, we required that studies have at least 25 subjects in both treatment and control, or, for cluster-assigned studies, at least 10 clusters in total. @paluck2021 found that studies with fewer than 25 subjects per arm, which constituted the smallest quintile of studies in the prejudice reduction literature, showed systematically larger effects than their larger peers. That paper also argued that fewer than 10 clusters would be too few to calculate meaningful standard errors. In practice, we excluded very few studies for having fewer than 25 subjects, but quite a few for having fewer than 10 clusters; many studies describing themselves as experiments had just one unit assigned to treatment and one to control, but recorded results at the level of individuals, thereby ignoring clustered standard errors. We treat these studies as, effectively, quasi-experiments and did not include them in our database.

We also required that the full papers be available on the internet, rather than just a summary or abstract, and written in English.

### 2.2 Our search process

Our cutoff date for papers was December 2023.

```{r where_do_studies_come_from, echo=F, include=F}

source("./functions/sum_tab.R")

paper_sources <- dat |> group_by(unique_paper_id) |> slice(1) |> 
  sum_tab(source)
paper_sources

prior_knowledge <- paper_sources["prior knowledge"]
initial_review_papers <- as.numeric(paper_sources["Bianchi (2018a)"]) + 
  as.numeric(paper_sources["Mathur (2021)"])

second_review_papers <- as.numeric(paper_sources["Chang (2023)"]) + 
  as.numeric(paper_sources["Harguess (2019)"]) +  as.numeric(paper_sources["Wynes (2018)"])

cv_papers <- as.numeric(paper_sources["CV (Sparkman)"]) + 
  as.numeric(paper_sources["CV (Jalil)"])

snowball_papers <- paper_sources["snowball search"]

systematic_search_papers <- paper_sources["systematic search"]
```

First, we read all qualifying studies that authors 1 and 3 knew of at the outset, which yielded `r prior_knowledge` papers.

Second, we located and read XX previous systematic reviews, starting with @mathur2021effectiveness as well as @bianchi2018restructuring and @bianchi2018conscious. Those three reviews yielded `r initial_review_papers` additional papers. We then read and categorized papers from XX additional reviews (assisted greatly by @grundy2022, a review of reviews); From @harguess2020, @chang2023 and @wynes2018, we learned of `r second_review_papers` additional papers.

Third, We checked the CVs of prominent researchers in the field, which yielded `r cv_papers` additional papers.

Fourth, we conducted a snowball search where we read papers our assembled papers had cited, papers suggested to us by article homepages, and papers that cited articles already in our database. This yielded `r snowball_papers` papers.

Finally, we conducted a systematic search of Google Scholar for the terms [OUR SEARCH TERMS ONCE WE ARE DONE]. This yielded `r systematic_search_papers` more papers, bringing our total sample size to `r num_papers` papers and `r num_studies` interventions.[^3]

[^3]: In four cases, we condensed multiple interventions into one statistical result based on how they were reported in the paper: @abrahamse2007, @dijkstra2022, @lacroix2020 and @peacock2017. These studies all present substantively null results and, in their results sections, group multiple treatment arms into singular statistical results, which we recorded. 

## 3 The meta-analytic database

We first offer some descriptive statistics of our database and then provide a theoretical review.

### 3.1 Descriptive overview of meta-analytic database

```{r descriptive_stats, echo = F, include=F}
library(stringr)
library(purrr)
source('./functions/sum_tab.R')

# Data preparation and basic statistics
paper_dat <- dat |> 
  group_by(unique_paper_id) |> 
  slice(1) |>
  mutate(pub_status = case_when(
    str_detect(doi_or_url, "osf\\.io") ~ "preprint",
    str_detect(doi_or_url, "10\\.") ~ "publication",
    TRUE ~ "dissertation_or_tech_report"),
    decade = as.character(cut(year, breaks = c(1999, 2009, 2019, 2029), labels = c("2000s", "2010s", "2020s")))) |>
  ungroup()

# Decade Analysis
decade_tab <- paper_dat |> count(decade)
first_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2000s"])
second_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2010s"])
third_decade_papers <- as.numeric(decade_tab$`n`[decade_tab$decade == "2020s"])

# Publication Type Analysis
doi_tab <- paper_dat |> count(pub_status)
published_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "publication"])
preprint_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "preprint"])
other_papers <- as.numeric(doi_tab$`n`[doi_tab$pub_status == "dissertation_or_tech_report"])

# Non-DOI Paper Analysis
non_doi_paper_stats <- paper_dat |> filter(pub_status != "publication") |> count(venue)
dissertation_papers <- as.numeric(non_doi_paper_stats$`n`[non_doi_paper_stats$venue == "dissertation"])
advocacy_papers <- as.numeric(non_doi_paper_stats$`n`[non_doi_paper_stats$venue == "advocacy org publication"])

# Venue Analysis
venue_counts <- sort(table(paper_dat$venue), decreasing = T)
num_venues <- length(venue_counts)
most_common_venue <- noquote(names(sort(venue_counts, decreasing = TRUE)[1]))
most_common_venue_count <- max(venue_counts)

# Correcting Journal Article Counts
journal_article_counts <- paper_dat |> 
  filter(pub_status == "publication") |> 
  count(venue) |> 
  arrange(desc(n))

# Cluster Assigned Analysis
cluster_sample_stats <- dat |> 
  filter(cluster_assigned == "Y") |> 
  summarise(n = n(),
            median = median(total_sample, na.rm = TRUE), 
            min = min(total_sample, na.rm = TRUE), 
            max = max(total_sample, na.rm = TRUE))

# Non-Clustered Sample Size Statistics
non_cluster_sample_stats <- dat |> 
  filter(cluster_assigned == "N") |> 
  summarise(n = n(),
            median = median(total_sample, na.rm = TRUE), 
            min = min(total_sample, na.rm = TRUE), 
            max = max(total_sample, na.rm = TRUE))

# Intervention Types Count
intervention_types_counts <- dat |> 
  summarise(cafeteria_or_restaurant = sum(cafeteria_or_restaurant_based == "Y", na.rm = TRUE),
            multi_component = sum(multi_component == "Y", na.rm = TRUE),
            leaflet_count = sum(leaflet == "Y", na.rm = TRUE),
            video_count = sum(video == "Y", na.rm = TRUE),
            internet_based = sum(internet == "Y", na.rm = TRUE))

# Emotional Activation
emotional_activation_stats <- dat |> 
  summarise(emotional_activated = sum(emotional_activation == "Y", na.rm = TRUE), 
            emotional_not_activated = sum(emotional_activation == "N", na.rm = TRUE))

# Pre-Analysis Plan Stats
pre_analysis_plan_stats <- dat |> 
  summarise(pre_analysis_yes = sum(public_pre_analysis_plan != "N", na.rm = TRUE),
            pre_analysis_no = sum(public_pre_analysis_plan == "N", na.rm = TRUE))

# Open Data Stats
open_data_stats <- dat |> 
  summarise(open_data_yes = sum(open_data != "N", na.rm = TRUE),
            open_data_no = sum(open_data == "N", na.rm = TRUE))

# self-reported?
self_report_nums <- dat |> sum_tab(self_report)

# where do studies take place and with whom?
country_dat <- dat |> sum_tab(country)
pop_dat <- dat |> sum_tab(population)
# delay 
median_delay <- median(dat$delay)
range_delay <- range(dat$delay)
```

The earliest paper in our sample is @allen2002, and the latest is @jalil2023. Remarkably, a majority of these papers have been published since 2020: `r first_decade_papers` in the 2000s, `r second_decade_papers` in the 2010s, and `r third_decade_papers` in the 2020s.

Out of `r num_papers` total papers, `r published_papers` were published in peer-reviewed journals. `r dissertation_papers` are dissertations, one is a preprint, and four were published by advocacy organizations. Among journal articles, `r num_venues` different journals are represented. The most common venue is *`r most_common_venue`,* with `r most_common_venue_count` papers, followed by three in _Journal of Environmental Psychology_, and two apiece in _American Journal of Public Health_, _Frontiers in Psychology_, and _Sustainability_. The remainder were published in field journals for food and nutrition, psychology, and public health.

This database of studies uses methodologically diverse approaches.  `r intervention_types_counts$cafeteria_or_restaurant` studies were cafeteria or restaurant-based, `r intervention_types_counts$multi_component` used multi-component strategies, meaning that their treatments comprised multiple elements which could each have induced behavioral change. `r intervention_types_counts$leaflet_count` studies employed leaflets,  `r intervention_types_counts$video_count` used videos, and `r intervention_types_counts$internet_based` were administered online.

`r cluster_sample_stats$n` studies were assigned to clusters. This subset had a median of `r cluster_sample_stats$median` clusters in total, ranging from `r cluster_sample_stats$min` to `r cluster_sample_stats$max`. There were `r non_cluster_sample_stats$n` studies with assignment at the level of the individual, with a median sample size of `r non_cluster_sample_stats$median`, and a range from `r non_cluster_sample_stats$min` to `r non_cluster_sample_stats$max`.

`r emotional_activation_stats$emotional_activated` had interventions with clearly emotionally activating content, e.g. video footage of industrial farming.

In terms of open science practices `r open_data_stats$open_data_yes` studies made their data openly available, and `r pre_analysis_plan_stats$pre_analysis_yes` papers had publicly available pre-analysis plans.

A majority of studies (`r country_dat["United States"]` of `r num_studies`) take place exclusively in the United States. with a further `r country_dat["Italy"]` in Italy and `r country_dat["United Kingdom"]` in the United Kingdom. Germany had `r country_dat["Germany"]` studies,  Sweden, the Netherlands, Denmark, Canada, and Australia had one each, and one internet-based study had participants in the United States, United Kingdom, Canada, Australia, and "other" [@Cooney2016].

Most studies in this database were administered to university students: `r pop_dat["university students"]` of `r num_studies`. A further `r pop_dat["adults"]` were administered to adults, while `r pop_dat["young women"]` was delivered to young women (ages 13-25) and `r pop_dat["all ages"]` to people of all ages: one at a restaurant [@coker2022] and one online survey with an age range of 18 to 82 [@bochmann2017].

Regarding dependent variables, self-reported outcomes are the norm: `r `self_report_nums['Y']` of `r `num_studies` studies. Finally, the average delay between the start of treatment and outcome measurement is `r median_delay` days, with a range of `r range_delay[1]` to `r range_delay[2]` days.

### 3.2 The leading theoretical approaches to reducing MAP consumption 
```{r theory_tab, echo=F, include=F}
library(stringr)
source('./functions/sum_tab.R')
source('./functions/study_count.R')

theory_tab <- dat |> sum_tab(theory)
secondary_theory_tab <- dat |> sum_tab(secondary_theory)

norms <- dat |>   filter(str_detect(theory, "norms"))

norms |> select(author, year, title, theory, secondary_theory)

norms_paper_count <- norms |> study_count('unique_paper_id')

hybrid_norm_approaches <- norms |> 
  filter(str_detect(theory, "&"))

norm_nudge_approaches <- norms |> 
  filter(str_detect(theory, "nudge"))

```

Three approaches characterize the policy-relevant MAP reduction research: norms, nudges and persuasion. Persuasion approaches comprise three sub-categories: health, environmental, and animal welfare appeals. 

Many interventions explicitly combine multiple theoretical approaches, for example providing information about the environmental case for MAP reduction and also a norm-based message about the increasing popularity of vegetarian options [@piester2020]. Our counts of interventions by category equal to more than the total number of interventions in the database.

#### 3.2.1 Norms

Norms-based approaches inform `r nrow(norms)` of `r num_studies` interventions in our database and `r `norms_paper_count` papers. These interventions typically try to normalize vegetarianism and veganism by creating a perception that eating plant-based meals is consistent with social norms. To do this, researchers might put up signs that say “[More and more [retail store name] customers are choosing our veggie options" [@coker2022, p. 2] or "In a taste test we did at the [name of cafe], 95% of people said that the veggie burger tasted good or very good!" [@piester2020, p. 5]. One study told participants that people who eat meat are more likely to endorse social hierarchy and embrace human dominance over nature [@allen2002], thus making them out to be a counter-normative outgroup.

Two theoretical splits characterize this literature. The first is between _descriptive_ and _injunctive_ norms: while "descriptive norms pertain to the behavior of a majority of people (e.g. “most people eat vegetarian at least twice a week”), injunctive norms describe what behavior most others approve or disapprove of (e.g. “most people approve of eating vegetarian at least twice a week”)" [@alblas2023, p. 993]. For example, that study's injunctive norm intervention told Dutch households with unusually high meat consumption rates that they were negative outliers, along with a frowning face symbol. The other theoretical divide is between _dynamic_ and _static_ norms. Dynamic norms emphasize how behavior is changing over time, while static norms emphasize the current state of affairs. For example, @sparkman2017 presented people waiting in line at a Stanford cafeteria the opportunity to participate in a 'survey' in exchange for a meal coupon, where the survey presented one of two norms-based messages: a 'static' message about how "30% of Americans make an effort to limit their meat consumption," or a dynamic message emphasizing how behavior is changing over time and that vegetarianism is taking off among participants' peers.

Dynamic norm messages are the predominant approach in this subset of the literature. However, many studies in this literature combine multiple approaches into one intervention. @aldoh2023, for example, employ a 2x2 design where either a static norm or dynamic norm intervention conveying information about meat-eating habits in the United Kingdom is either combined with a visual cue (a graph showing the percentage of British people who took efforts to reduce their meat consumption either holding steady or going up over time) or not, These approaches are theoretically congruent, while other studies deliberately combine multiple approaches. In two cases, authors combined norms-appeals with explicit reasons to reduce MAP consumption. @hennessy2016 provided participants with one of two leaflets; the "why vegetarian" leaflet contained both "pictures of vegetarian celebrities in the why-focused leaflet to demonstrate a social norm" health, environmental, and animal welfare reasons to adopt a vegetarian diet. @lacroix2020 combine a descriptive social norm ("45% of Canadians are al- ready making efforts to reduce their consumption of meat") with environmental and health reasons to reduce consumption of red and processed meat.

Finally, `r nrow(norm_nudge_approaches)` deploy what we consider to be norms-based nudges in unobtrusive ways in dining halls. Two of four interventions in @piester2020 posted signs at a university canteen that said "In a taste test we did at the [name of cafe], 95% of people said that the veggie burger tasted good or very good!" [@piester2020, p. 5]; one of those intervention arms also included an environmental message: "Many people don’ know how much our food choices impact the environment. Eating sustainable foods is a good way we can fight climate change!" [@piester2020, p. 5]. @coker2022 placed a dynamic norm message about the growing popularity of veggie burgers in a chain of restaurants.

Whether these interventions are nudges is debatable. On the one hand, they seek to exploit a widespread heuristic of following group norms, and they do so by raising the salience of certain items in an unobtrusive way. On the other, They are plainly attempts to change underlying preferences by providing factual information. However, in these hybrid cases, the suggested change is implicit and can easily be ignored, which suggests a "libertarian paternalistic" perspective on behavior change [@thaler2003]. A further relevant distinction comes from @brachem2019, who argue that unlike descriptive norms, injunctive norms "work at least partly...through the implication of social sanctions" (p.9), which amount to incentives. Therefore, we coded norm-based interventions that that were cheap, unobtrusive, and descriptive as a nudge as well. This excludes, for instance, a survey with a norm message taken on the line to a dining hall because the delivery method is intrusive. obtrusive.

### 3.2.2 Nudges
```{r nudge_theory, include=F, echo=F}
nudge_studies <- dat |> filter(str_detect(theory, "nudge"))

hybrid_nudge_approach_number <- sum(str_detect(nudge_studies$theory, "&"))

norms_nudge_approaches <- nudge_studies |> filter(str_detect(theory, "norms"))

nudge_studies |> select(author, year, theory, secondary_theory)

pure_nudge_count <- sum(!str_detect(nudge_studies$theory, "&"))
print(dat |> filter(theory == "nudge") |> select(brief_description))
```

As discussed above, the precise boundaries around the idea of 'nudge' can be blurry. We refer to the original definition in @thaler2009, p. 6:

> A nudge...is any aspect of the choice architecture that alters people’s behavior in a predictable way without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid. Nudges are not mandates.

We also follow guidance from @hansen2016, who argues that a nudge should refer to "features that influence behaviour in ways not in accordance with that of economic rationality" (p.162) by making use of cognitive biases. By this definition,"rational persuasion or argument" [@ @hansen2016, p. 169] is not a nudge because it works on the conscious, deliberative mind rather than on automatic, prone-to-bias processes. 

By these standards, `r nrow(nudge_studies)` intervention in our database are nudges printed in `r length(unique(nudge_studies$doi_or_url))` papers. `r hybrid_nudge_approach_number` of these studies are combine nudges with social norm messages, as previously discussed. Further, two interventions in @piester2020 placed a message about the environmental benefits of reducing MAP consumption in a university canteen with no social norm message, thus combining nudge and persuasion approaches.

`r pure_nudge_count` study is a pure nudge. @andersson2021 [TODO: describe it]

This relative dearth of studies, specifically the lack of studies that _just_ nudge rather than combining nudges with another approach, might surprise researchers familiar with  the sprawling literature on nudges and diet. We attribute this mainly to our strict focus on MAP consumption as a dependent variable. This entailed excluding all studies that measured only hypothetical outcomes, which were startlingly common. For instance, @campbell2014 intercepted college students on their way to a dining hall, asked them to complete a survey, and then led them to another room where students were with different hypothetical menus comprised of options from the dining hall. Students told researchers which food they preferred and then left to go eat their meal, which the researchers did not track. As the authors put it, this design might “be critiqued for not providing actual food choices and thus lacking any real consequences” (p. 16). Along the same lines, @bacon2018 asked MTurk participants to “imagine a scenario in which they were catching up with a friend for dinner in a nice restaurant one evening during the week…they were also presented with an image of a cozy table in a restaurant” (p. 17). The main outcome was probability of selecting a vegetarian option from a hypothetical menu.

Second, we excluded studies that measured just red and/or processed meat consumption, e.g.  @carfora2017correlational and @carfora2017randomised, which sent text-based reminders to participants asking them to monitor their red or processed meat consumption.

We also excluded some interesting studies that assigned treatment at the level of a restaurant or dining hall and included too few units for meaningful analysis. [McClain et al. 2013](https://www.tandfonline.com/doi/abs/10.1080/07448481.2012.755189) and [Reinders et al. 2017](https://ijbnpa.biomedcentral.com/articles/10.1186/s12966-017-0496-9), for instance, test plausible theories of change in a college cafeteria and a chain of restaurants, respectively, but had just 2 and 3 units in their respective treatment arms. Along similar lines, some strong studies in dining halls with otherwise strong designs and plausible theories of change did not feature truly random assignment [@gravert2021; @garnett2020].s

Finally, we note that prior systematic reviews of the the nudge and diet literature include studies that have (what we consider to be) disqualifying threats to internal validity, e.g. differential screening procedures for treatment and control [@rees2018] or changes to treatment after assignment based on subjects' prior diets [@morren2021]. As @simonsohn2022 put it, combining "studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts" with high-quality studies creates averages that are "virtually guaranteed to lack a meaningful interpretation" (p. 551).

### Persuasion

We now turn to direct efforts to persuade people to reduce their MAP consumption. These studies generally provide direct arguments and relevant, factual arguments for dietary change. They are divided into three subcategories: environmental, health, and animal welfare appeals.

#### Appeals to the environment

These studies focus on the environmental harms of consuming animal products and/or promote plant-based alternatives as more sustainable. This framing informs X of Y interventions in our database.

Environmental arguments are communicated in a plethora of ways, including [leaflets](https://core.ac.uk/download/pdf/158315429.pdf), [signs in college cafeterias](https://doi.org/10.1016/j.appet.2020.104842), [op-eds](https://doi.org/10.1016/j.jenvp.2021.101592), [daily reminder text messages](https://doi.org/10.1016/j.jenvp.2021.101592), and [in-class lectures](https://assets.researchsquare.com/files/rs-2047134/v1_covered.pdf?c=1665164513).

The landmark study in this branch of the literature is [Jalil et al. (2023)](https://doi.org/10.1038/s43016-023-00712-1), who randomly assigned undergraduate classes to hear a “50 min talk about the role of meat consumption in global warming, along with information about the health benefits of reduced meat consumption,” and then measured their subsequent food choices at the college’s dining facilities. Overall, they find that students in the treatment group “reduced their meat consumption by 5.6 percentage points with no signs of reversal over 3 years.” No other study in this literature shows well-identified effects enduring this long.

### 3.1 Appeals to animal welfare

Direct appeals to eat less meat for the sake of animals’ wellbeing inform 13 of 42 interventions in our database. These interventions take many forms. Some studies, often run by animal advocacy organizations such as [Faunalytics](https://forum.effectivealtruism.org/topics/faunalytics), [The Humane League](https://forum.effectivealtruism.org/topics/the-humane-league), and [Mercy for Animals](https://forum.effectivealtruism.org/topics/mercy-for-animals), distribute [pamphlets](https://www.sciencedirect.com/science/article/abs/pii/S0195666322000721?via%3Dihub) or [booklets](https://osf.io/nwcgf) on animal welfare issues. Others have people watch advocacy [videos](https://mercyforanimals.org/blog/impact-study/), sometimes in [virtual](https://doi.org/10.31219/osf.io/fapu8) [reality](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3938994), or commercial media that touch on animal welfare issues, such as the movie [Babe](https://effectivethesis.org/wp-content/uploads/2022/03/THE-INFLUENCE-OF-MOVIE-ON-BEHAVIORAL-CHANGE-IN-INDIVIDUAL-MEAT-AND-DAIRY-PRODUCTS-CONSUMPTION.pdf) or the Simpsons episode [Lisa the Vegetarian](https://onlinelibrary.wiley.com/doi/10.1111/j.1747-0080.2010.01446.x). One paper investigates whether an animal’s [perceived cuteness affects people’s willingness to eat it](https://www.sciencedirect.com/science/article/abs/pii/S0195666317306190?via%3Dihub).

These studies share a commitment to the idea that people are motivated by animal welfare concerns, but otherwise draw from different disciplines, theories of change, and information mediums. However, as [Mathur et al. (2021 b)](https://www.sciencedirect.com/science/article/pii/S0195666321001847) note, social desirability bias in this literature is likely “widespread.” The typical study in this literature tries to persuade people that eating meat is morally unacceptable, often by confronting them with upsetting footage from factory farms, and then asks them a few weeks later how much meat they ate in the past week. [Experimenter demand effects](https://www.elgaronline.com/display/edcoll/9781788110556/9781788110556.00031.xml) are likely.

Two studies in this strand meet our inclusion criteria and also measure outcomes unobtrusively. The first is [Haile et al. (2021)](https://www.frontiersin.org/articles/10.3389/fpsyg.2021.668674/full), who distributed pro-vegan pamphlets on a college campus, tracked what students ate at their dining halls and find that “the pamphlet had no statistically significant long-term aggregate effects.” The second is [Epperson and Gerster (2021)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3938994), which shows that a “360° video about the living conditions of pigs in intensive farming via a virtual reality (VR) headset” led to a 6-9% reduction in purchased meals that contained meat in university canteens. 

#### Appeals to health

@lohmann2022, @klockner2017 recommended people eat other kinds of MAP...

Health concerns motivate X of Y interventions in our database. These studies are often conducted and/or written by medical doctors. For example, [Emmons et al. (2005 a)](https://aacrjournals.org/cebp/article/14/6/1453/258325/Project-PREVENT-A-Randomized-Trial-to-Reduce) assigned individuals who had received a “diagnosis of adenomatous colorectal polyps” (which are [potentially precancerous](https://www.mountsinai.org/health-library/diseases-conditions/colorectal-polyps)) to either a usual care condition or a tailored intervention that provided counseling and materials on risk factors for colorectal cancer, “including red meat consumption, fruit and vegetable intake, multivitamin intake, alcohol, smoking, and physical inactivity.” The outcome variable was self-reported servings of red meat per week measured at an 8-month follow-up, which the authors dichotomized into “more than three servings per week of red meat” or not. Overall, there was an 18% reduction in this outcome for the treatment group and a 12% reduction for the usual care condition.

One challenge to integrating these results into our database is that they often focus on red meat and/or processed meat consumption rather than MAP as a whole. There is a risk, therefore, that these interventions might induce substitution towards other animal-based products. [Klöckner & Ofstad (2017)](https://doi.org/10.1016/j.jenvp.2017.01.006), for example, recommended that subjects “substitute beef with other meats or fish,” which are arguably worse for animal welfare on a [per-ounce-of-meat](https://slatestarcodex.com/2015/09/23/vegetarianism-for-meat-eaters/) [basis](https://www.vox.com/future-perfect/22430749/beef-chicken-climate-diet-vegetarian) (see [Mathur 2022](https://www.science.org/doi/10.1126/science.abo2535) for further discussion). We did not include this study in our meta-analytic database because we cannot clearly deduce its overall effect on MAP consumption. However, we included all red and/or processed meat consumption studies that did not specifically argue for switching to other MAP. (See [Matt-Navarro and Sparkman (forthcoming)](https://static1.squarespace.com/static/5e90e46fd1119766887d1dc3/t/646cc4f6d0b87674aa83a548/1684849910918/FishNormsInterventionBrief.pdf) for research targeting fish consumption.)



### Strengths and weaknesses of the literature as a whole
#### Regression to the meat: the lack of long-term outcome measurement in choice architecture approaches
- Alter one environment, how do people compensate? Do they? To track this -- to assess general equilibrium effects -- you need to track both what people at and also what they do after they leave the dining hall. No study in our dataset does this

#### Social desirability bias..
...Consider @fehrenbach2015, a dissertation that tested the “effectiveness of two video messages designed to encourage Americans to reduce their meat consumption.” The study had two treatment arms and a control. Both treatment videos sought to induce a feeling of “high threat” by informing viewers of the “the negative health effects of high meat consumption;” one video also sought to induce feelings of “high efficacy” by suggesting “easy ways to reduce their meat consumption,” while the “low efficacy” group’s video “only included a very minor efficacy component in the conclusion.” The videos were 7 and 4 minutes long, respectively. Before the study, on the day of the study, and again a week later, participants were asked about their attitudes and intentions towards eating meat, as well as how much meat they’d eaten in the past 7 days.

Overall, the high threat/high efficacy group reported that they ate an average of 3.16 fewer meals involving meat in the week following the intervention than the one before it, compared to 2.11 for the high threat/low efficacy group and 1.92 for the control group. As a benchmark, the population ate meat at an average of 13.64 meals per week before the intervention (SD = 4.21).

This study strikes us as having a high risk of social desirability bias for three reasons.

First, the study is designed to make people feel a sense of “high threat” from eating meat, and then asks them a week later about how much meat they ate. There are grounds for doubting how much respondents would accurately recount their eating habits. This problem is typical of this literature.

Second, the study asks participants to recall a week’s worth of meals; previous research has found that [daily food diaries lead to more accurate reports](https://pubmed.ncbi.nlm.nih.gov/7635601/). As [Mathur et al. (2021a)](https://www.sciencedirect.com/science/article/pii/S0195666321001847) put it:

> Many existing studies measure meat consumption in terms of, for example, Likert-type items that categorize the number of weekly meals containing meat (e.g., “none”, “1–5 meals”, etc.) or in terms of reductions from one’s previous consumption. When possible, using finer-grained absolute measures, such as the number of servings of poultry, beef, pork, lamb, fish, etc., would enable effect sizes to be translated into direct measures of societal impact.

Third, the decline in meat-eating among the control group suggests that the intended direction of the experiment might have been crystal clear to everyone, whether they watched the video or not.

In sum, using broad stroke, self-reported outcomes in a context where meat is being presented as bad for you seems like a high-risk environment for [experimenter demand effects](https://www.elgaronline.com/display/edcoll/9781788110556/9781788110556.00031.xml).

#### intermeat substitution

However, a tricky case emerged in studies that attempted to reduce red or processed meat consumption rather than MAP consumption as a whole. Some of these studies recommended that people substitute to other MAP, such as chicken or fish, but none to our knowledge measured any outcome besides red and/or processed meat. We decided to include studies focused on red and/or processed meat so long as they did not specifically advise inter-MAP substitution.


## Meta-analytic Methods

- Something or does this go into the appendix?

1. We take the outcome that most clearly maps to changes in actual consumption behavior. 
2. We take the latest possible outcome to test for the presence of enduring effects, and our sample sizes are taken from the same measurement wave.
3. For cluster-assigned treatments, our Ns are the number of clusters rather than participants. This includes studies that cluster by day (e.g. everyone who comes to a restaurant on some day gets treated).
4. We convert all effect sizes to estimates of standardized mean differences: Average Treatment Effect (ATE) / standard deviation (SD).
5. When possible, we calculate ATE using [difference in differences](https://edge.edx.org/assets/courseware/v1/b8d2a8030b7aa5f2762a464bf7f8b0c7/c4x/BerkeleyX/CEGA101AIE/asset/Module_2.5_Difference_in_Differences.pdf) (DiD) — ((Treatment_posttest - Treatment_pretest) - (Control_posttest - Control_pretest)) — which we prefer to difference in means (DiM) — (Treatment_posttest - Control_posttest) — because it leads to more precise estimates. When pretest scores aren’t available, we use DiM.
6. When possible, we standardize by the SD of the control group, a measure called Glass’s Delta (written as $\Delta$). We prefer Glass’s Delta to Cohen’s d, a measure that standardizes by the SD of the entire sample, because we want to to avoid any additional assumptions about equivalence of variance between treatment and control groups. When we don’t have enough information to calculate $\Delta$, we use d.
7. For conversions from statistical tests like t-test or F-test, we use standard equations from [Cooper, Hedges and Valentine (2009)](https://psycnet.apa.org/record/2009-05060-000). The one exception is our difference in proportions estimator, which, to the best of our knowledge, Donald P. Green first proposed for a [previous meta-analysis](https://doi.org/10.1146/annurev-psych-071620-030619). See our [d_calc.R function on GItHub for specifics](https://github.com/setgree/vegan-meta/blob/main/functions/d_calc.R).
8. When authors tell us that results were “not significant” but don’t specify more precisely, we call the effect type "unspecified null" and record it as $\Delta$ = 0.01.

Our meta-analysis employs a random effects model rather than a fixed effects model, for reasons explained [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9393987/). Our code mainly uses functions from the metafor and tidyverse packages in R as well as some custom wrappers that Seth wrote for previous meta-analyses.

The coefficients we report are: 

* Glass’s Delta ($\Delta$), which indicates a meta-analytic estimate: a weighted average of many point estimates, where larger, more precisely estimated studies influence the average proportionally more than smaller ones.
* Beta (β), a regression coefficient. Linear regression tells you E(Y | X), i.e. for some value of X (or many Xs), the expected value of Y. We report this coefficient when we discuss publication bias or the effects associated with covariates. 
* SE (standard error) is the standard deviation of the sampling distribution, e.g. $\Delta$ or β. Smaller standard errors mean more precise estimates.
* P-value, a measure of statistical significance. See [Lakens (2022), chapter 1](https://lakens.github.io/statistical_inferences/01-pvalue.html) for an overview.

## 5 Meta-analysis

```{r main_results, echo=F, include=F}
library(metafor, quietly = T)
library(ggplot2, warn.conflicts = F)
library(knitr)
library(purrr)
library(sessioninfo)
library(stringr)

source('./functions/d_calc.R')
source('./functions/var_d_calc.R')
source('./functions/study_count.R')
source('./functions/map_robust.R')
#' count_and_robust helper function;
#' necessary b/c `filter |> split |> imap()` didn't work for some reason

count_and_robust <- function(data) {
      bind_cols(study_count(data), map_robust(data))
  }
dat <- dat |> mutate(d = mapply(
        FUN = d_calc,
        stat_type = eff_type,
        stat =  u_s_d,
        sample_sd = ctrl_sd,
        n_t = n_t_post,
        n_c = n_c_post),
      var_d = mapply(
        FUN = var_d_calc,
        d = d,
        n_c = n_c_post,
        n_t = n_t_post),
      se_d = sqrt(var_d)) |> 
  select(-X)

main_result <- dat |> count_and_robust()

# sign test
sign_table <- table(dat$neg_null_pos)
sign_table_test <-  binom.test(sign_table[[3]], sum(sign_table[1:3]))
# I don't think we need the rest of this but here's some approaches to 
'the probability of a positive result is distinguishable from a coin flip'

null_proportion <- sign_table[[2]] / sum(sign_table)
sign_table_test <- binom.test(sign_table[[3]], sum(sign_table), p = null_proportion)
dat$positive_numeric <- ifelse(dat$neg_null_pos == 1, 1, 0)
t_test_result <- t.test(dat$positive_numeric, mu = 0.5, alternative = "greater")
# something like this

# processed and red meat?
dat |> 
  split(str_detect(dat$outcome, "red meat") | str_detect(dat$outcome, "processed meat")) |> 
  map(count_and_robust)

```

Our meta-analysis of `r num_studies` policy-relevant interventions yields an overall meta-analytic effect of `r `main_result$Delta` (se = `rmain_result$se`), p = `r main_result$pval`. Though statistically significant, this effect does not meet the conventional standard for a 'small' result in the behavioral sciences. This stands in marked contrast to previous reviews, which found [PUT IN RESULTS OF PREVIOUS METAS]. This diverge seems to be a product of our inclusion criteria, specifically the requirement that studies measure actual consumption behavior rather than self-reported intentions or hypothetical choices. [say more about this].

Another way to assess the overall effect of this literature is with a vote counting procedure, which counts the prevalence of significant positive, null, and significant negative results in the words of the papers themselves. In our sample, `r sign_table[[3]]` studies had a positive effect, `r sign_table[[2]]` had a negative effect, and `r sign_table[[1]]` had a null effect. In other words, the average study in our sample finds a null result.  [do we need a statistical test here? I think not because it's so obvious]


The dashed line in the figure below represents the overall meta-analytic effect, and the black points represent individual studies. The size of the points corresponds to the precision of the estimate, with larger points indicating more precise estimates. The color of the points corresponds to the theoretical approach of the intervention. 

```{r fig, echo=F}
source('./functions/map_robust.R')
count_and_robust <- function(data) {
      bind_cols(study_count(data), map_robust(data))
  }

# figure
fig <- dat |>
  mutate(theory_category = if_else(
    str_detect(theory, "&"), "hybrid", theory), 
    study_name = reorder(as.factor(paste0(author, " ", year)), desc(se_d))) |>
  ggplot(aes(y = study_name, x = d, 
             xmin = d - (1.96 * se_d),
             xmax = d + (1.96 * se_d))) + 
  geom_point(size = 1, aes(color = theory_category)) +
  geom_errorbarh(height = .1, aes(color = theory_category)) +
  geom_vline(xintercept = 0, color = "black", alpha = .5) +
  geom_vline(xintercept = (dat |> map_robust())$Delta, 
             color = 'black', lty = 'dashed') +
  scale_x_continuous(name = expression(paste("Glass's", " ", Delta))) +
  scale_y_discrete() +
  ylab("Study") +
  ggtitle("Vegan m eta forest plot") + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        legend.title = element_text(size = 15),
        legend.text = element_text(size = 12))
# robustness checks: pre-analysis plan
pap_split <- dat |> mutate(has_pap = if_else(public_pre_analysis_plan == 'N', 0, 1)) |> 
  split(~has_pap) |>  
  map(count_and_robust)
pap_split[[1]]$Delta
# figure

```
## Conclusion
PRICE STUDIES
Look at Garnett studies and also [Vellinga et al. (2022)](https://doi.org/10.1186/s12889-022-13535-9)
### Bibliography

