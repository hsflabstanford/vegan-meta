---
title: "Vegan meta-analysis pre-analysis plan"
author: "Seth Green"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  word_document: default
editor_options:
  chunk_output_type: console
---

# Some priors we have
* If there was a durable, easy-to-scale method for reducing meat consumption, youâ€™d already know about it. It would be news the way [Wegovy is news](https://nymag.com/press/2023/02/people-have-swapped-their-old-diets-for-a-dose-of-ozempic.html).

* Putative effects from observational research often disappear when put to a [rigorous, randomized test](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1330479). [Confounders abound](https://www.jstor.org/stable/2676645).
* Attitudes and intentions are [much more malleable](https://thehumaneleague.org/article/gfu-rcts) than behavior, and on sensitive subjects, people sometimes say one thing and do another. This has been [well understood for decades](https://www.jsad.com/doi/abs/10.15288/jsa.1981.42.355?journalCode=jsa).

* Anything touching on food is a sensitive, personal subject, so we should expect significant biases in studies that focus on attitudes and intentions.

# Guidelines for data collection 

Before mock analyses, here are some general principles we followed when coding studies.

* We take the outcome that most clearly maps to changes in actual consumption behavior. 

* We take the latest possible outcome to test for the presence of enduring effects. 

* N of treatment and N of control are from the latest possible post-test as well.

* We use Glass's $\Delta$ rather than Cohen's *d*, meaning we standardize by the SD of the control group at baseline whenever possible. This is to avoid any additional assumptions about equivalence of variance between treatment and control groups. When studies don't provide enough information to estimate the control group's SD, we use combined sample estimates of population variance or statistical tests that implicitly reflect this information.

* For cluster-assigned treatments, our Ns are the # of clusters rather than participants. This includes studies that cluster by day (e.g. everyone who comes to a restaurant on some day gets treated).

* when authors don't tell us enough to calculate Glass's $\Delta$ but they do tell us that the results were null, we call the effect type "unspecified null" and record it as $\Delta$ = 0.01.

* For binary variables, we treat the incidence of the event in the treatment group and control group as draws from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) whose variance is $p*(1-p)$, and whose standard deviation is (therefore) $\sqrt{p * (1-p)}$. Here, $p$ is the incidence of the dependent variable in the control group as a percentage. If the DV is "proportion of vegetarian meals,  if the control group ordered 100 meals and 60% of them were vegetarian, then $p = 0.4$. So if the treatment group proportion is $p_1$ and the control group proportion is $p_2$,  Glass's $\Delta$ is $\frac{p_1 - p_2}{\sqrt{p_2 * (1 -p_2)}}$

# data analysis on a simulated dataset

Our data analysis will basically look like this in broad outline, except with real data. This simulated dataset has more or less the same variables as our real dataset. There will be a few differences, e.g. we'll probably do something more complicated to isolate the leaflet studies (or else modify the dataset). But in general, these are the main points.

Anything else we do is exploratory based on trends we observe in the data once we actually start the analysis.

**libraries, functions, options, and simulated data**

```{r setup, message=F}
#' libraries
library(metafor, quietly = T)
library(dplyr, warn.conflicts = F)
library(ggforestplot) # https://nightingalehealth.github.io/ggforestplot
library(ggplot2, warn.conflicts = F)
library(knitr)
library(purrr)
library(sessioninfo)
library(stringr)

#' functions
source('./functions/d_calc.R')
source('./functions/map_robust.R')
source('./functions/sum_lm.R')
source('./functions/var_d_calc.R')

#' options and reproducibility
options(scipen = 99)
set.seed(11111988)

```

make a dataset
```{r make_dataset}
 dat <- data.frame(unique_study_id = 1:24,
                  study_name = c('a study', 'b study', 'c study', 'd study', 
                                 'e study', 'f study', 'g study', 'h study', 
                                 'i study', 'j study', 'k study', 'l study',
                                 'm study', 'n study', 'o study', 'p study',
                                 'q study', 'r study', 's study', 't study',
                                 'u study', 'v study', 'w study', 'x study'),
                  u_s_d  = abs(rnorm(24, 1, 1)),
                  ctrl_sd = abs(rnorm(24,4,1)),
                  eff_type = sample(c("d_i_m", "reg_coef", "d_i_d"), 24, T),
                  theory = as.factor(sample(
                    c("persuasion", "psychology", "behavioral economics"), 
                    24, replace = T)),
                  cluster = as.logical(rbinom(24, 1, 0.4)),
                  self_report = as.logical(rbinom(24, 1, 0.2)),
                  leaflet     = as.logical(rbinom(24, 1, 0.2)),
                  doi_or_url = as.logical(rbinom(24, 1, 0.2)),
                  delay = sample(1:30, 24, replace = T),
                  year = sample(2005:2023, 24, replace = T),
                  pos_neg_neutral = sample(-1:1, 24, replace = T),
                  population = sample(c("college students", "high schoolers", 
                                       "adults", "children"), 24, replace = T),
                  country = sample(c("internet", "United States", "Italy", "UK"), 
                                   24, replace = T)) |> # add new variables that depend on previous variables
  mutate(n_t_post = ifelse(cluster == 1, 
                           sample(5:100, sum(cluster == 1), replace = TRUE),
                           sample(25:1000, sum(cluster == 0), replace = TRUE)),
         n_c_post = ifelse(cluster == 1, 
                           sample(5:100, sum(cluster == 1), replace = TRUE),
                           sample(25:1000, sum(cluster == 0), replace = TRUE)),
         decade = as.factor(case_when(year <= 1989 ~ "1980s",
                                      year >= 1990 & year <= 1999 ~ "1990s",
                                      year >= 2000 & year <= 2009 ~ "2000s",
                                      year >= 2010 ~ "2010s",
                                      TRUE ~ "Other")),
         persuasion_category = 
           case_when(theory == "persuasion" ~ 
                       sample(c("animal welfare", "environment", "health"), 
                              n(), replace = T), 
                     TRUE ~ NA)) |> 
  mutate(d = mapply( # add d, var_d and se_d (these can be integrated into the previous mutate chunk)
    FUN = d_calc,
    stat_type = eff_type,
    stat =  u_s_d,
    sample_sd = ctrl_sd,
    n_t = n_t_post,
    n_c = n_c_post),
    var_d = mapply(
      FUN = var_d_calc,
      d = d,
      n_c = n_c_post,
      n_t = n_t_post),
    se_d = sqrt(var_d))
```

### 4.0 descriptive stats
mean N, country table, and population

```{r descriptive_stats}
dat |> group_by(cluster) |> 
  summarise(pop_total = mean(n_c_post) + mean(n_t_post))
table(dat$country)
table(dat$population)
```

### 4.1 What is the overall average meta-analytics effect?

```{r avg_eff}
dat |> map_robust()
```

Fig 1: forest plot

```{r forest_plot}
dat |> arrange(var_d) |>  ggforestplot::forestplot(study_name, 
                         estimate = d, se = se_d, 
                         colour = theory, 
                         xlab = expression(paste("Glass's", " ", Delta)), 
                         ylab = "Study",
                         title = "Forest Plot") +
    geom_vline(xintercept = (dat |> map_robust())$beta, 
                            lty = 'dashed') +  # color = '#00BFC4'
  theme(plot.title = element_text(hjust = 0.5))
```

Sign test (how many studies were positive, negative, or neutral in their authors' own words)

```{r sign_test}
sign_table <- table(dat$pos_neg_neutral)
binom.test(sign_table[[3]], sum(sign_table[1:3]))

```

### 4.2 publication bias
```{r pub_bias}
dat |> sum_lm()

dat |> ggplot(aes(d, se_d)) + 
  geom_point(aes(color = theory)) + 
  stat_smooth(method = 'lm', se = F, color = 'grey', 
              lty = 'dashed', linewidth = 1) + 
  theme_minimal() +
  ggtitle('relationship between effect size and standard erors')

#' split by published/unpublished (DOI = published)

dat |> split(~doi_or_url) |> map(sum_lm) 
dat |> split(~doi_or_url) |> imap(~map_robust(.x) |> kable('markdown')) 
```

### 4.3 differences by clustering, theory, persuasion theory, and self-report? 

```{r cluster}

dat |> split(~cluster) |> imap(~map_robust(.x) |> kable('markdown'))
dat |> split(~theory) |> imap(~map_robust(.x) |> kable('markdown'))
dat |> split(~self_report) |> imap(~map_robust(.x) |> kable('markdown'))

dat |> filter(theory == 'persuasion') |> 
  split(~persuasion_category) |> 
  imap(~map_robust(.x) |> 
         kable('markdown'))
```

### 4.4 Social desirability penalty
Mathur et al. (2021b) provide a nice estimate of the magnitude of social desirability bias in self-reported outcome research of about $\beta$ = 0.1. What if we discount all self-reported outcomes bhy about that much, what do the overall effects look like?

```{r social_desirability_penalty}
dat |>  mutate(d =  if_else(self_report, d - 0.1, d)) |>map_robust()
```

### 4.5 do leaflet studies work? 

```{r leaflet}
dat |> filter(leaflet) |> map_robust() |> kable('markdown')
```

### 4.6 How do studies administered online compare to in-person studies?

```{r}
# effect size comparison
dat |> split(~str_detect(country, "internet")) |> map(map_robust)

# simple null vs not comparison (not simple to code, thank you chatGPT)
dat %>%
  mutate(country_group = ifelse(grepl("internet", country), "Internet", "Other")) %>%
  group_by(country_group) %>%
  do({
    test_result <- binom.test(sum(.$pos_neg_neutral == 1), length(.$pos_neg_neutral), conf.level = 0.95)
    tibble(
      estimate = test_result$estimate,
      conf_low = test_result$conf.int[1],
      conf_high = test_result$conf.int[2],
      p_value = round(test_result$p.value, 5)
    )
  })
```

### 4.7 Any relationship between delay and outcome?

```{r delay}
dat |> sum_lm(d, delay)
dat |> ggplot(aes(d, delay)) + geom_point() + 
  stat_smooth(method = 'lm', se = F) + theme_minimal()
```

### 4.8 any relationship between publication date and effect size or validity?

```{r publication_date}

dat |> sum_lm(d, year)

dat |> split(~decade) |>  
  map(map_robust)

```

### 4.9 record computational environment
```{r reproducibility}
session_info()
```