---
classoptions: 
  - sn-nature      
  - referee         # Optional: Use double line spacing 
  # - lineno        # Optional: Add line numbers
  # - iicol         # Optional: Double column layout
title: "Meaningfully reducing consumption of meat and animal products is an unsolved problem: results from a meta-analysis"
titlerunning: MAP-reduction-meta
authors: 
  - firstname: Seth Ariel
    lastname: Green
    email: setgree@stanford.edu
    affiliation: 1
    corresponding: TRUE
  - firstname: Maya B.
    lastname: Mathur
    affiliation: 1
  - firstname: Benny
    lastname: Smith 
    affiliation: 2
affiliations:
  - number: 1
    info:
      orgdiv: Humane and Sustainable Food Lab
      orgname: Stanford University
  - number: 2
    info:
      orgname: Allied Scholars for Animal Protection 
keywords:
  - meta-analysis
  - meat
  - plant-based
  - randomized controlled trial
  
abstract: |
  Which theoretical approach leads to the broadest and most enduring reductions in consumptions of meat and animal products (MAP)? We address these questions with a theoretical review and meta-analysis of rigorous randomized controlled trials with consumption outcomes. We meta-analyze 36 papers comprising 42 studies, 114 interventions, and approximately 88,000 subjects. We find that these papers employ four major strategies to changing behavior: choice architecture, persuasion, psychology, and a combination of persuasion and psychology. The pooled effect of all 114 interventions on MAP consumption is $\Delta$ = 0.065, indicating an unsolved problem. Reducing consumption of red and processed meat is an easier target: $\Delta$ = 0.249, but because of missing data on potential substitution to other MAP, we canâ€™t say anything definitive about the consequences of these interventions on animal welfare. We further explore effect size heterogeneity by approach, population, and study features. We conclude that while no theoretical approach provides a proven remedy to MAP consumption, designs and measurement strategies have generally been improving over time, and many promising interventions await rigorous evaluation.
date: "`r Sys.Date()`"
output: 
  rticles::springer_article:
    keep_tex: true
bibliography: "./vegan-refs.bib"
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{comment}
  - \usepackage{anyfontsize}
  - \usepackage{caption}
---

```{r setup, include=FALSE}
# so that knitr labels figures
knitr::opts_chunk$set(fig.path = "./figures/",
                      echo = FALSE,
                      out.extra = "")

# directory modifications so we can put the manuscript stuff in its own folder
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(rprojroot::find_rstudio_root_file())
options(tinytex.clean = TRUE) # switch to FALSE to get the bbl file for overleaf

source('./scripts/libraries.R')
source('./scripts/functions.R')
source('./scripts/load-data.R')
source('./scripts/red-and-processed-meat.R')
source('./scripts/models.R')
source('./scripts/tables.R')
source('./scripts/figures.R')
```

# Introduction {#sec1}

Reducing global consumption of meat and animal products (MAP) is vital to reducing chronic disease and the risk of zoonotic pandemics [@willett2019; @landry2023; @hafez2020], abating environmental degradation and climate change [@scarborough2023; @koneswaran2008; @greger2010], and improving animal welfare [@kuruc2023; @scherer2019].
However, MAP is widely regarded as normal, necessary, and a dietary staple [@piazza2022; @milford2019].
Global MAP consumption is increasing annually [@godfray2018] and expected to continue doing so [@whitton2021].

There is a vast and diverse literature investigating potential means to reverse this trend.
Example approaches include providing free access to meat substitutes [@katare2023], changing the price [@horgen2002] or perceptions [@kunst2016] of meat, or attempting to persuade people to change their diets [@bianchi2018conscious].
A large portion of this literature seeks to alter the contexts in which MAP is selected [@bianchi2018restructuring], for instance by changing menu layouts [@bacon2018; @gravert2021] or placing vegetarian items more prominently in dining halls [@ginn2024].
Some interventions are associated with large impacts [@hansen2021; @boronowsky2022; @reinders2017], and prior reviews have concluded that some frequently studied approaches, such as making vegetarian meals the default [@meier2022] and using persuasive messaging that appeals to animal welfare [@mathur2021meta], may be consistently effective. [[LINKING SENTENCE ABOUT HOW PRACTICIONIERS ARE BEGINNING TO PUT THESE IDEAS INTO PRACTICE -- WRI REPORT]]

However, much of this literature is beset by design and measurement limitations.
Many interventions are either not randomized [@garnett2020] or underpowered [@delichatsios2001].
Others record outcomes that are imperfect proxies MAP consumption, such as attitudes and intentions [@mathur2021effectiveness], yet behaviors often do not track with these psychological processes [@porat2024].
Further, many studies measure only immediate impacts [@hansen2021; @griesoph2021] rather than longer-term effects, or focus on hypothetical choices [@raghoebar2020; @vermeer2010].
Last, numerous studies that aim to reduce consumption of red and processed meat (RPM) may induce people to switch to other forms of MAP, such as chicken or fish [@grummon2023].
While RPM is of special concern for health and greenhouse gas emissions [@abete2014; @lescinsky2022], increasing chicken or fish consumption may lead to substantially worse outcomes for animal welfare [@mathur2022ethical], and fails to reduce the risk of zoonotic outbreaks from factory farms [@hafez2020] or land and water pollution [@grvzinic2023].

In the past few years, a new wave of MAP reduction research has made commendable methodological advances in design, outcome measurement validity, and statistical power.
Historically, in some scientific fields, strong effects detected in early studies with methodological limitations were ultimately overturned by more rigorous follow-ups [@wykes2008; @paluck2019; @scheel2021].
Does this phenomenon hold in the MAP reduction literature as well?

We answer this question with a meta-analysis of rigorously designed RCTs aimed at creating lasting reductions in MAP consumption [@andersson2021; @kanchanachitra2020; @abrahamse2007; @acharya2004; @banerjee2019; @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @cooney2014; @cooney2016; @feltz2022; @haile2021; @hatami2018; @hennessy2016; @jalil2023; @mathur2021effectiveness; @merrill2009; @norris2014; @peacock2017; @polanco2022; @sparkman2021; @weingarten2022; @piester2020; @aldoh2023; @allen2002; @camp2019; @coker2022; @sparkman2020; @berndsen2005; @bertolaso2015; @fehrenbach2015; @mattson2020; @shreedhar2021].
These RCTs all measured consumption outcomes at least a single day after treatment was first administered, and all had at least 25 subjects in both treatment and control, or, in the case of cluster-assigned studies, at least ten clusters in total. [[LINKING SENTENCE ABOUT RPM STUDIES]]

Studies in our meta-analytic database pursued one of four theoretical approaches: choice architecture (the manipulation of how MAP is presented to diners), psychological appeals (typically manipulations of perceived norms around eating meat), explicit persuasion (centered around animal welfare, the environment, and/or health), or a combination of psychological and persuasion messages.
Interventions varied in delivery method, for example, documentary films [@mathur2021effectiveness], leaflets [@peacock2017], university lectures [@jalil2023], op-eds [@haile2021], and changes to menus in cafeterias [@andersson2021] and restaurants [@coker2022; @sparkman2021].

We estimated overall effect sizes as well as effect sizes associated with different theoretical approaches and delivery mechanisms.
Although we find some heterogeneity across theories and mechanisms, we find consistently smaller effects on MAP consumption than previous reviews have suggested [@bianchi2018restructuring; @byerly2018; @chang2023; @harguess2020; @kwasny2022; @mathur2021meta; @meier2022; @pandey2023], with some intriguing exceptions.
Thus, contradicting previous reviews that analyzed a wider array of designs and outcomes, we conclude that meaningfully reducing MAP consumption is an unsolved problem.
However, many promising approaches still await rigorous evaluation.

```{r useful_constants}
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
num_interventions <- as.numeric(nrow(dat))

n_total <-  round_to(x = sum(dat$n_c_total_pop) + sum(dat$n_t_total_pop), 1000, floor)

decade_tab <- dat |> group_by(unique_paper_id) |>  slice(1) |>  ungroup() |> count(decade)
```

```{r robust_data, include=F}
source('./scripts/robustness-check.R')
```

```{r red_meat_numbers, include=F}
RPMC_papers <- as.numeric(max(RPMC$unique_paper_id))
RPMC_studies <- as.numeric(max(RPMC$unique_study_id))
RPMC_interventions <- as.numeric(nrow(RPMC))
n_meat_total <-  plyr::round_any(x = sum(RPMC$n_c_total_pop) + sum(RPMC$n_t_total_pop), 1000, floor)

# also supplementary_numbers
excluded_count <- nrow(read.csv('./data/excluded-studies.csv'))


```

# Results {#sec2}

## An overall small effect with some heterogeneity by approach {#sec2.1}

Our overall meta-analytic effect size is $\Delta$ = `r model$Delta` (SE = `r model$se`), p = `r model$pval`.
The aggregate effect is statistically significant, but does not indicate a meaningful, enduring reduction.

Figure 1 displays the distribution of effect sizes, grouped by paper, with each individual point representing an intervention.
The overall effect size is plotted at the bottom.

```{r forest_plot, out.width='120%'}
forest_plot
```

Table 1 reports the number of studies, interventions, and approximate subjects, as well as the pooled effect size, per theoretical approach.

`r table_one`

Table 2 displays the numbers and findings of persuasion interventions by topic.

`r table_two`

These small effects may surprise readers of previous reviews, which typically found more positive results [@mathur2021meta; @meier2022; @mertens2022].
We attribute this difference to our stricter inclusion criteria.
For instance, of the ten largest effect sizes recorded in [@mathur2021effectiveness], nine were non-consumption outcomes and the tenth came from a non-randomized design.

```{r nulls_and_cis}
effect_size_table <- dat |> sum_tab(neg_null_pos)

significant_CIs <- dat |> select(author, year, d, se_d, neg_null_pos) |>
  mutate(lower_bound = d - (1.96 * se_d),
         upper_bound = d + (1.96 * se_d)) |>
  filter(lower_bound < 0 & upper_bound < 0 |
           lower_bound > 0 & upper_bound > 0) |> 
  mutate(direction = case_when
         (lower_bound > 0 ~ 1,
           upper_bound < 0 ~ 0))

# pap analysis
pap_model <- dat |> filter(public_pre_analysis_plan != 'N') |> map_robust()

## this difference is not statistically significant
dat <- dat |> mutate(pap_yes = if_else(public_pre_analysis_plan == 'N', FALSE, TRUE))

pap_difference <- robumeta::robu(d ~ pap_yes, dat, unique_study_id, var_d)

# open data analysis
open_data_model <- dat |> filter(open_data != 'N') |> map_robust()

dat <- dat |> mutate(open_data_yes = if_else(open_data == 'N', FALSE, TRUE))

open_data_difference <- robumeta::robu(d ~ open_data_yes, dat, unique_study_id, var_d)
```

Per the papers' own calculations and data, `r effect_size_table['0']` of `r num_interventions` interventions had null effects on net MAP consumption.
However, many studies present a wide variety of outcomes and present significant results as well.

\begin{comment}
Could put this back in: "Using our calculations of effect size and standard error `r (nrow(significant_CIs))` interventions have 95% confidence intervals that do not overlap with zero, `r sum(significant_CIs$direction)` of which are positive effects, out of `r nrow(dat)` interventions." But I think it's not necesssarys
\end{comment}

## Moderate evidence of publication bias {#sec2.2}

We conduct four tests for publication bias.
None is conclusive.

\begin{comment} 
Could put in introductory remarks about how this puts our main results in one light or another? 
\end{comment}

First, in our dataset, effect size and standard error are positively correlated, though not significantly.

Second, the `r pap_model$N_studies` studies with pre-analysis plans have a smaller effect: $\Delta$ = `r pap_model$Delta` (SE = `r pap_model$se`), p = `r pap_model$pval`.

Third, the `r open_data_model$N_studies` studies with openly available data also have a smaller effect: $\Delta$ = `r open_data_model$Delta` (SE = `r open_data_model$se`), p = `r open_data_model$pval`.

Fourth, the pooled effect size of interventions published in peer-reviewed journals is larger than the pooled effect size from advocacy organization publications, preprints, and student theses.

We consider these tests to be moderate evidence of publication bias in this literature.
On the other hand, the sheer quantity of null results suggests that finding small, insignificant effects on MAP consumption is not a major barrier to publication.

`r table_three`

```{r pbulication_bias_checks, include=F}

# "$\Delta$ and standard error are positively correlated, though not significantly"
dat |> sum_lm(d, se_d)
# a la the contact hypothesis re-evaluated, the implication here is that an infinitely powered
# study would have an effect size of 0.0008

# within journal articles? 
dat |> filter(pub_status == 'Journal article') |> sum_lm()
```

## Red and Processed Meat is an easier target {#sec2.3}

```{r rpm_health_count, include=F}
health_rpm_studies <- as.numeric(nrow(RPMC |> filter(str_detect(secondary_theory, 'health'))))
```

On average, interventions aimed at reducing consumption of red and processed meat (RPM) outperform general MAP reduction interventions: $\Delta$ = `r rpmc_model$Delta` \text{(SE = `r rpmc_model$se`)}, p = `r rpmc_model$pval`.
All but one of these studies employs persuasion, and a majority (`r health_rpm_studies` of `r nrow(RPMC)`) appeal to personal health.
However, these studies do not collect data on white meat and/or fish consumption, and therefore their impact on MAP consumption overall is unknown.
(One study in our primary dataset aimed to reduce RPM consumption but measured meat consumption overall [@shreedhar2021]. That study found moderate positive effects in the short-run and no effects in the long run.)

RPM is of special concern for its environmental and health consequences [@grummon2023], [+ one more cite] but some categories of meat considered sustainable are arguably worse for animals on a pound-for-pound basis [@mathur2022ethical].
For some plausible patterns of substitution, these interventions are a net good for personal health and the environment and a net bad for animal welfare.

## Psychological interventions work sometimes, but it is not clear why or when {#sec2.4}

```{r include=F, echo=F}

# look at the distribution of effect sizes

psychology_studies <- dat |> filter(str_detect(theory, "Psychology")) |> select(author, year, theory, d, se_d, intervention_condition, neg_null_pos) |> arrange(neg_null_pos, d) |> print(n = 30)

psych_proportions  <- psychology_studies |>
  sum_tab(neg_null_pos)
```

The overall effect for psychology interventions is $\Delta$ = `r psychology_model$Delta` (SE = `r psychology_model$se`), p = `r psychology_model$pval`.
Of these `r nrow(psychology_studies)` interventions, `r psych_proportions['0']` are self-reported nulls.
Moreover, the spread of results within the dominant psychological approach (norms interventions) is unusually large.
For example, one standout paper with four included studies, each featuring real-world settings and objectively measured consumption outcomes, finds one significant positive result, two nulls, and one significant backlash [@sparkman2020].
We do not see, in this collection of studies, a clear limiting principle for when psychological interventions achieve their goals.

A forthcoming meta-analysis of dynamic norms interventions concludes that their overall effects on MAP consumption are negligible [@Weikertova2024].

## The evidence for choice architecture on MAP consumption is promising, but scant {#sec2.5}

Although choice architecture studies are common in the diet literature writ large [@olafsson2024; @cadario2020; @szaszi2018], only three such studies from two papers [@kanchanachitra2020; @andersson2021] met our inclusion criteria.
Two of those three had moderate effect sizes on the order of a few percentage points of MAP reduction.
The third had a comparatively large effect ($\Delta$ = `r round(dat |> filter(author == 'Kanchanachitra') |> arrange(desc(d)) |> slice(1) |> pull(d),3)`); that study sought to reduce the fish sauce consumption at a Thai university, and its most effective intervention paired a modified spoon that made it harder for people to serve themselves fish sauce along with a sign enjoining diners to not add more than 2/3 teaspoons of fish sauce to their meal.

\begin{comment}
Arguably this second component  means that this study no longer qualifies as choice architecture because the message is not intended to act on unconscious processes; we include it because there is no reported enforcement mechanism and the modified spoon is a distinctly architectural change.
\end{comment}

```{r broader_choice_model, include=F}
broader_choice_model <- dat |> filter(theory == 'Choice Architecture' | 
                                        str_detect(secondary_theory, 'nudge')) |> 
  map_robust()
```

We restrict our choice architecture analysis to studies that literally alter the architecture of a choice, but two other studies in our dataset describe themselves as pursuing nudges.
The first encourages participants to take a pledge to go vegetarian [@banerjee2019], which implicitly corrects for time-inconsistent preferences, while the second embeds a statement about the proportion of people who chose a vegetarian meal in a specified time period at a university canteen [@griesoph2021], which creates what the authors call a "social comparison nudge." If we include these studies in our choice architecture model, we get a pooled effect size of $\Delta$ = `r broader_choice_model$Delta` (SE = `r broader_choice_model$se`), p = `r broader_choice_model$pval`, gleaned from 8 interventions with approximately `r broader_choice_model$N_subjects` subjects.

\begin{comment} maybe say something about Had our search extended to 2024, we would have likely included many more studies, many of which find small or null effects [cite]
\end{comment}

## Persuasion messages are more persuasive in conjunction with psychological appeals {#sec2.6}

```{r persuasion_psych_papers}
psych_studies_papers <- dat |> 
  filter(theory == 'Persuasion & Psychology') |> 
  summarise(count_unique_paper_id = n_distinct(unique_paper_id)) |> pull(count_unique_paper_id)
```

Combining persuasion messages with psychological appeals leads to a pooled effect size that is moderately larger than either approach on its own: $\Delta$ = `r persuasion_psychology_model$Delta` (SE = `r persuasion_psychology_model$se`), p = `r persuasion_psychology_model$pval`.
With just `r persuasion_psychology_model$N_studies` studies drawn from `r psych_studies_papers` papers in this sub-sample, we do not consider this a particularly robust finding, and we note that the overall effect is still substantively small.
However, we do consider synthesizing multiple theories into a singular framework to be a promising path forward for researchers and encourage more such theoretical innovations.

## Health studies work better for RPM than for MAP {#sec2.7}

```{r health_studies, echo=F, include=F}
health_studies <- dat |> 
  filter(str_detect(secondary_theory, 'health')) |> 
  select(author, year, secondary_theory, d, se_d, 
         self_report, neg_null_pos) |> 
  arrange(neg_null_pos, d) |> print(n = 30)

health_proportions <- 
  health_studies |> sum_tab(neg_null_pos)

RPM_health_model <- RPMC |>  
  filter(str_detect(secondary_theory, 'health')) |> map_robust()

# health studies form the majority of RPM studies
RPM_health_numbers <- RPMC |> mutate(health_component = if_else(str_detect(secondary_theory, 'health'), T,F)) |> 
  sum_tab(health_component)
```

The pooled effect size for persuasion studies with a health component on MAP consumption is $\Delta$ = `r health_model$Delta` (SE = `r health_model$se`), p = `r health_model$pval`.
This is small and not significant, albeit larger than the overall pooled effect.

Health appeals are a component of `r RPM_health_numbers['TRUE']` of `r nrow(RPMC)` interventions aimed at reducing RPM consumption, and are generally more effective there: $\Delta$ = `r RPM_health_model$Delta` (SE = `r RPM_health_model$se`), p = `r RPM_health_model$pval`.
This fits with a broader context where official nutritional guidelines typically encourage consumers to reduce RPM and consume moderate amounts of lean meat and fish.

We also judge many health studies to be at elevated risk of self-reporting bias.
For example, one study seeks to induce a sense of fear in subjects [@berndsen2005], while others target people who are at risk of cancer [@hatami2018] or cancer survivors [@james2015; @lee2018] with reasons they should change their diet, and then ask subjects to self-report what they have eaten recently.

## Environmental appeals have modest positive effects {#sec2.8}

```{r environment_studies, include=F}
# exploratory
# do environmental studies work better for certain groups?
# my intuition would be they work best for college students, & Jalil study is nice evidence of that,
# but the data don't reflect that
dat |> filter(str_detect(secondary_theory, 'environment')) |> 
  split(~population) |> map(map_robust)

dat |> filter(str_detect(secondary_theory, 'animal')) |>  
  mutate(ad_yes_no = if_else(advocacy_org == 'N', FALSE, TRUE)) |> 
  split(~ad_yes_no) |> map(map_robust)

jalil_data <- dat |> filter(author == 'Jalil')
```

The pooled effect size for persuasion studies with an environmental component is $\Delta$ = `r environment_model$Delta` (SE = `r environment_model$se`), p = `r environment_model$pval`.
The strongest evidence that these appeals produce real-world impacts is [@jalil2023], which substituted an introductory lecture in a first-year economics class for a lecture on the environmental and health consequences of meat, focusing mostly on the environment, and then tracked student meal choices in dining halls for three years following.
That study found that treatment led to an overall reduction in MAP consumption of 5.6% ($\Delta$ = `r round(jalil_data$d, 3)`).
This study has low statistical power due to its small number of clusters (5 each in treatment and control, and SE = `r round(jalil_data$se_d, 3)`).
However, due to its exceptional commitment to long-term, oblique outcome measurement, we consider it to be reasonably strong evidence for this intervention's efficacy among the target population.

## Animal welfare appeals are almost always ineffective {#sec2.9}

```{r animal_welfare_studies, include=F}
animal_proportions  <- dat |> filter(str_detect(secondary_theory, 'animal')) |>
  sum_tab(neg_null_pos)

animal_proportions['0']

neg_animal_results <- dat |> filter(str_detect(secondary_theory, 'animal')) |> filter(d<0) |> count()

advocacy_model <- dat |> filter(advocacy_org != 'N') |> map_robust()
```

The pooled effect size for persuasion studies with an animal welfare component is $\Delta$ = `r animal_model$Delta` (SE = `r animal_model$se`), p = `r animal_model$pval`.
A full `r animal_proportions['0']` of `r animal_model$N_interventions` interventions in this category are self-described nulls.
Slightly more than half (`r neg_animal_results` of `r animal_model$N_interventions`) lead to increases in MAP consumption, though just one of these effects is statistically significant.

The `r advocacy_model$N_studies` studies and `r advocacy_model$N_interventions` interventions using materials from advocacy organizations find an overall effect of $\Delta$ = `r advocacy_model$Delta` (SE = `r advocacy_model$se`), p = `r advocacy_model$pval`.

## Mixed evidence of effect delay over time {#sec2.10}

A key quantity of interest is whether MAP reduction interventions are habit-forming.
A large effect observed immediately [@hansen2021] could plausibly dissipate quickly and create no enduring changes.
Moreover, gudiing someone to have a vegetarian meal that they do not enjoy might create a net backlash against non-MAP meals.
On the other hand, a small push in the right direction \textemdash a nudge towards a vegan meal that someone really enjoys \textemdash might start a cascade of positive changes.

```{r effect_decay, include=F}
delay_model <- dat |> sum_lm(d, delay)
endline_delay_model <- dat |> sum_lm(d, delay_post_endline)
# "tiny, positive relationship"
delay_model[2,1]

# "tiny, negative relationship"
endline_delay_model[2,1]

```

In our dataset, we find mixed, inconclusive evidence of effect decay over time.
First, there is a tiny, positive relationship between number of days separating treatment onset from outcome measurement and a tiny, negative relationship between number of days separating treatment *conclusion* from measurement.
We consider these results a wash.

Another way to consider effect decay is to look at studies which measure consumption at multiple points [e.g. @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @jalil2023].
We note that for the most part, effects seem to decline over time; One prominent counterexample is [@jalil2023], where effects persist for at least three years.
However, because most of the studies we looked at featured attrition over time, we do not place too much stock in this result.

\begin{comment} There is probably a way to do this quantitatively and if we get asked to do it in review, I'll do it, but it's a fair bit of work and I think we've made the general point 
\end{comment}

## Heterogeneity by reporting, cluster assignment, delivery method, and country {#sec2.11}

```{r confounding_check, include=F}
confound_table <- dat |> filter(self_report == 'N') |> 
  group_by(str_detect(population, 'university')) |> 
  summarise(count = n()) |> as_tibble()
```

Our sample of studies is comparatively small, and many differences between studies are confounded.
For example, `r confound_table$count[[2]]` of `r confound_table$count[[2]] + confound_table$count[[1]]` interventions with objectively reported outcomes are also studies of university populations.
Nevertheless we offer a few tentative explorations of potential moderators of effect size.

```{r heterogeneity_analyses, include=F}

self_report_table <- dat |> split(~self_report) |> map(map_robust)  |> 
 bind_rows(.id = 'self_report')

cluster_results <- dat |> split(~cluster_assigned) |> map(map_robust)
delivery_table <- table(dat$delivery_method)

# "There was no meaningful relationship between delay and effect size"
dat |> sum_lm(d, delay)

# university vs adults
university_results <- dat |> filter(str_detect(population, 'university')) |> map_robust()
adult_results <- dat |> filter(str_detect(population, 'adult')) |> map_robust()
```

Contrary to our expectations, self-reported and objectively collected outcomes were not meaningfully different: `r self_report_table[self_report_table$self_report == "N", "Delta"]` for objectively reported vs `r self_report_table[self_report_table$self_report == "Y", "Delta"]` for self-reported.
Likewise, the difference in effect sizes for studies where treatment was assigned to clusters (e.g. cafeteria or day of treatment) vs. individuals is small.

Table 4 displays the effect size associated with the four most common delivery mechanisms in our dataset, which were also the groups with enough clusters for meta-analysis to be viable.

`r table_four`

Table 5 displays effects associated with different regions.
We see weak evidence that these interventions are most effective in Europe.

`r table_five`

Interventions with adult subjects are moderately more effective ($\Delta$ = `r adult_results$Delta`, SE = `r adult_results$se`, p = `r adult_results$pval`) than those with a university population ($\Delta$ = `r university_results$Delta`, SE = `r university_results$se`, p = `r university_results$pval`).

# Methods {#sec3}

**Search**: We employed a multi-pronged search strategy for assembling our database.
First, we checked the bibliographies of three recent reviews [@mathur2021meta; @bianchi2018conscious; @bianchi2018restructuring] for relevant studies.
Second, we checked any possibly relevant study that either cited or was cited by studies we from our first round of coding.
Third, we checked the bibliographies of authors whose studies we coded.
Fourth, we contacted leading researchers in the field with our in-progress database to see if we had missed any.
Fifth, we repeated steps two and three with new studies we had.
Sixth, we searched Google Scholar for terms that had come up in studies repeatedly (e.g. "dynamic norms + meat", "MAP reduction", and "plant-based diet + effective").

\begin{comment} 
does this need more description? This is not entirely reproducible I think but TBH it was not a major source of studies in our database
\end{comment}

Seventh, we checked a database emerging from a parallel project being conducted by Rethink Priorities.
Eighth, we identified a further 130+ systematic reviews and checked their bibliographies, and obtained qualified studies from six of those reviews [@ammann2023; @chang2023; @DiGennaro2024; @harguess2020; @ronto2022; @wynes2018].
Ninth, we used an AI search tool (<https://undermind.ai>) to check for gray literature.
All three authors contributed to the search.

**Coding:** For quantitative outcomes, we selected the latest possible outcome that had enough subjects to meet our inclusion criteria.
Sample sizes were drawn from the same post-test.
All effect sizes were standardized by the standard deviation of the outcome for the control group at baseline whenever possible (Glass's $\Delta$).
All effect size conversions were conducted by the first author using methods and R code initially developed for previous papers [@paluck2019; @paluck2021; @porat2024] using standard techniques from [@cooper2019], with the exception of a difference in proportion estimator created for [@paluck2021] and explained in our appendix.

**Meta-analysis:** Our initial set of analyses was pre-registered on the Open Science Framework in November 2023 (<https://osf.io/j5wbp>), although the project evolved substantially over time.
We describe four important deviations in our appendix.
Our analyses use functions and models from the `robumeta` [@fisher2015], `metafor` [@viechtbauer2010], and `tidyverse` [@wickham2019] packages in `R` [@Rlang].
Our meta-analyses use robust variance estimation methods [@hedges2010].

# Discussion {#Sec4}

We offer three lenses through which to view our results.

First, one might focus on the small effect sizes and the evidence of publication bias and conclude that the meager effects we detect are likely overestimates, and therefore conclude that the true effect being estimated in this dataset is a strict null.

Second, one might argue that our assembled database of studies *is* successfully changing consumption behavior, but in ranges too small for most studies to detect.
By this light, future studies should replicate existing approaches with sufficient power to detect much smaller effects.

Moreover, a change of a few percentage points might be significant in some contexts.
For example, if a college aiming to reduce its carbon output might calculates that meat consumption accounts for 20% of its carbon emissions, a 5.4% reduction in meat consumption [@jalil2023] would achieve about a 1% reduction in carbon output.
Whether this is comparatively cost-effective depends on the other options available.

```{r largest_eff_sizes, include=F}
large_effs <- dat |> filter(d>=0.5) |> 
  select(author, year, d, se_d, theory, secondary_theory, 
         n_t_post, n_c_post,  n_t_total_pop, n_c_total_pop)

average_total_large_effs_n <- round(median(large_effs$n_t_total_pop)) + round(median(large_effs$n_c_total_pop))

```

Third, one might look at the largest effect sizes in our dataset, for instance, the five papers with an effect size of $\Delta \geq 0.5$ [@bianchi2022; @carfora2023; @kanchanachitra2020; @merrill2009; @piester2020] and seek to replicate and/or expand their approaches.

We do not have a clear preference between these interpretations.
However, we are generally encouraged by trends in this literature.

First, as previously noted, a majority of studies that meet our inclusion criteria have been published in the past few years, suggesting an overall increase in attention to design and measurement validity.

Second, we applaud researchers in this field for publishing null results when they encounter them.

Third, the tentative but positive results from studies that combine persuasive messages with psychological appeals suggest that theoretically synergistic approaches are a promising path forward.

Fourth, we notice that the universe of possible interventions and settings is much broader than those we analyzed, suggesting that some promising approaches await rigorous evaluation For example, direct contact with animals on an animal sanctuary, price gradations, high-intensity vegan meal planning, door-to-door canvassing, or studies taking place in diverse settings such as retirement homes are all promising candidates.

\backmatter

\bmhead{Supplementary information}

All code data, and documentation are available on GitHub (\url{https://github.com/setgree/vegan-meta}) and Code Ocean [LINK]

\bmhead{Acknowledgments}

*Thanks to Alex Berke, Alix Winter, Anson Berns, Hari Dandapani, Adin Richards, Martin Gould, and Matt Lerner for comments on an early draft. Thanks to Jacob Peacock, Andrew Jalil, Gregg Sparkman, Joshua Tasoff, Lucius Caviola, Natalia Lawrence, and Emma Garnett for help with assembling the database and providing guidance on their studies. We gratefully acknowledge funding from the NIH (grant XXX) and Open Philanthropy (YYY).*

# Declarations {.unnumbered}

\newpage

# Appendix {#Sec5}

## Supplementary Methods {#Sec5.1}

### Converting difference in proportions to standardized mean difference {#Sec5.1.1}

Conventional methods of converting binary outcomes to estimates of standardized mean differences have some notable downsides.
For example, any given odds ratio is compatible with multiple effect sizes depending on the rate of occurrence of the dependent variable [@gomila2021].
We address this by treating all binary variables as draws from a Bernoulli distribution with variance $p(1 - p)$, where p is the proportion of some event's occurrence.
For example, if 50% of the treatment group ate vegetarian meals vs 45% for the control group, then Glass's $\Delta = \frac{0.05}{\sqrt{0.45 * (1-0.45)}} = 0.1$.

### Four deviations from pre-analysis plan {#Sec5.1.2}

Our pre-analysis plan registered some general principles and hypotheses for our search process, but did not otherwise do much to constrain how or where we searched.
It also included a synthesized dataset and some mock analyses that resemble our final analyses in general form.
However, as the project evolved over time, we made four substantive changes to our paper that we did not anticipate at the pre-analysis stage.

First, our initial draft combined RPM and MAP studies, taking the former as providing face value estaimtes of the latter.
However, we later decided that RPM reduction was fundamentally a separate estimand, and that without firm data on substitution to other kinds of MAP, we could not say anything definite about the net effect on demand for MAP.

Second, and related, we initially included studies that were not necessarily aimed at achieving overall MAP reduction, but rather a generally healthier diet with some amount of chicken and/or fish, for instance the Mediterranean diet.
Many of these studies had otherwise qualifying measurement strategies and were generally highly powered and well-designed [@beresford2006].
However, we ultimately concluded that these were a separate estimand as well, and we did not want to add noise to the estimate of studies that were aimed more specifically at reducing MAP consumption rather than at inter-MAP substitution.
Further, we almost all of these studies featured self-reported data, and comparatively few tracked all relevant categories of MAP.

```{r alt_model, include=F}
alt_model <- dat |> map_robust(model = 'RMA')
```

Third, our initial analyses used the random effects model from `metafor` to calculate pooled effect sizes.
However, as we assembled our dataset, we noticed that many papers had, across interventions, non-independent observations, typically in the form of multiple treatments compared to a single control group.
Upon discussion, the team's statistician (MBM) suggested that the `CORR` model from the `robumeta` package would be a better fit.

Using our original model from `metafor`, we detect a pooled effect size of `r alt_model$Delta` (SE = `r alt_model$se`), p = `r alt_model$pval`.
In relative terms, this is substantially smaller, but in absolute terms, both this model and our main model produce very small estimates.
Table S2 provides an overview of alternate estimates by our main theoretical approaches.

Fourth, we added many moderators to our dataset that we did not plan on, such as a broad category for delivery method, whether a study was intended to be emotionally activating, or whether a program had multiple components.
We did not end up focusing on these in our main paper but include them in our datasets for completeness.

## Supplementary Figure {#sec5.2}

This figure displays the relationship between standard error and effect size.
The colors correspond to theoretical approach and the shapes correspond to the venue where results were published.

```{r supplementary_figure, echo=F, message=F, out.width='120%'}
supplementary_figure
```

## Supplementary Tables {#sec5.3}

Table S1 displays the category of source where we learned of papers in our main dataset.
\captionsetup[table]{labelformat=empty}

```{r supp_table_one}
supplementary_table_one
```

Table S2 displays the pooled effect size by theoretical approach when using random effects estimation methods from the `metafor` package rather than the robust variance estimation methods implemented in the `robumeta` package.

```{r supp_table_two}
supplementary_table_two
```

## Supplementary Discussion {#sec5.4}

### The limits of systematic search for MAP reduction papers {#sec5.4.1}

This literature has remarkable methodological, disciplinary, and theoretical diversity.
However, it also has few if any agreed upon terms to describe itself.
For instance,the term "MAP" is nonstandard; other papers discuss animal-based proteins, animal products, meat, edible animal products, plant-based foods, plant-based protein, and so on.
This diversity of language poses a particular challenge for anyone seeking to systematically review this literature.
Whether one has identified the correct terms that each relevant study uses to describe itself is, for all practical purposes, unknowable.

This informed our search process.
Rather than starting with a list of search terms, we began by reading prior reviews, and then reading the studies cited by those reviews, to get a sense of the language that studies used to describe themselves.
We then pursued the multi-pronged, iterative search process described in the main text.
Ultimately, we used systematic search techniques to fill in the the blanks when we had an intuition that we were missing studies employing a particular approach.

The following are the Google Scholar search terms we used:

-   "random" "nudge" "meat"
-   "meat" "purchases" "information" "nudge"
-   "nudge" "theory" "meat" "purchasing"
-   "meat" "alternatives" "default" "nudge"
-   "dynamic" "norms" "meat"
-   "norms" "animal" "products"

For each of these terms, we looked through ten pages of results.

### Edge cases for study inclusion {#sec5.4.2}

Arguably the hardest decision in meta-analysis is what studies to include or exclude.
By far the most common reason for exclusion was category of dependent variable (e.g. measuring attitudinal or intentional outcomes).
However, many cases were harder and required some deliberation.

Some studies limit dietary portions or switch what people are served (e.g. children being served more vegetables at lunchtime).
We did not include these studies because they were essentially guaranteed to have effects that were either positive or at least bounded at zero.
However, we did include studies that provided free access to meat alternatives [@acharya2004; @bianchi2022] and measured outcomes after the intervention had concluded.
(There were not enough of these studies in our main dataset to analyze this approach separately, but their effect sizes are `r round(dat |> filter(author == 'Acharya') |> pull(d),3)` and `r round(dat |> filter(author == 'Bianchi') |> pull(d),3)` respectively.)

Other studies induce a form of treatment in their control groups, for instance asking all subjects to take a pledge to go vegetarian.
Our main database only includes studies with a pure control group or an unrelated placebo.
However, we include a selection of studies in our supplementary robustness check.

Another common design feature we encountered was treatment assigned at the level of alternating weeks at a cafeteria.
Generally these studies did not have enough weeks to meet our sample size requirements.
We also note that simply alternating weeks is not equivalent to random assignment.

Last, we encountered many studies that measured fruit and or/vegetable consumption but not MAP consumption.
In some cases, it might have been possible to add assumptions about substitution and estimate effect sizes, e.g. if menus were fixed, but we exclusively meta-analyzed studies that reported MAP consumption directly.

## Robustness check: including 15 additional studies that with near-random assignment {#sec5.4.3}

While reviewing papers, we identified `r robust_num_papers` high-quality papers comprising `r robust_num_studies` studies, `r robust_num_interventions` interventions, and approximately `r robust_n_total` subjects that did not meet our search criteria.
The most common exclusion reasons for exclusion were:

-   statistically underpowered (typically too few clusters)

-   not fully randomized (e.g. treatment was altered by week but not randomly)

-   lacking in a control group (meaning they compared two MAP reduction treatments)

-   without delay between treatment onset and measurement.

One additional study encouraged participants to switch to fish.
Each of these robustness checks studies measures consumption outcomes rather than attitudes and intentions.

This new dataset yields a pooled effect size of $\Delta$ = `r robustness_model$Delta` (SE = `r robustness_model$se`), p = `r robustness_model$pval`.
Integrating these studies with our main dataset yields an overall effect size of $\Delta$ = `r integrated_model$Delta` (SE = `r integrated_model$se`), p = `r integrated_model$pval`.
We would have considered this moderate evidence of these interventions' efficacy at reducing MAP consumption.

A clear source of divergence between the small effect sizes in our main dataset and the comparatively larger ones in the supplementary dataset is that studies without delayed measurement tend to have larger effects.
For example, [@hansen2021] tested the effects of switching the default meal to being vegetarian at three academic conferences, and found remarkable effects: in one case, a rise from 2 % of participants choosing vegetarian meals to 87%.
However, we do not assume that this represents an enduring reduction in MAP consumption.
We are concerned about "regression to the meat:" the possibility that a subject who is guided towards eating less meat at one meal might eat more at the next.
We encourage researchers to design future default studies with this challenge in mind, e.g. by measuring habits both at the treated meal and also the next one [@vocski2024].

We also see that interventions with too few clusters or too few participants to meet our criteria have much larger effects on average: $\Delta$ = `r underpower_model$Delta` (SE = `r underpower_model$se`), p = `r underpower_model$pval`.

We draw two lessons from this robustness check.
The first is that methodological rigor is associated with smaller effect sizes.
The second is that our results are sensitive to how one defines and operationalizes methodological rigor.
A team that defined rigor differently, or that was concerned about different sources of bias, might have found very different results.

### Defining the the theoretical boundaries between studies requires judgment calls {#sec5.4.4}

Tallying how many studies pursue a given theory of change requires defining those theories and drawing boundaries between them.
This proved tricky in some cases.
For instance, the words 'choice architecture' and 'nudge' [@thaler2009] are not necessarily interchangeable in this literature.
Many studies described themselves as implementing nudges but were not necessarily altering anything about the architecture of a choice, and neither were they obviously seeking to operate on 'unconscious' processes [@garnett2020].
For instance, a text message reminder of reasons to eat less meat is cheap and easy to ignore, and is arguably designed to correct for time-inconsistent preferences, a kind of cognitive bias.
On the other hand, such a text message also provides relevant information about the choice set, and if every intervention that attempted this was a nudge, most studies in our database would be nudges.
We decided that the clearest way around this was to separate interventions that altered the literal architecture of a choice, and therefore were plausibly working on unconscious processes, from interventions that tried to alter how people think or feel about what they're eating.

Likewise with social norms messages.
[@mols2015] identify "unthinking conformity" as an example of a "human failing" that nudges take as their "starting point" (p. 4), and if social norms activate an unthinking desire to conform, then arguably a message about how many people are going vegetarian in one's community is a nudge.
Our view is that a social norm prompt might engender a rich array of possible reactions, both cognitive and affective, and we do not assume that "unthinking conformity" is the dominant or exclusive response.
Therefore, we do not classify the norms interventions in our database as nudges.

A future project might investigate exactly what reactions are occurring by asking subjects how well they recall a norms message and what it made them think about.
A high prevalence of subjects who are unable to recall the message's specifics but nevertheless cut back on MAP consumption would be evidence that norms are acting through automatic rather than reflective processes.'

Reasonable people might have defined the theoretical boundary conditions differently.
For instance, rather than grouping psychology approaches together, one might separate interpersonal processes (norms) from purely personal processes, e.g. pledges, implementation intentions, or response inhibition training.
For this reason, we included in our dataset both `theory` and `secondary_theory` columns, and in the latter we include more specific information about papers' approaches to behavior change.
We invite readers to explore different categories and their respective pooled effect sizes by building on the code and data we provide.

### Notes on prior reviews {#sec5.4.5}

It was striking to us there have been more systematic reviews of MAP reduction research than there have been studies meeting our criteria.
We encourage scholars to pursue more randomized controlled trials with consumption outcomes.

We turn now to a selective overview of prior reviews of dietary research that were highly relevant to this one.

Among the reviews that found MAP reduction interventions to be effective, several focused exclusively on choice architecture.
[@arno2016] found that nudges led to an average increase of healthy dietary choices of 15.3%, while [@byerly2018] found that committing to reduce meat intake and making menus vegetarian by default were more effective than educational interventions.
However, the vast majority of vegetarian-default studies we analyzed for this paper did not qualify for our analysis because they lacked delayed outcomes, and their net effect on MAP consumption is unknown.

[@bianchi2018restructuring] found that reducing meat portions, making alternatives available, moving meat products to be less conspicuous, and changing meat's sensory properties can all reduce meat demand.
[@pandey2023] found that changing the presentation and availability of sustainable products was effective in increasing demand for them, as was providing information about them.

In a meta-review, [@grundy2022] found environmental education to be most promising, with substantial evidence also supporting health information, emphasizing social norms, and decreasing meat portions.

Some reviews have focused on particular settings for MAP reduction interventions.
[@hartmannboyce2018] found that grocery store interventions, such as price changes, suggested swaps, and changes to item availability, were effective at changing purchasing choices.
However, that review covered a wide variety of health interventions, such as reducing consumption of dietary fat and increasing fruit and vegetable purchases.
It is unclear how directly such findings translate to MAP reduction efforts.

[@chang2023] focused on university meat-reduction interventions and found more promising results than did reviews that looked at the wider public.
This suggests that students and young people may be particularly receptive to MAP reduction interventions.
[@harguess2020] reviewed 22 studies on meat consumption and found promising results for educational interventions focused on the environment, health, and animal welfare.
That paper recommends using animal imagery to cause an emotional response and utilizing choice architecture interventions.
Our review, by contrast, found no relationship between animal welfare appeals and MAP consumption.

Taking a different angle, [@adleberg2018] reviewed the literature on protests in a variety of movements and found mixed evidence of efficacy.
The authors recommend that animal advocacy protests have a specific target (e.g. a particular institution) and "ask."

Other studies provide insights on who is most easily influenced by interventions to reduce MAP consumption.
For example, [@blackford2021] found that nudges focused on "system 1" thinking were more effective at encouraging sustainable choices than those focused on "system 2," and that interventions had greater effects on females than males.
Our review also featured studies showing differences between men and women.

[@rosenfeld2018] reports that meat avoidance is associated with liberal political views, feminine gender, and higher openness, agreeableness and neuroticism.
That review also identifies challenges and barriers to vegetarianism, such as recidivism and hostility from friends and family.
Future research could tailor interventions to address these barriers.

Several reviews have had mixed or inconclusive results.
For instance, [@bianchi2018conscious] found that health and environmental appeals appear to change dietary intentions in virtual environments, but did not find evidence of actual consumption changes.
In the same vein, [@kwasny2022] notes that most existing research focuses on attitudes and intentions and lacks measures of actual meat consumption over an extended period of time.
[@taufik2019] reviewed many studies aimed at increasing fruit and vegetable intake, but found far fewer that looked at reducing MAP consumption.
[@benningstad2020] found that dissociation of meat from its source plays a role in meat consumption, but no extant research that included behavioral outcomes.

A few reviews have found evidence that seems to recommend against particular interventions.
[@greig2017] reviewed the literature on leafleting for vegan/animal advocacy outreach, and observed biases towards overestimating impact.
That paper concluded that leafleting does not seem cost-effective, though with significant uncertainty.
This accords with our findings on advocacy organization materials' limited impacts.

[@nisa2019] meta-analyzed interventions to improve household sustainability, of which reducing MAP consumption was one of several.
Although they found small effect sizes for most interventions, they concluded that nudges were comparatively effective.
Many such nudge studies looked at meat consumption.
Similarly, [@rau2022] reviewed the literature on environmentally friendly behavior changes, including but not limited to diet change, and found small or nonexistent effects in most cases.
Only fifteen interventions in that paper were described as â€œvery successful,â€ and none of these related to food.

We draw two lessons from these papers.
The first is that the marginal value of a new rigorous evaluation is much higher than that of a new systematic review.
The second is that the category of dependent variable matters for estimating impact.
We encourage researchers who care about reducing MAP consumption to measure it directly whenever possible.

\begin{comment}
First I feel a little strange saying that the marginal value of a review is low (then whay are we writing this paper?) Second, maybe there's a space for discussion about what counts as  meaningful? mention that ease of implementation matters in terms of whatâ€™s meaningful. The costs of fully exposing one person is the relevant denominator. The costs of recruitment are part of the cost. Maybe some people are more amenable to nudges after hearing an argument for
\end{comment}

\newpage

# References
