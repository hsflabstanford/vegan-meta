---
classoptions: 
  - sn-nature      
  # - referee       # Optional: Use double line spacing 
  # - lineno        # Optional: Add line numbers
  # - iicol         # Optional: Double column layout
title: "Meaningfully reducing consumption of meat and animal products is an unsolved problem: results from a meta-analysis"
titlerunning: MAP-reduction-meta
authors: 
  - firstname: Seth Ariel
    lastname: Green
    email: setgree@stanford.edu
    affiliation: 1
    corresponding: TRUE
  - firstname: Maya B.
    lastname: Mathur
    affiliation: 1
  - firstname: Benny
    lastname: Smith 
    affiliation: 2
affiliations:
  - number: 1
    info:
      orgdiv: Humane and Sustainable Food Lab
      orgname: Stanford University
  - number: 2
    info:
      orgname: Allied Scholars for Animal Protection 
keywords:
  - meta-analysis
  - meat
  - plant-based
  - randomized controlled trial
  
abstract: |
  Which theoretical approach leads to the broadest and most enduring reductions in consumptions of meat and animal products (MAP)? We address these questions with a theoretical review and meta-analysis of rigorous Randomized Controlled Trials. We meta-analyze 36 papers comprising 43 studies, 114 interventions, and approximately 88000 subjects. We find that these papers employ four major strategies to changing behavior: choice architecture, persuasion, psychology, and a combination of persuasion and psychology. The pooled effect of these interventions on MAP consumption outcomes is $\Delta$ = 0.065, indicating an unsolved problem. Reducing consumption of red and processed meat is an easier target: $\Delta$ = 0.258, but because of missing data on potential substitution to other MAP, we can’t say anything definitive about the consequences of these interventions on animal welfare. We further explore effect size heterogeneity by approach, population, and study features. We conclude that while no theoretical approach provides a proven remedy to MAP consumption, designs and measurement strategies have generally been improving over time, and many promising interventions await rigorous evaluation.
date: "`r Sys.Date()`"
output: 
  rticles::springer_article:
    keep_tex: true
bibliography: "./vegan-refs.bib"
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{comment}
  - \usepackage{anyfontsize}
  - \usepackage{caption}
---

------------------------------------------------------------------------

```{r setup, include=FALSE}
# so that knitr labels figures
knitr::opts_chunk$set(
	fig.path = "./figures/",
	echo = FALSE,
	out.extra = ""
)
# so we can put the manuscript stuff in its own folder
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(rprojroot::find_rstudio_root_file())
options(tinytex.clean = TRUE) # switch to FALSE to get the bbl file

source('./scripts/libraries.R')
source('./scripts/functions.R')
source('./scripts/load-data.R')
source('./scripts/red-and-processed-meat.R')
source('./scripts/models.R')
source('./scripts/tables.R')
source('./scripts/figures.R')
```

# Introduction {#sec1}

Consumption of meat and animal products (MAP) is increasingly recognized as a major contributor to premature deaths [@willett2019; @landry2023], public health risks [@slingenbergh2004; @graham2008], ecological harms [@greger2010] and climate change [@scarborough2023; @koneswaran2008] as well as an ethical crisis in its own right [@kuruc2023; @singer2023].

Supply-side interventions, such as banning or taxing certain practices or products, risk political backlash if they lack broad public support.
It is of vital importance, therefore, to assess which strategies and theoretical perspectives lead to consistent reductions in demand for MAP, under which conditions, and for which populations.

The research on diet and its antecedents and consequences is vast.
By our count, there have been at least `r round_to(nrow(read.csv('./data/review-of-reviews.csv')), 10)` previous published dietary reviews in the past two decades, with at least 37 focused specifically on MAP reduction.
However, comparatively few of these are quantitative, and most prior reviews investigated particular approaches, for example choice architecture [@bianchi2018restructuring], rather than comparing the efficacy of different strategies.
Moreover, two prior investigations revealed three common gaps in the MAP reduction literature: a dearth of long-term follow-ups, missing consumption outcomes, and inattention to the gap between intentions and behavior [@mathur2021meta; @mathur2021effectiveness].

Our paper addresses these concerns by meta-analyzing randomized controlled trials that

-   were designed to voluntarily reduce MAP consumption, rather than encouraging substitution from red meat to white meat or to fish, or removing items from someone's plate

-   had least 25 subjects each in treatment and control, or, for cluster-randomized trials, at least 10 clusters in total;

-   measured MAP consumption, whether self-reported or observed directly, rather than (or in addition to) attitudes, intentions, beliefs or hypothetical choices;

-   featured a pure control group or placebo, rather than comparing multiple treatments

-   recorded outcomes at least a single day after the start of treatment.

Additionally, studies needed to be publicly circulated by December 2023 and published in English.

```{r useful_constants}
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
num_interventions <- as.numeric(nrow(dat))

n_total <-  round_to(x = sum(dat$n_c_total_pop) + sum(dat$n_t_total_pop), 1000, floor)

decade_tab <- dat |> group_by(unique_paper_id) |>  slice(1) |>  ungroup() |> count(decade)
```

We coded `r num_papers` such papers [@andersson2021; @kanchanachitra2020; @abrahamse2007; @acharya2004; @banerjee2019; @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @hennessy2016; @piester2020; @cooney2014; @cooney2016; @feltz2022; @haile2021; @hatami2018; @jalil2023; @mathur2021effectiveness; @merrill2009; @norris2014; @peacock2017; @polanco2022; @sparkman2021; @weingarten2022; @aldoh2023; @allen2002; @camp2019; @coker2022; @griesoph2021; @sparkman2017; @sparkman2020; @berndsen2005; @bertolaso2015; @fehrenbach2015; @mattson2020; @shreedhar2021] comprising `r num_studies` separate studies, `r num_interventions` interventions, and approximately `r n_total` subjects.
(This is an approximation because some interventions were administered at the level of day or cafeteria and did not record a precise number of human subjects.) The earliest paper was published in 2002 [@allen2002], and a majority (`r decade_tab$n[3]` of `r num_papers`) have been published since 2020.

```{r red_meat_numbers, include=F}
RPMC_papers <- as.numeric(max(RPMC$unique_paper_id))
RPMC_studies <- as.numeric(max(RPMC$unique_study_id))
RPMC_interventions <- as.numeric(nrow(RPMC))
n_meat_total <-  plyr::round_any(x = sum(RPMC$n_c_total_pop) + sum(RPMC$n_t_total_pop), 1000, floor)

# also supplementary_numbers
excluded_count <- nrow(read.csv('./data/excluded-studies.csv'))


```

We also coded four supplementary datasets.
First is a collection of `r RPMC_papers` papers [@anderson2017; @carfora2017correlational; @carfora2017randomised; @carfora2019; @carfora2019informational; @delichatsios2001talking; @dijkstra2022; @emmons2005cancer; @emmons2005project; @jaacks2014; @james2015; @lee2018; @perino2022; @schatzkin2000; @sorensen2005; @wolstenholme2020] aimed at reducing, and measuring, consumption of red and/or processed meat (RPM), comprising `r RPMC_studies` studies, `r RPMC_interventions` interventions, and approximately `r n_meat_total` subjects.

```{r robust_data, include=F}
source('./scripts/robustness-check.R')
```

Second is a dataset of `r robust_num_papers` studies that we disqualified for methodological reasons but that we include in a supplementary robustness check [@alblas2023; @beresford2006; @dannenberg2023; @delichatsios2001eatsmart; @epperson2021; @frie2022; @garnett2020; @hansen2021; @kaiser2020; @lentz2020; @lindstrom2015; @loy2016; @piazza2022; @reinders2017; @vlaeminck2014].

Third, we coded a dataset of `r excluded_count` studies that we excluded along with their reasons for exclusion.

```{r review_of_reviews}
review_n <- nrow(read.csv('./data/review-of-reviews.csv'))
```

Fourth is a dataset of `r review_n` review papers that we reviewed while searching for papers.

All datasets are provided in our supplementary materials.

Studies in our primary database pursued three theories of change: choice architecture, psychology, and persuasion, or a combination of persuasion and psychology.

**Choice Architecture** studies [@andersson2021; @kanchanachitra2020] manipulate aspects of physical environments to make non-MAP options more salient, such as placing a vegetarian meal at eye level on a billboard menu [@andersson2021] or making it more laborious for people to serve themselves fish sauce [@kanchanachitra2020].

```{=tex}
\begin{comment}
Do we put in something here about the line between choice architecture and nudge? I currently have it in the results section?
(See our results section forA handful of other studies [NAME THEM] identify their interventions as nudges, but do not alter the actual architecture of a choice, instead doing [WHAT THEY DO]. Our quantitative results are presented both with and without these studies included with the choice architecture studies.)
\end{comment}
```
**Persuasion** studies [@kanchanachitra2020; @abrahamse2007; @acharya2004; @banerjee2019; @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @hennessy2016; @piester2020; @cooney2014; @cooney2016; @feltz2022; @haile2021; @hatami2018; @jalil2023; @mathur2021effectiveness; @merrill2009; @norris2014; @peacock2017; @polanco2022; @sparkman2021; @weingarten2022] appeal directly to people to eat less MAP.
These studies formed the majority of our database.
Arguments focus on health, the environment (usually climate change), and animal welfare.
Some are designed to be emotionally activating, e.g. presenting upsetting footage of factory farms [@polanco2022], while others present facts about, e.g., the relationship between diet and cancer [@hatami2018].
Many persuasion studies combine arguments, such as a lecture on the health and environmental consequences of eating meat [@jalil2023].

**Psychology** studies [@aldoh2023; @allen2002; @camp2019; @coker2022; @griesoph2021; @piester2020; @sparkman2017; @sparkman2020] usuaully manipulate the interpersonal,cognitive, or affective factors associated with eating meat.
The most common psychological intervention is centered on social norms.
These studies seek to alter the perceived popularity of desired outcomes, e.g. plant-based dishes [@sparkman2017].
Norms might be descriptive, stating how many people engaged in the desired behavior, [@aldoh2023],or dynamic,teling subjects that the number of people engaging in desired behavior is increasing [@aldoh2023; @coker2022; @sparkman2017; @sparkman2020].
Another study looked at response inhibition training, where subjects are trained to avoid responding impulsively to meat [@camp2019].
The first psychology study meeting our criteria was published in 2017.

Finally, a group of interventions combines **persuasion** approaches with **psychological** appeals to reduce MAP consumption [@berndsen2005; @bertolaso2015; @carfora2023; @fehrenbach2015; @hennessy2016; @mattson2020; @piester2020; @shreedhar2021].

These interventions typically suggest reasons to eat less meat side by side with information about changing consumer habits in society, i.e they combine norms and persuasion approaches.
Others combine reasons to change one's diet along with the extended parallel process model -- how they react to fear [@fehrenbach2015] -- or an implementations intentions model [@shreedhar2021] where subjects implement plans for changing their behavior.

# Results {#sec2}

## An overall small effect with some heterogeneity by approach {#sec2.1}

Our overall meNta-analytic effect size is $\Delta$ = `r model$Delta` (SE = `r model$se`), p = `r model$pval`.
The aggregate effect is statistically significant, but does not indicate a meaningful reduction.

Figure 1 displays the distribution of effect sizes, grouped by paper, with each individual point representing an intervention.
The overall effect size is plotted at the bottom.

```{r forest_plot, out.width='120%'}
forest_plot
```

Table 1 reports the number of studies, interventions, and subjects (approximately) per approach, as well as the pooled effect sizes per approach.

`r table_one`

Table 2 displays the numbers and findings of persuasion interventions by topic.

`r table_two`

These small effects may surprise readers of previous reviews, which typically found more positive results [@mathur2021meta; @meier2022; @mertens2022].
We attribute this difference to our stricter inclusion criteria.
For instance, of the ten largest effect sizes recorded in [@mathur2021effectiveness], nine were non-consumption outcomes and the tenth came from a non-randomized design.

```{r nulls_and_cis}
effect_size_table <- dat |> sum_tab(neg_null_pos)

significant_CIs <- dat |> select(author, year, d, se_d, neg_null_pos) |>
  mutate(lower_bound = d - (1.96 * se_d),
         upper_bound = d + (1.96 * se_d)) |>
  filter(lower_bound < 0 & upper_bound < 0 |
           lower_bound > 0 & upper_bound > 0) |> 
  mutate(direction = case_when
         (lower_bound > 0 ~ 1,
           upper_bound < 0 ~ 0))

# pap analysis
pap_model <- dat |> filter(public_pre_analysis_plan != 'N') |> map_robust()

## this difference is not statistically significant
dat <- dat |> mutate(pap_yes = if_else(public_pre_analysis_plan == 'N', FALSE, TRUE))

pap_difference <- robumeta::robu(d ~ pap_yes, dat, unique_study_id, var_d)

# open data analysis
open_data_model <- dat |> filter(open_data != 'N') |> map_robust()

dat <- dat |> mutate(open_data_yes = if_else(open_data == 'N', FALSE, TRUE))

open_data_difference <- robumeta::robu(d ~ open_data_yes, dat, unique_study_id, var_d)
```

Per the papers' own calculations and data, `r effect_size_table['0']` of `r num_interventions` interventions had null effects on net MAP consumption.
However, many studies present a wide variety of outcomes, or include MAP reduction as one of many components of a broader program of behavior change, and present significant results as well.

```{=tex}
\begin{comment}
Could put this back in: "Using our calculations of effect size and standard error `r (nrow(significant_CIs))` interventions have 95% confidence intervals that do not overlap with zero, `r sum(significant_CIs$direction)` of which are positive effects, out of `r nrow(dat)` interventions."
\end{comment}
```
## Moderate evidence of publication bias {#sec2.2}

We conduct four tests for publication bias.
None is conclusive.

```{=tex}
\begin{comment} 
Could put in introductory remarks about how this puts our main results in one light or another? 
\end{comment}
```
First, in our dataset, effect size and standard error are positively correlated, though not significantly.

Second, the `r pap_model$N_studies` studies with pre-analysis plans have a marginally smaller effect: $\Delta$ = `r pap_model$Delta` (SE = `r pap_model$se`), p = `r pap_model$pval`.

Third, the `r open_data_model$N_studies` studies with openly available data also have a marginally smaller effect: $\Delta$ = `r open_data_model$Delta` (SE = `r open_data_model$se`), p = `r open_data_model$pval`.

Fourth, the pooled effect size of interventions published in peer-reviewed journals is larger than the pooled effect size from everything else (advocacy organization publications, preprints, and student theses).

`r table_three`

```{r pbulication_bias_checks, include=F}

# "$\Delta$ and standard error are positively correlated, though not significantly"
dat |> sum_lm(d, se_d)
# a la the contact hypothesis re-evaluated, the implication here is that an infinitely powered
# study would have an effect size of 0.0008

# within journal articles? 
dat |> filter(pub_status == 'Journal article') |> sum_lm()
```

## Red and Processed Meat is an easier target {#sec2.3}

```{r rpm_health_count, include=F}
health_rpm_studies <- as.numeric(nrow(RPMC |> filter(str_detect(secondary_theory, 'health'))))
```

On average, interventions aimed at reducing consumption of red and processed meat (RPM) outperform general MAP reduction interventions: $\Delta$ = `r rpmc_model$Delta` \text{(SE = `r rpmc_model$se`)}, p = `r rpmc_model$pval`.
Each of the RPM reduction studies employs persuasion, and a majority (`r health_rpm_studies` of `r nrow(RPMC)`) appeal to personal health.
However, these studies do not collect data on white meat and/or fish consumption, and therefore their impact on MAP consumption overall is unknown.
(One study in our primary dataset aimed at reducing RPM consumption but measures meat consumption overall [@shreedhar2021]. That study found moderate positive effects in the short-run and no effects in the long run.)

RPM is of special concern for its environmental and health consequences [@grummon2023], but some categories of meat considered sustainable are arguably worse for animals on a pound-for-pound basis [@mathur2022ethical].
For some plausible patterns of substitution, these interventions are net positive for health and the environment and net negative for animal welfare.

## Psychological interventions work sometimes, but it is not clear why or when {#sec2.4}

```{r include=F, echo=F}

# look at the distribution of effect sizes

psychology_studies <- dat |> filter(str_detect(theory, "Psychology")) |> select(author, year, theory, d, se_d, intervention_condition, neg_null_pos) |> arrange(neg_null_pos, d) |> print(n = 30)

psych_proportions  <- psychology_studies |>
  sum_tab(neg_null_pos)
```

The overall effect for intervention with a psycholgy component is $\Delta$ = `r psychology_model$Delta` (SE = `r psychology_model$se`), p = `r psychology_model$pval`.
Of these `r nrow(psychology_studies)` interventions, `r psych_proportions['0']` are self-reported nulls.
Moreover, the spread of results within the dominant psychological approach (norms interventions) is unusually large.
For example, one standout paper with four included studies, each featuring real-world settings and objectively measured consumption outcomes, finds one significant positive result, two nulls, and one significant backlash [@sparkman2020].
We do not see, in this collection of studies, a clear limiting principle for when norms interventions achieve their goals.

```{=tex}
\begin{comment} say something about dannenberg 2024 or the 2024 meta-analysis that finds vey little?
\end{comment}
```
## The evidence for choice architecture on MAP consumption is promising, but scant {#sec2.5}

Although nudges are common in the diet literature writ large [@olafsson2024; @cadario2020; @szaszi2018], only three nudge studies from two papers [@kanchanachitra2020; @andersson2021] met our inclusion criteria.
Two of three studies had moderate effect sizes on the order of a few percentage points of MAP reduction.
The third had a comparatively large effect ($\Delta$ = `r round(dat |> filter(author == 'Kanchanachitra') |> arrange(desc(d)) |> slice(1) |> pull(d),3)`); that study sought to reduce the fish sauce consumption at a Thai university, and its most effective intervention paired a modified spoon that made it harder for people to serve themselves fish sauce along with a sign enjoining diners to not add more than 2/3 teaspoons of fish sauce to their meal.

```{r broader_choice_model, include=F}
broader_choice_model <- dat |> filter(theory == 'Choice Architecture' | 
                                        str_detect(secondary_theory, 'nudge')) |> 
  map_robust()
```

We restrict our choice architecture analysis to studies that literally alter the architecture of a choice, but two other studies in our dataset describe themselves as pursuing nudges.
The first encourages participants to take a pledge to go vegetarian [@banerjee2019], which implicitly corrects for time-inconsistent preferences, while the second embeds a statement about the proportion of people who chose a vegetarian meal in a specified time period at a university canteen [@griesoph2021], which creates what the authors call a "social comparison nudge." If we include these studies in our choice architecture model, we get a pooled effect size of $\Delta$ = `r broader_choice_model$Delta` (SE = `r broader_choice_model$se`), p = `r broader_choice_model$pval` gleaned from 8 interventions with approximately `r broader_choice_model$N_subjects` subjects.

```{=tex}
\begin{comment} maybe say something about Had our search extended to 2024, we would have likely included many more studies, many of which find small or null effects [cite]
```
## Health studies work better for RPM than for MAP {#sec2.6}

```{r health_studies, echo=F, include=F}
health_studies <- dat |> 
  filter(str_detect(secondary_theory, 'health')) |> 
  select(author, year, secondary_theory, d, se_d, 
         self_report, neg_null_pos) |> 
  arrange(neg_null_pos, d) |> print(n = 30)

health_proportions <- 
  health_studies |> sum_tab(neg_null_pos)

RPM_health_model <- RPMC |>  
  filter(str_detect(secondary_theory, 'health')) |> map_robust()

# health studies form the majority of RPM studies
RPM_health_numbers <- RPMC |> mutate(health_component = if_else(str_detect(secondary_theory, 'health'), T,F)) |> 
  sum_tab(health_component)
```

The pooled effect size for persuasion studies with a health component is $\Delta$ = `r health_model$Delta` (SE = `r health_model$se`), p = `r health_model$pval`.
This is small and not significant, albeit larger than the overall pooled effect.

Health appeals are a component of `r RPM_health_numbers['TRUE']` of `r nrow(RPMC)` interventions aimed at reducing RPM consumption, and are generally more effective there: $\Delta$ = `r RPM_health_model$Delta` (SE = `r RPM_health_model$se`), p = `r RPM_health_model$pval`.
This fits with the broader context where official nutritional guidelines typically encourage consumers to reduce RPM and consume moderate amounts of lean meat and fish.

We also judge many health studies to be at elevated risk of self-reporting bias.
For example, one study seeks to induce a sense of fear in subjects [@berndsen2005], while others target people who are at risk of cancer [@hatami2018] or cancer survivors [@james2015; @lee2018] with reasons they should change their diet, and then ask subjects to self-report what they have eaten recently.

## Environmental appeals have modest positive effects {#sec2.7}

```{r environment_studies, include=F}
# exploratory
# do environmental studies work better for certain groups?
# my intuition would be they work best for college students, & Jalil study is nice evidence of that,
# but the data don't reflect that
dat |> filter(str_detect(secondary_theory, 'environment')) |> 
  split(~population) |> map(map_robust)

dat |> filter(str_detect(secondary_theory, 'animal')) |>  
  mutate(ad_yes_no = if_else(advocacy_org == 'N', FALSE, TRUE)) |> 
  split(~ad_yes_no) |> map(map_robust)

jalil_data <- dat |> filter(author == 'Jalil')
```

The pooled effect size for persuasion studies with an environmental component is $\Delta$ = `r environment_model$Delta` (SE = `r environment_model$se`), p = `r environment_model$pval`.
The strongest evidence that these appeals produce real-world impacts is [@jalil2023], which substituted an introductory lecture in a first-year economics class for a lecture on the environmental and health consequences of meat, focusing mostly on the environment, and then tracked student meal choices in dining halls for three years following.
That study found that treatment led to an overall reduction in MAP consumption of 5.6% ($\Delta$ = `r round(jalil_data$d, 3)` (SE = `r jalil_data$se_d`)).
Due to this study's exceptional commitment to long-term, oblique outcome measurement, we consider it to be reasonably strong evidence for this intervention's efficacy among the population from which the sample was drawn.

## Animal welfare appeals are almost always ineffective {#sec2.8}

```{r animal_welfare_studies, include=F}
animal_proportions  <- dat |> filter(str_detect(secondary_theory, 'animal')) |>
  sum_tab(neg_null_pos)

animal_proportions['0']

neg_animal_results <- dat |> filter(str_detect(secondary_theory, 'animal')) |> filter(d<0) |> count()

advocacy_model <- dat |> filter(advocacy_org != 'N') |> map_robust()
```

The pooled effect size for persuasion studies with an animal welfare component is $\Delta$ = `r animal_model$Delta` (SE = `r animal_model$se`), p = `r animal_model$pval`.
A full `r animal_proportions['0']` of `r animal_model$N_interventions` interventions in this category are self-described nulls.
Slightly more than half (`r neg_animal_results` of `r animal_model$N_interventions`) lead to increases in MAP consumption, though just one of these effects is statistically significant.

The `r advocacy_model$N_studies` studies and `r advocacy_model$N_interventions` interventions using materials from advocacy organizations find an overall effect of `r advocacy_model$Delta` (SE = `r advocacy_model$se`), p = `r advocacy_model$pval`.

## Mixed evidence of effect delay over time {#sec2.9}

A key outcome for our paper is whether MAP reduction interventions are habit-forming.
A large effect observed immediately [@hansen2019] could plausibly dissipate quickly and create no enduring changes.
On the other hand, a small push in the right direction might start a cascade of changes leading to permanent dietary change.

```{r effect_decay, include=F}
delay_model <- dat |> sum_lm(d, delay)
endline_delay_model <- dat |> sum_lm(d, delay_post_endline)
```

In our dataset, we observe mixed, inconclusive evidence of effect decay over time.
First, there is a tiny, positive relationship between number of days separating treatment onset from outcome measurement ($\beta$ = `r delay_model[2,1]`), and a tiny , negative relationship between number of days separating treatment *conclusion* from measurement ($\beta$ = `r endline_delay_model[2,1]`).
We consider these conflicting results a wash.

Another way to consider effect decay is to look at studies which measure consumption at multiple time points [e.g. @bianchi202022; @bochmann2022 @bschaden2020, @carfora2023; @jalil2023].
We note that for the most part, effects seem to decline over time; [@jalil2023], where effects persist for at least three years, is a prominent counter-example.
However, because most of the studies we looked at featured attrition over time, we do not place too much stock in this result.

```{=tex}

\begin{comment} There is probably a way to do this quantitatively and if we get asked to do it in review, I'll do it, but it's a fair bit of work and I think we've made the general point 
\end{comment}
```
## Persuasion messages are more persuasive in conjunction with psychological appeals {#sec2.10}

```{r persuasion_psych_papers}
psych_studies_papers <- dat |> 
  filter(theory == 'Persuasion & Psychology') |> 
  summarise(count_unique_paper_id = n_distinct(unique_paper_id)) |> pull(count_unique_paper_id)
```

The conjunction of persuasion messages with psychological appeals is, in this dataset, the best supported theory for producing consistent, albeit small changes in dietary behavior: `r persuasion_psychology_model$Delta` (SE = `r persuasion_psychology_model$se`), p = `r persuasion_psychology_model$pval`.
This is about twice as large as the pooled effect size from persuasion messages on their own and about one-third larger than psychological appeals on their own.
However, with just `r`persuasion_psychology_model$N_studies`studies from`r psych_studies_papers` to go on, we do not consider this an especially robust result, and we note that the pooled effect size is still substantively small.
Nevertheless, we see this as tentative evidence that theoretical innovation in the form of synergizing multiple theories is a promising avenue of MAP reduction research.

## Heterogeneity by reporting, cluster assignment, delivery method, and country {#sec2.11}

```{r confounding_check, include=F}
confound_table <- dat |> filter(self_report == 'N') |> 
  group_by(str_detect(population, 'university')) |> 
  summarise(count = n()) |> as_tibble()
```

Our sample of studies is comparatively small, and many differences between studies are confounded.
For example, `r confound_table$count[[2]]` of `r confound_table$count[[2]] + confound_table$count[[1]]` interventions with objectively reported outcomes are also studies of university populations.
Nevertheless we offer a few tentative explorations of potential moderators of effect size.

```{r heterogeneity_analyses, include=F}

self_report_table <- dat |> split(~self_report) |> map(map_robust)  |> 
 bind_rows(.id = 'self_report')

cluster_results <- dat |> split(~cluster_assigned) |> map(map_robust)
delivery_table <- table(dat$delivery_method)

# "There was no meaningful relationship between delay and effect size"
dat |> sum_lm(d, delay)

# university vs adults
university_results <- dat |> filter(str_detect(population, 'university')) |> map_robust()
adult_results <- dat |> filter(str_detect(population, 'adult')) |> map_robust()
```

Contrary to our expectations, self-reported and objectively collected outcomes were not meaningfully different: `r self_report_table[self_report_table$self_report == "N", "Delta"]` for objectively reported vs `r self_report_table[self_report_table$self_report == "Y", "Delta"]` for self-reported.
Likewise, the difference in effect sizes for studies where treatment was assigned to clusters (e.g. cafeteria or day of treatment) vs. individuals is small.

Table 4 displays the effect size associated with the four most common delivery mechanisms in our dataset, which were also the groups with enough clusters for meta-analysis to be viable.

`r table_four`

Table 5 displays effects associated with different regions.

`r table_five`

Interventions with adult subjects are moderately more effective ($\Delta$ = `r adult_results$Delta` (SE = `r adult_results$se`), p = `r adult_results$pval`) than those with a university population ( ($\Delta$ = `r university_results$Delta` (SE = `r university_results$se`), p = `r university_results$pval`).

# Methods {#sec3}

**Search**: We employed a multi-pronged search strategy for assembling our database.
First, we checked the bibliographies of three recent reviews [@mathur2021meta; @bianchi2018conscious; @bianchi2018restructuring] for relevant studies.
Second, we checked any possibly relevant study that either cited or was cited by studies we from our first round of coding Third, we checked the bibliographies of authors whose studies we coded.
Fourth, we contacted leading researchers in the field with our in-progress database to see if we had missed any.
Fifth, we repeated steps two and three with new studies we had.
Sixth, we searched Google Scholar for terms that had come up in studies repeatedly (e.g. "dynamic norms + meat", "MAP reduction", and "plant-based diet + effective").

```{=tex}
\begin{comment} 
does this need more description? This is not entirely reproducible I think but TBH it was not a major source of studies in our database
\end{comment}
```
Sixth, we checked database emerging from a parallel project being conducted by Rethink Priorities.
Seventh, we identified a further 130+ systematic reviews and checked their bibliographies, and obtained qualified studies from six of those reviews [@ammann2023; @chang2023; @DiGennaro2024; @harguess2020; @ronto2022; @wynes2018].
Eighth, we used an AI search tool (<https://undermind.ai>) to check for gray literature.
All three authors contributed to the search.

**Coding:** For quantitative outcomes, we selected the latest possible outcome that had enouugh subjects to meet our inclusion criteria.
Sample sizes were drawn from the same post-test.
All effect sizes were standardized by the standard deviation of the outcome for the control group at baseline whenever possible (Glass's $\Delta$).
All effect size conversions were conducted by the first author using methods and R code initially developed for previous papers [@paluck2019; @paluck2021; @porat2024] using standard techniques from [@cooper2019], with the exception of a difference in proportion estimator created for [@paluck2021] and explained in our appendix.

**Meta-analysis:** Our initial set of analyses was pre-registered on the Open Science Framework in November 2023 (<https://osf.io/j5wbp>), although the project evolved substantially over time.
We describe four important deviations in our appendix.
Our analyses use functions and models from the `robumeta` [@fisher2015], `metafor` [@viechtbauer2010], and `tidyverse` [@wickham2019] packages in `R` [@Rlang].
Our meta-analyses use robust variance estimation methods [@hedges2010].

# Discussion {#Sec4}

We offer three lenses through which to view our results.

First, one might focus on the small effect sizes and the moderate evidence of publication bias and conclude that what meager effects we do detect are likely overestimates, and therefore conclude that the true effect being estimated in this dataset is a null.

Second, one might argue that our assembled database of studies *is* successfully changing consumption behavior, but in ranges too small for most studies to detect.
By this light, future studies should replicate existing approaches with sufficient power to detect much smaller effects.

Moreover, a change of a few percentage points might be significant in some contexts.
For example, if a college aiming to reduce its carbon output might calculates that meat consumption accounts for 20% of its carbon emissions, a 5.4% reduction in meat consumption [@jalil2023] would achieve about a 1% reduction in carbon output.
Whether this is comparatively cost-effective depends on the other options available.

```{r largest_eff_sizes, include=F}
large_effs <- dat |> filter(d>=0.5) |> 
  select(author, year, d, se_d, theory, secondary_theory, 
         n_t_post, n_c_post,  n_t_total_pop, n_c_total_pop)

average_total_large_effs_n <- round(median(large_effs$n_t_total_pop)) + round(median(large_effs$n_c_total_pop))

```

Third, one might look at the largest effect sizes in our dataset, for instance, the seven interventions with an effect size of $\Delta \geq 0.5$ [@bianchi2022; @carfora2023; @kanchanachitra2020; @merrill2009; @piester2020] and seek to replicate and/or expand their approaches.
However, we'd caution that these seven interventions are comparatively small, with a median of `r average_total_large_effs_n` subjects.

We do not have a clear preference between these interpretations.
However, we are generally encouraged by trends in this literature.

First, as previously noted, a majority of studies that meet our inclusion criteria have been published in the past few years, suggesting an overall increase in attention to design and measurement validity.
Second, we applaud researchers in this field for publishing null results when they find them.
Third, we notice that the universe of possible interventions and settings is much broader than those we analyzed, suggesting that some promising approaches await rigorous evaluation.
e.g. direct contact with animals on an animal sanctuary, price gradations, high-intensity vegan meal planning, door-to-door canvassing, or studies taking place in diverse settings such as retirement homes.
Fourth, the tentative but promising results from studies that combine persuasive messages with psychological appeals suggest that theoretically synergistic approaches are a promising path forward.

\backmatter

\bmhead{Supplementary information}

All code data, and documentation are available on GitHub (\url{https://github.com/setgree/vegan-meta}) and Code Ocean [LINK]

\bmhead{Acknowledgments}

*Thanks to Alex Berke, Alix Winter, Anson Berns, Hari Dandapani, Adin Richards, Martin Gould, and Matt Lerner for comments on an early draft. Thanks to Jacob Peacock, Andrew Jalil, Gregg Sparkman, Joshua Tasoff, Lucius Caviola, Natalia Lawrence, and Emma Garnett for help with assembling the database and providing guidance on their studies. We gratefully acknowledge funding from the NIH (grant XXX) and Open Philanthropy (YYY).*

# Declarations {.unnumbered}

\newpage

# Appendix

## Supplementary Methods

### Converting difference in proportions to standardized mean difference

Conventional methods of converting binary outcomes to estimates of standardized mean difference have some notable downsides, e.g. any given odds ratio is compatible with multiple possible effect sizes depending on the rate of occurrence of the dependent variable [@gomila2021].
We address this by treating all binary variables as draws from a Bernoulli distribution with $p(1 - p)$, where p is the proportion of some event's occurrence.
For example, if 50% of the treatment group ate vegetarian meals vs 45% for the control group, then Glass's $\Delta = \frac{0.05}{\sqrt{0.45 * (1-0.45)}} = 0.1$.

### Four deviations from pre-analysis plan

Our pre-analysis plan registered some general principles and hypotheses for our search process, but did not otherwise do much to constrain how or where we searched.
It also included a synthesized dataset and some mock analyses that resemble our final analyses in general form.
However, as the project evolved over time, we made four substantive changes to our paper that we did not anticipate at the pre-analysis stage.

First, our initial draft combined RPM and MAP studies, taking the former as providing face value estaimtes of the latter.
However, we later decided that RPM reduction was fundamentally a separate estimand, and that without firm data on substitution to other kinds of MAP, we could not say anything definite about the net effect on demand for MAP.

Second, and related, we initially included studies that were not necessarily aimed at achieving overall MAP reduction, but rather a generally healthier diet with some amount of chicken and/or fish, for instance the Mediterranean diet.
Many of these studies had otherwise qualifying measurement strategies and were generally highly powered and well-designed [@beresford20].
However, we ultimately concluded that these were a separate estimand as well, and we did not want to add noise to the estimate of studies that were aimed more specifically at reducing MAP consumption rather than causing inter-MAP substitution. Further, we almost all of these studies featured self-reported data, and comparatively few tracked all relevant categories of MAP.

```{r alt_model, include=F}
alt_model <- dat |> map_robust(model = 'RMA')
```

Third, our initial analyses used the random effects model from `metafor` to calculate pooled effect sizes.
However, as we assembled our dataset, we noticed that many papers had, across interventions, non-independent observations, typically in the form of multiple treatments compared to a single control group.
Upon discussion, the team's statistician (MBM) suggested that the `CORR` model from the `robumeta` package would be a better fit.

Using our original model from `metafor`, we detect a pooled effect size of `r alt_model$Delta` (SE = `r alt_model$se`), p = `r alt_model$pval`.
In relative terms, this is substantially smaller, but in absolute terms, both this model and our main model produce very small estimates.
Table S2 provides an overview of alternate estimates by our main theoretical approaches.

Fourth, we added many moderators to our dataset that we did not plan on, such as a broad category for delivery method, whether a study was intended to be emotionally activating, or whether a program had multiple components.
We did not end up focusing on these in our main paper but include them in our dataset in case they are of interest.

## Supplementary Figure

This figure displays the relationship between standard error and effect size.
The colors correspond to theoretical approach and the shapes correspond to the venue where results were published.

```{r supplementary_figure, echo=F, message=F, out.width='120%'}
supplementary_figure
```

## Supplementary Tables

Table S1 displays the category of source where we learned of papers in our main dataset.
\captionsetup[table]{labelformat=empty}

```{r supp_table_one}
supplementary_table_one
```

Table S2 displays the pooled effect size by theoretical approach when using standard random effects estimation methods from the `metafor` package (rather than the robust variance estimation methods we ended up using from the `robumeta` package).

```{r supp_table_two}
supplementary_table_two
```

## Supplementary Discussion

### The limits of systematic search in the MAP reduction literature

The MAP reduction literature has remarkable methodological, disciplinary, and theoretical diversity.
However, it also has few if any agreed upon terms to describe itself
For instance,the term "MAP" is not standard; other papers discuss animal-based proteins, animal products, meat, edible animal products, plant-based foods, plant-based protein, and so on.
This diversity of language poses a particular challenge for anyone seeking to systematically review this literature because
whether one has identified the correct terms that each relevant study uses to describe itself is, for all practical purposes, unknowable.

This informed our search process.
Rather than starting with a list of search terms, we began by reading prior reviews, and then reading the studies cited by those reviews, to get a sense of the language that studies used to describe themselves.
We then pursued the multi-prongded, iterative search process described in the main text.
Ultimately, we used systematic search techniques to fill in the the blanks when we had an intuition that we were missing studies employing a particular approach.

The following are the Google Scholar search terms we used:

-   random nudge meat
-   meat purchases information nudge
-   nudge theory meat purchasing
-   meat alternatives default nudge
-   dynamic norms meat
-   norms animal products

For each of these terms, we looked through ten pages of results.

### Edge cases for study inclusion

Arguably the hardest decision in meta-analysis is what studies to include or exclude.
By far the most common reason for exclusion was category of dependent variable (e.g. measuring attitudinal or intentional outcomes).
However, many cases were harder and required some discussion.
Here are a few cases we found difficult.

Some studies limit dietary portions or switch what people are served (e.g. children being served more vegetables at lunchtime).
We did not include these studies because they were essentially guaranteed to have effects that were either positive or at least bounded at zero.
However, we did include studies that provided free access to meat alternatives [@acharya2004; @bianchi2022] and measured outcomes after the intervention had concluded.
(There were not enough of these studies in our main dataset to analyze this approach separately, but their effect sizes are `dat |> filter(author == 'Acharya') |> pull(d)` and `dat |> filter(author == 'Bianchi') |> pull(d)` respectively.)

Other studies induce a form of treatment in their control groups, for instance asking all subjects to take a pledge to go vegetarian or encouraging them not to change their diet over the course of study, which could potentially induce changes relative to baseline consumption patterns.
Our main database only includes studies with a pure control group or an unrelated placebo.
However, we include a selection of studies in our supplementary robustness check.

Another common design limitation we encountered was treatment assigned at the level of alternating weeks at a cafeteria.
Generally these studies did not have enough weeks to meet our sample size requirements, but we also note that simply alternating weeks is not random assignment, and it is possible that consumption patterns in these studies will not be equivalent between groups in expectation.

Last, we encountered many studies that measured fruit and or/vegetable consumption but not MAP consumption.
In some cases, it might have been possible to add assumptions about substitution and estimate effect sizes, but we exclusively meta-analyzed studies that reported this information directly.


## Robustness check: including X additional studies that with near-random assignment

While reviewing papers, we identified `r robust_num_papers` high-quality studies that did not meet our search criteria for one of several reasons The most common were that they were underpowered (typically too few clusters), not fully randomized (e.g. treatment was altered by week but not randomly), lacking in a control group (meaning they compared two MAP reduction treatments), or without delay between treatment onset and measurement. One additional study encouraged participants to switch to fish. 

By and large these st

Tthe possibility that a subject who is guided towards eating less meat at one meal via a strong default eats more at the next, i.e. "regression to the meat." Most cafeteria-based studies we looked at were not designed to measure this kind of spillover effect, although such designs are possible [@vocski2024]. For the studies with zero delay between treatment onset and measurement, we believe that effects are likely to attenuate over time. We encourage researchers to design future default studies with this challenge in mind.

-   much larger effect size on average in these studies
-   still only looking at consumption outcomes
-   robustness studies get excluded for one of eight reasons: horse race design, no delay, underpowered, not randomized randomization issue, statistical issue, encourages substitution to fish, and portion size change
-   mention that the default studies lead the way here but we are concerned about net effect not effect on one meal. What happens after the default gets changed is not clear. If it's a great vegan meal, it might be habit-forming in the good direction. If it's a bad one, it might be habit-forming in the wrong direction. If it's perceived as "not quite a full meal," people might be more inclined to eat meat at the next meal. Who knows?
-   do effect sizes by each exclusion category? (or at least the main ones)
-   amalgamate everything -- get bigger effect size. amalgamate the RPMC studies -- get even bigger effect sizes

[TO FILL IN]

### Defining the the theoretical boundaries between studies requires judgment calls

Tallying how many studies pursue a given theory of change requires defining those theories and drawing boundaries between them.
This proved tricky in some cases.
For instance, the words 'choice architecture' and 'nudge' [@thaler2009] are not necessarily interchangeable in this literature.
Many of the studies we looked at that described themselves as implementing nudges were not necessarily altering anything about the architecture of a choice, and neither were they obviously seeking to operate on 'unconscious' processes [@garnett2020].
For instance, a text message reminder of reasons to eat less meat has a nudge *feel* to it -- it is cheap and easy to ignore -- and one could argue that the purpose of such an intervention is to correct for time-inconsistent preferences, which is a kind of unconscious bias.
Moreover, [@hausman2010] define nudges as "ways of influencing choice without limiting the choice set or making alternatives appreciably more costly in terms of time, trouble, social sanctions, and so forth" (p. 126)," and by this definition, a daily text reminder of reasons not to eat meat is a nudge.

On the other hand, such a text message also provides relevant information about the choice set, and if every intervention that attempted this was a nudge, essentially every study in our database would be a nudge.
We decided that the clearest boundary was between studies that altered the literal architecture of a choice, and therefore were plausibly working on unconscious processes, rather than interventions that tried to alter how people think or feel about what they're eating.

Likewise with social norms messages.
[@mols2015] identify "unthinking conformity" as an example of a "human failing" that nudges take as their "starting point" (p. 4), and if social norms activate an unthinking desire to conform, then arguably a message about how many people are going vegetarian in one's community is a nudge.
Our view, however, is that a social norm prompt might engender a rich array of possible reactions, both cognitive and affective, and we do not assume that "unthinking conformity" is the dominant or exclusive response.
Therefore, we do not classify the norms interventions in our database as nudges.

A future project might investigate exactly what reactions are occurring by asking subjects how well they recall a norms message and what it made them think about.
A high prevalence of subjects who are unable to recall the message's specifics but nevertheless cut back on MAP consumption would be evidence that norms are acting through automatic rather than reflective processes.'

Reasonable people might have defined the theoretical boundary conditions differently.
For instance, rather than grouping psychology approaches together, one might separate \_inter_personal processes (norms) from purely personal processes, e.g. pledges, implementation intentions, or response inhibition training.
For this reason, we included in our dataset both `theory` and `secondary_theory` columns, and in the latter we include more specific information about papers' approaches to behavior change.
We invite readers to explore different categories and their respective pooled effect sizes by building on the code and data we provide.

### Notes on prior reviews

It was striking to us there are many more systematic reviews of dietary research than there are studies meeting our search criteria.
This is a rectifiable imbalance.
We encourage scholars to pursue more randomized controlled trials with consumption outcomes.

Findings in previous studies vary widely in scope and implications.
Many reviews have concluded that various classes of interventions show promise for causing diet change.
However, the wide array of studies reviewed warrants some skepticism.
For instance, we encourage future researchers to distinguish carefully between hypothetical consumption outcomes — what someone says they would select from a proposed menu — from actual consumption data.

Here we provide a narrative overview of a selection of prior reviews that were of high relevance to this one.

Among the reviews that found MAP reduction interventions to be effective, several focused exclusively on choice architecture.
[@arno2016] found that nudges led to an average increase of healthy dietary choices of 15.3%, while [@byerly2018] found committing to reduce meat intake and making menus vegetarian by default to be much more effective than educational interventions.
However, the vast majority of vegetarian-default studies we reviewed did not qualify for our analysis because they lacked delayed outcomes, and their net effect on MAP consumption is unknown.

[@bianchi2018restructuring] found that reducing meat portions, making alternatives available, moving meat products to be less conspicuous, and changes to sensory properties can reduce meat demand.
[@pandey2023] found that changing the presentation and availability of sustainable products was effective, as was providing information.

In a meta-review, [@grundy2022] found environmental education to be most promising, with substantial evidence also supporting health information, emphasizing social norms, and decreasing meat portions.

Some reviews have focused on particular settings for MAP reduction interventions.
[@hartmannboyce2018] found that grocery store interventions such as price changes, suggested swaps and changes to item availability are effective at changing purchasing choices.
However, that review covered a variety of health interventions such as reducing fat consumption and increasing fruit and vegetable purchases.
It is unclear how directly such findings translate to MAP reduction efforts.

[@chang2023] focused on university meat-reduction interventions and found more promising results than other reviews that studied the wider public.
This suggests that students and young people may be particularly receptive to MAP reduction interventions.
[@harguess2020] reviewed 22 studies on meat consumption and found promising results for educational interventions focused on the environment, health, and animal welfare.
They recommend using animal imagery to cause an emotional response and utilizing choice architecture interventions.
Our review, by contrast, found essentially no relationship between animal welfare appeals and MAP consumption.

Taking a different angle, [@adleberg2018] reviewed the literature on protests in a variety of movements and found that mixed evidence of efficacy.
The authors recommend that animal advocacy protests have a specific target (e.g. a particular institution) and "ask."

Other studies provide insights on who is most easily influenced by interventions to reduce MAP consumption.
For example, [@blackford2021] found that nudges focused on "system 1" thinking were more effective at encouraging sustainable choices than those focused on "system 2," and that interventions had greater effects on females than males.
Our review also featured studies showing differences between men and women.

[@rosenfeld2018] reports that meat avoidance is associated with liberal political views, feminine gender, and higher openness, agreeableness and neuroticism.
That review also identifies challenges and barriers to vegetarianism, such as recidivism and hostility from friends and family.
Future research could tailor interventions to address these barriers, such as by focusing on commitment devices to reduce recidivism.

Several reviews have had mixed or inconclusive results.
For instance, [@bianchi2018conscious] found that health and environmental appeals appear to change dietary intentions in virtual environments, but they did not find evidence of actual consumption changes.
In the same vein, [@kwasny2022] notes that most existing research focuses on attitudes and intentions and lacks measures of actual meat consumption over an extended period of time.
[@taufik2019] reviewed many studies on increasing fruit and vegetable intake, but found far fewer on reducing animal consumption.

[@benningstad2020] found, in a systematic review, that dissociation of meat from its source plays a role in meat consumption, but no extant research that included behavioral outcomes. [@graca2019] developed a theoretical framework for understanding MAP reduction, finding variables worth investigating further in future studies, while [@pitt2017] provide insights on how food environments influence consumer choices. That paper did not draw specific conclusions about reducing MAP consumption.
A few reviews have found evidence that seems to recommend against particular interventions. [@greig2017] reviewed the literature on leafleting for vegan/animal advocacy outreach, and observed biases towards overestimating impact. That paper concluded that leafleting does not seem cost-effective, though with significant uncertainty. This accords with our findings on advocacy organizations' limited effects.

[@nisa2019] meta-analyzed interventions to improve household sustainability, of which reducing MAP consumption was one of several. Although they found small effect sizes for most interventions, they concluded that nudges were comparatively effective. Many such nudge studies looked at meat consumption. Similarly, [@rau2022] reviewed the literature on environmentally friendly behavior changes, including but not limited to diet change, and found small or nonexistent effects in most cases. Only 15 interventions were described as “very successful,” and none of these related to food.

Finally, we note that a forthcoming meta-analysis of dynamic norms interventions concludes that their overall effects on MAP consumption are negligible [@Weikertova2024].

\newpage

# References
