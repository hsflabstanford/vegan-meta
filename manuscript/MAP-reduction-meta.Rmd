---
classoptions: 
  - sn-nature      
  # - referee       # Optional: Use double line spacing 
  # - lineno        # Optional: Add line numbers
  # - iicol         # Optional: Double column layout
title: "Meaningfully reducing consumption of meat and animal products is an unsolved problem: results from a meta-analysis"
titlerunning: MAP-reduction-meta
authors: 
  - firstname: Seth Ariel
    lastname: Green
    email: setgree@stanford.edu
    affiliation: 1
    corresponding: TRUE
  - firstname: Maya B.
    lastname: Mathur
    affiliation: 1
  - firstname: Benny
    lastname: Smith 
    affiliation: 2
affiliations:
  - number: 1
    info:
      orgdiv: Humane and Sustainable Food Lab
      orgname: Stanford University
  - number: 2
    info:
      orgname: Allied Scholars for Animal Protection 
keywords:
  - meta-analysis
  - meat
  - plant-based
  - randomized controlled trial
  
abstract: |
  Which theoretical approach leads to the broadest and most enduring reductions in consumptions of meat and animal products (MAP)? We address these questions with a theoretical review and meta-analysis of rigorous Randomized Controlled Trials (RCTs). We meta-analyze 33 papers comprising 39 studies,107 interventions, and approximately 87000 subjects. We find that these papers employ nudge, norms, or persuasion approaches to changing behavior. The pooled effect of these interventions on MAP consumption outcomes is $\Delta$ = 0.058, indicating an unsolved problem. Reducing consumption of red and processed meat is an easier target: $\Delta$ = 0.249, but because of missing data on potential substitution to other MAP, we can’t say anything definitive about the consequences of these interventions on animal welfare. We further explore effect size heterogeneity by approach, population, and study features. We conclude that while no theoretical approach provides a proven remedy to MAP consumption, designs and measurement strategies have generally been improving over time, and many promising interventions await rigorous evaluation.
date: "`r Sys.Date()`"
output: 
  rticles::springer_article:
    keep_tex: true
bibliography: "./vegan-refs.bib"
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{comment}
  - \usepackage{anyfontsize}
  - \usepackage{caption}
---

---

```{r setup, include=FALSE}
# so that knitr labels figures
knitr::opts_chunk$set(
	fig.path = "./figures/",
	echo = FALSE,
	out.extra = ""
)
# so we can put the manuscript stuff in its own folder
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(rprojroot::find_rstudio_root_file())
options(tinytex.clean = TRUE) # switch to FALSE to get the bbl file

source('./scripts/libraries.R')
source('./scripts/functions.R')
source('./scripts/load-data.R')
source('./scripts/red-and-processed-meat.R')
source('./scripts/models.R')
source('./scripts/tables.R')
source('./scripts/figures.R')

```

# Introduction {#sec1}

Consumption of meat and animal products (MAP) is increasingly recognized as a major contributor to premature deaths [@willett2019; @landry2023], public health risks [@slingenbergh2004; @graham2008], ecological harms [@greger2010] and climate change [@scarborough2023; @koneswaran2008] as well as an ethical crisis in its own right [@kuruc2023; @singer2023].

Supply-side interventions, such as banning or taxing certain practices or products, risk political backlash if they lack broad public support.
It is of vital importance, therefore, to assess which strategies and theoretical perspectives lead to consistent reductions in demand for MAP, under which conditions, and for which populations.

The research on diet and its antecedents and consequences is vast.
By our count, there have been at least 120 previous published dietary reviews in the past two decades, with at least 37 focused specifically on MAP reduction.
However, comparatively few of these are quantitative, and most prior reviews investigated particular approaches, for example choice architecture [@bianchi2018restructuring], rather than comparing approaches to one another.
Moreover, two prior investigations revealed three common gaps in the MAP reduction literature: a dearth of long-term follow-ups, missing consumption outcomes, and inattention to the gap between intentions and behavior [@mathur2021meta; @mathur2021effectiveness].

Our paper addresses these concerns by meta-analyzing randomized controlled trials (RCTs) that

-   were designed to voluntarily reduce MAP consumption, rather than encouraging substitution from red meat to white meat or to fish, or removing items from someone's plate

-   had least 25 subjects each in treatment and control, or, for cluster-randomized trials, at least 10 clusters in total;

-   measured MAP consumption, whether self-reported or observed directly, rather than (or in addition to) attitudes, intentions, beliefs or hypothetical choices;

-   recorded outcomes at least a single day after the start of treatment.

Additionally, studies needed to be publicly circulated by December 2023 and published in English.

```{r useful_constants}
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
num_interventions <- as.numeric(nrow(dat))

n_total <-  round_to(x = sum(dat$n_c_total_pop) + sum(dat$n_t_total_pop), 1000, floor)

decade_tab <- dat |> group_by(unique_paper_id) |>  slice(1) |>  ungroup() |> count(decade)
```

We coded `r num_papers` such papers [@aldoh2023; @allen2002; @alblas2023; @coker2022; @griesoph2021; @piester2020; @sparkman2017; @sparkman2020; @andersson2021; @kanchanachitra2020; @bochmann2017; @bschaden2020; @cooney2016; @feltz2022; @haile2021; @mathur2021effectiveness; @peacock2017; @polanco2022; @sparkman2021; @abrahamse2007; @acharya2004; @berndsen2005; @bertolaso2015; @bianchi2022; @fehrenbach2015; @hatami2018; @jalil2023; @merrill2009; @norris2014; @weingarten2022; @carfora2023; @hennessy2016; @mattson2020] comprising `r num_studies` separate studies, `r num_interventions` interventions, and approximately `r n_total` subjects.
(This is an approximation because some interventions were administered at the level of day or cafeteria and did not record a precise number of human subjects.) The earliest paper was published in 2002 [@allen2002], and a majority (`r decade_tab$n[3]` of `r num_papers`) have been published since 2020.

```{r red_meat_numbers, include=F}
RPMC_papers <- as.numeric(max(RPMC$unique_paper_id))
RPMC_studies <- as.numeric(max(RPMC$unique_study_id))
RPMC_interventions <- as.numeric(nrow(RPMC))
n_meat_total <-  plyr::round_any(x = sum(RPMC$n_c_total_pop) + sum(RPMC$n_t_total_pop), 1000, floor)

```

We also coded a supplementary dataset of `r RPMC_papers` papers aimed at reducing, and measuring, consumption of red and/or processed meat (RPM) [@carfora2017correlational; @carfora2017randomised; @carfora2019; @carfora2019informational; @delichatsios2001; @dijkstra2022; @emmons2005cancer; @emmons2005project; @jaacks2014; @james2015; @lee2018; @perino2022; @schatzkin2000; @sorensen2005], comprising `r RPMC_studies` studies, `r RPMC_interventions` interventions, and approximately `r n_meat_total` subjects. We also coded an additional XXX papers that we disqualified for methodological reasons but that we include in a supplementary robustness check.
Last, we compiled a third dataset of over 780 excluded studies, along with their reason(s) for exclusion.

Studies in our database pursued three theories of change: norms, nudges, and persuasion, or a combination of norms and persuasion.

**Norms** studies [@aldoh2023; @allen2002; @alblas2023; @coker2022; @griesoph2021; @piester2020; @sparkman2017; @sparkman2020] manipulate the perceived popularity of desired outcomes, e.g. plant-based dishes [@sparkman2017].
Norms might be descriptive ("33% of British people...successfully engaged in one or more...behaviours to eat less meat" [@aldoh2023]), injunctive (a message with a frowning face for subjects who eat more meat than the average person in their country [@alblas2023]), or dynamic, i.e. they tell subjects that the number of people engaging in desired behavior is increasing [@aldoh2023; @coker2022; @sparkman2017; @sparkman2020].
The first norms study meeting our criteria was published in 2017.

**Nudge** studies [@andersson2021; @kanchanachitra2020] manipulate aspects of physical environments to make non-MAP options more salient, such as placing a vegetarian meal at eye level on a billboard menu [@andersson2021] or making it more laborious for people to serve themselves fish sauce [@kanchanachitra2020].

**Persuasion** studies [@kanchanachitra2020; @abrahamse2007; @acharya2004; @berndsen2005; @bertolaso2015; @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @cooney2016; @fehrenbach2015; @feltz2022; @haile2021; @hatami2018; @hennessy2016; @mathur2021effectiveness; @norris2014; @peacock2017; @polanco2022; @sparkman2021; @jalil2023; @merrill2009; @weingarten2022] appeal directly to people to eat less MAP.
These studies formed the majority of our database.
Arguments focus on health, the environment (usually climate change), and animal welfare.
Some are designed to be emotionally activating, e.g. presenting upsetting footage of factory farms [@bertolaso2015], while others present facts about, e.g., the relationship between diet and cancer [@hatami2018].
Many persuasion studies combine arguments, such as a lecture on the health and environmental consequences of eating meat [@jalil2023] or a leaflet with information in all three categories [@hennessy2016].

Finally, a handful of studies combines **norms and persuasion** approaches [@hennessy2016; @carfora2023; @mattson2020; @piester2020].
These interventions typically suggest reasons to eat less meat side by side with information about changing consumer habits in society.

Three papers [@piester2020; @hennessy2016; @kanchanachitra2020] evaluate multiple interventions reflecting contrasting theoretical approaches.
```{=tex}
\begin{comment} 
Note that two interventions also provide free meat alternatives?
\end{comment}
```

# Results {#sec2}

## An overall small effect with some heterogeneity by approach {#sec2.1}

Our overall meta-analytic effect size is $\Delta$ = `r model$Delta` (SE = `r model$se`), p = `r model$pval`.
The aggregate effect is statistically significant, but does not indicate a meaningful reduction.

Figure 1 displays the distribution of effect sizes, grouped by paper, with each individual point representing an intervention.
The overall effect size is plotted at the bottom.

```{r forest_plot, out.width='120%'}
forest_plot
```

Table 1 reports the number of studies, interventions, and subjects (approximately) per approach, as well as the pooled effect sizes per approach.

`r table_one`

Table 2 displays the numbers and findings of persuasion interventions by topic.

`r table_two`

These small effects may surprise readers of previous reviews, which typically found more positive results [@mathur2021meta; @meier2022; @mertens2022].
We attribute this difference to our stricter inclusion criteria.
For instance, of the ten largest effect sizes recorded in [@mathur2021effectiveness], nine were non-consumption outcomes and the tenth came from a non-randomized design.

```{r nulls_and_cis}
effect_size_table <- dat |> sum_tab(neg_null_pos)

significant_CIs <- dat |> select(author, year, d, se_d, neg_null_pos) |>
  mutate(lower_bound = d - (1.96 * se_d),
         upper_bound = d + (1.96 * se_d)) |>
  filter(lower_bound < 0 & upper_bound < 0 |
           lower_bound > 0 & upper_bound > 0) |> 
  mutate(direction = case_when
         (lower_bound > 0 ~ 1,
           upper_bound < 0 ~ 0))

# pap analysis
pap_model <- dat |> filter(public_pre_analysis_plan != 'N') |> map_robust()

## this difference is not statistically significant
dat <- dat |> mutate(pap_yes = if_else(public_pre_analysis_plan == 'N', FALSE, TRUE))

pap_difference <- robumeta::robu(d ~ pap_yes, dat, unique_study_id, var_d)

# open data analysis
open_data_model <- dat |> filter(open_data != 'N') |> map_robust()

dat <- dat |> mutate(open_data_yes = if_else(open_data == 'N', FALSE, TRUE))

open_data_difference <- robumeta::robu(d ~ open_data_yes, dat, unique_study_id, var_d)
```

Per the papers' own calculations, `r effect_size_table['0']` of `r num_interventions` interventions had null effects on net MAP consumption.
However, many studies present a wide variety of outcomes, or include MAP reduction as one of many components of a broader program of behavior change, and present significant results as well.
```{=tex}
\begin{comment}
Using our calculations of effect size and standard error `r (nrow(significant_CIs))` interventions have 95% confidence intervals that do not overlap with zero, `r sum(significant_CIs$direction)` of which are positive effects, out of `r nrow(dat)` interventions.
\end{comment}
```
## Moderate evidence of publication bias {#sec2.2}

We conduct four tests for publication bias.

```{=tex}
\begin{comment} 
introductory remarks about how this puts our main results in one light or another? 
\end{comment}
```
First, in our dataset, $\Delta$ and standard error are positively correlated, though not significantly.

Second, the `r pap_model$N_studies` studies with pre-analysis plans have a marginally smaller effect: $\Delta$ = `r pap_model$Delta` (SE = `r pap_model$se`), p = `r pap_model$pval`.
This difference is not statistically significant.

Third, the `r open_data_model$N_studies` studies with openly available data also have a marginally smaller effect: $\Delta$ = `r open_data_model$Delta` (SE = `r open_data_model$se`), p = `r open_data_model$pval`.
This difference is also not statistically significant.

Fourth, the pooled effect size of interventions published in peer-reviewed journals is about 9 times larger than the equivalent effect size in student theses (table 3).
Interventions published by advocacy organizations produce a small backlash effect on average.

`r table_three`

```{r pbulication_bias_checks, include=F}

# "$\Delta$ and standard error are positively correlated, though not significantly"
dat |> sum_lm(d, se_d)
# a la the contact hypothesis re-evaluated, the implication here is that an infinitely powered
# study would have an effect size of 0.0008

# within journal articles? 
dat |> filter(pub_status == 'Journal article') |> sum_lm()
```

## Red and Processed Meat is an easier target {#sec2.3}

```{r rpm_health_count, include=F}
health_rpm_studies <- as.numeric(nrow(RPMC |> filter(str_detect(secondary_theory, 'health'))))
```

On average, interventions aimed at reducing consumption of red and processed meat (RPM) outperform general MAP reduction interventions: $\Delta$ = `r rpmc_model$Delta` \text{(SE = `r rpmc_model$se`)}, p = `r rpmc_model$pval`.
Each of the RPM reduction studies employs persuasion, and a majority (`r health_rpm_studies` of `r nrow(RPMC)`) appeal to personal health.
However, these studies do not collect data on white meat and/or fish consumption, and therefore their impact on MAP consumption overall is unknown.

RPM is of special concern for its environmental and health consequences [@grummon2023], but some categories of meat considered sustainable are arguably worse for animals on a pound-for-pound basis [@mathur2022ethical].
For some plausible patterns of substitution, these interventions are net positive for health and the environment and net negative for animal welfare.

## Norms work sometimes, but it is not clear why or when {#sec2.4}

```{r include=F, echo=F}
norms_overall_model <- dat |> filter(str_detect(theory, "norms")) |> map_robust()

# look at the distribution of effect sizes

norms_studies <- dat |> filter(str_detect(theory, "norms")) |> select(author, year, theory, d, se_d, intervention_condition, neg_null_pos) |> arrange(neg_null_pos, d) |> print(n = 30)

norms_proportions  <- norms_studies |>
  sum_tab(neg_null_pos)
```

The overall effect for intervention with a norms component is $\Delta$ = `r norms_overall_model$Delta` (SE = `r norms_overall_model$se`), p = `r norms_overall_model$pval`.
Of these `r nrow(norms_studies)` interventions, `r norms_proportions['0']` are self-reported nulls.
Moreover, the spread of norms results is unusually large. For example, one standout paper with four included studies, each featuring real-world settings and objectively measured consumption outcomes, finds one significant positive result, two nulls, and one significant backlash [@sparkman2020].
We do not see, in this collection of studies, a clear limiting principle for when norms interventions achieve their goals.


## The evidence for nudges on MAP consumption is scant {#sec2.5}

Although nudges are common in the diet literature writ large [@olafsson2024; @cadario2020; @szaszi2018], only two nudge studies met our inclusion criteria [@andersson2021].
Both had moderate effect sizes on the order of a few percentage points of MAP reduction.

## Health studies work better for RPM than for MAP {#sec2.6}

```{r health_studies, echo=F, include=F}
health_studies <- dat |> 
  filter(str_detect(secondary_theory, 'health')) |> 
  select(author, year, secondary_theory, d, se_d, 
         self_report, neg_null_pos) |> 
  arrange(neg_null_pos, d) |> print(n = 30)

health_proportions <- 
  health_studies |> sum_tab(neg_null_pos)

RPM_health_model <- RPMC |>  
  filter(str_detect(secondary_theory, 'health')) |> map_robust()

# health studies form the majority of RPM studies
RPM_health_numbers <- RPMC |> mutate(health_component = if_else(str_detect(secondary_theory, 'health'), T,F)) |> 
  sum_tab(health_component)
```

The pooled effect size for persuasion studies with a health component is $\Delta$ = `r health_model$Delta` (SE = `r health_model$se`), p = `r health_model$pval`.
This is small and not significant, albeit larger than the overall pooled effect.

Health appeals are a component of `r RPM_health_numbers['TRUE']` of `r nrow(RPMC)` interventions aimed at reducing RPM consumption, and are generally more effective there: $\Delta$ = `r RPM_health_model$Delta` (SE = `r RPM_health_model$se`), p = `r RPM_health_model$pval`.
This fits with the broader context where official nutritional guidelines typically encourage consumers to reduce RPM and consume moderate amounts of lean meat and fish.

We also judge many health studies to be at elevated risk of self-reporting bias. For example, one study seeks to induce a sense of fear in subjects [@berndsen2005], while others target people who are at risk of cancer [@hatami2018] or cancer survivors [@james2015; @lee2018] with reasons they should change their diet, and then ask subjects to self-report what they have eaten recently.

## Environmental appeals have modest positive effects {#sec2.7}

```{r environment_studies, include=F}
# exploratory
# do environmental studies work better for certain groups?
# my intuition would be they work best for college students, & Jalil study is nice evidence of that,
# but the data don't reflect that
dat |> filter(str_detect(secondary_theory, 'environment')) |> 
  split(~population) |> map(map_robust)

dat |> filter(str_detect(secondary_theory, 'animal')) |>  
  mutate(ad_yes_no = if_else(advocacy_org == 'N', FALSE, TRUE)) |> 
  split(~ad_yes_no) |> map(map_robust)

jalil_data <- dat |> filter(author == 'Jalil')
```

The pooled effect size for persuasion studies with an environmental component is $\Delta$ = `r environment_model$Delta` (SE = `r environment_model$se`), p = `r environment_model$pval`.
The strongest evidence that these appeals produce real-world impacts is [@jalil2023], which substituted an introductory lecture in a first-year economics class for a lecture on the environmental and health consequences of meat, focusing mostly on the environment, and then tracked student meal choices in dining halls for three years following.
That study found that treatment led to an overall reduction in MAP consumption of 5.6% ($\Delta$ = `r round(jalil_data$d, 3)` (SE = `r jalil_data$se_d`)). Due to this study's exceptional commitment to long-term, oblique outcome measurement, we consider it to be reasonably strong evidence for this intervention's efficacy among the population from which the sample was drawn.

## Animal welfare appeals are almost always ineffective {#sec2.8}

```{r animal_welfare_studies, include=F}
animal_proportions  <- dat |> filter(str_detect(secondary_theory, 'animal')) |>
  sum_tab(neg_null_pos)

animal_proportions['0']

neg_animal_results <- dat |> filter(str_detect(secondary_theory, 'animal')) |> filter(d<0) |> count()

advocacy_model <- dat |> filter(advocacy_org != 'N') |> map_robust()
```

The pooled effect size for persuasion studies with an animal welfare component is $\Delta$ = `r animal_model$Delta` (SE = `r animal_model$se`), p = `r animal_model$pval`.
A full `r animal_proportions['0']` of `r animal_model$N_interventions` interventions in this category are self-described nulls.
Slightly more than half (`r neg_animal_results` of `r animal_model$N_interventions`) lead to increases in MAP consumption, though just one of these effects is statistically significant.

The  `r advocacy_model$N_studies` studies and `r advocacy_model$N_interventions` interventions using materials from advocacy organizations find an overall effect of `r advocacy_model$Delta` (SE = `r advocacy_model$se`), p = `r advocacy_model$pval`.

## Heterogeneity by reporting, cluster assignment, delivery method, and country {#sec2.9}

```{r confounding_check, include=F}
confound_table <- dat |> filter(self_report == 'N') |> 
  group_by(str_detect(population, 'university')) |> 
  summarise(count = n()) |> as_tibble()

```

Our sample of studies is comparatively small, and many differences between studies are confounded. 
For example, `r confound_table$count[[2]]` of `r confound_table$count[[2]] + confound_table$count[[1]]` interventions with objectively reported outcomes are also studies of university populations.
Nevertheless we offer a few tentative explorations of potential moderators of effect size.

```{r heterogeneity_analyses, include=F}

self_report_table <- dat |> split(~self_report) |> map(map_robust)  |> 
 bind_rows(.id = 'self_report')

cluster_results <- dat |> split(~cluster_assigned) |> map(map_robust)
delivery_table <- table(dat$delivery_method)

# "There was no meaningful relationship between delay and effect size"
dat |> sum_lm(d, delay)

# university vs adults
university_results <- dat |> filter(str_detect(population, 'university')) |> map_robust()
adult_results <- dat |> filter(str_detect(population, 'adult')) |> map_robust()
```

Contrary to our expectations, self-reported and objectively collected outcomes were not meaningfully different: `r self_report_table[self_report_table$self_report == "N", "Delta"]` for objectively reported vs `r self_report_table[self_report_table$self_report == "Y", "Delta"]` for self-reported.
Likewise, the difference in effect sizes for studies where treatment was assigned to clusters (e.g. cafeteria or day of treatment) vs. individuals is small.

Table 4 displays the effect size associated with the four most common delivery mechanisms in our dataset, which were also the groups with enough clusters for meta-analysis to be viable.

`r table_four`

Table 5 displays effects associated with different regions.

`r table_five`

There was no meaningful relationship between delay and effect size.

Interventions with adult subjects are moderately more effective ($\Delta$ =`r adult_results$Delta` (SE = `r adult_results$se`), p = `r adult_results$pval`) than those with a university population ( ($\Delta$ =`r university_results$Delta` (SE = `r university_results$se`), p = `r university_results$pval`)),

# Methods {#sec3}

**Search**: We employed a multi-pronged search strategy for assembling our database.
First, we checked the bibliographies of three recent reviews [@mathur2021meta; @bianchi2018conscious; @bianchi2018restructuring] for relevant studies.
Second, we checked any possibly relevant study that either cited or was cited by studies we from our first round of coding
Third, we checked the bibliographies of authors whose studies we coded.
Fourth, we contacted leading researchers in the field with our in-progress database to see if we had missed any.
Fifth, we repeated steps two and three with new studies we had.
Sixth, we searched Google Scholar for terms that had come up in studies repeatedly (e.g. "dynamic norms + meat", "MAP reduction", and "plant-based diet + effective"). 
```{=tex}
\begin{comment} 
does this need more description? This is not entirely reproducible I think but TBH it was not a major source of studies in our database
\end{comment}
```
Sixth, we checked database emerging from a parallel project being conducted by Rethink Priorities.
Seventh, we identified a further 100+ systematic reviews and checked their bibliographies, and obtained qualified studies from six of those reviews [@ammann2023; @chang2023; @DiGennaro2024; @harguess2020;@ronto2022;@wynes2018].
Eighth, we used an AI search tool (\url{<https://undermind.ai>}) to check for gray literature.
All three authors contributed to the search.

**Coding:** For quantitative outcomes, we selected the latest possible outcome that had enouugh subjects to meet our inclusion criteria.
Sample sizes were drawn from the same post-test.
All effect sizes were standardized by the standard deviation of the outcome for the control group at baseline whenever possible (Glass's $\Delta$).
All effect size conversions were conducted by the first author using methods and R code initially developed for previous papers [@paluck2019; @paluck2021; @porat2024] using standard techniques from [@cooper2019], with the exception of a difference in proportion estimator created for [@paluck2021] and explained in our appendix. 

**Meta-analysis:** Our initial set of analyses was pre-registered on the Open Science Framework in November 2023 (\url{<https://osf.io/j5wbp>}), although the project evolved substantially over time. We describe four important deviations in our appendix.
Our analyses use functions and models from the `robumeta` [@fisher2015] and `tidyverse` [@wickham2019] packages in `R` [@Rlang].
Our analysis uses robust variance estimation methods [@hedges2010].

# Discussion {#Sec4}

We offer three lenses through which to view our results.

First, one might focus on the small effect sizes and the moderate evidence of publication bias and conclude that what meager effects we do detect are likely overestimates, and therefore conclude that the true effect being estimated in this dataset is a null.

Second, one might argue that our assembled database of studies *is* successfully changing consumption behavior, but in ranges too small for most studies to detect.
By this light, future studies should replicate existing approaches with sufficient power to detect much smaller effects.

Moreover, a change of a few percentage points might be significant in some contexts.
For example, if a college calculates that meat consumption accounts for 20% of its carbon emissions, a 5.4% reduction in meat consumption [@jalil2023] achieves about a 1% reduction in carbon output.
Whether this is a cost-effective method of reducing carbon output depends on the alternatives.

```{r largest_eff_sizes, include=F}
large_effs <- dat |> filter(d>=0.5) |> 
  select(author, year, d, se_d, 
         n_t_post, n_c_post,  n_t_total_pop, n_c_total_pop)

average_total_large_effs_n <- round(mean(large_effs$n_t_post)) + round(mean(large_effs$n_c_post))

```

Third, one might look at the largest effect sizes in our dataset, for instance, the seven interventions with an effect size of $\Delta \geq 0.5$ [@bianchi2022; @carfora2023; @merrill2009; @piester2020] and seek to replicate and/or expand their approaches.
However, we'd caution that these seven interventions are comparatively small, with an average of `r average_total_large_effs_n` subjects.

We do not have a clear preference between these interpretations.
However, we are generally encouraged by trends in this literature.

First, as previously noted, a majority of studies that meet our inclusion criteria have been published in the past few years, suggesting an overall increase in attention to design and measurement validity.
Second, we applaud researchers in this field for publishing  null results when they find them.
Third, we notice that the universe of possible interventions and settings is much broader than those we analyzed, suggesting that some promising approaches await rigorous evaluation.
e.g. direct contact with animals on an animal sanctuary, price gradations, high-intensity vegan meal planning, door-to-door canvassing, or studies taking place in diverse settings such as retirement homes.

\backmatter

\bmhead{Supplementary information}

All code data, and documentation are available on GitHub (\url{https://github.com/setgree/vegan-meta}) and Code Ocean [LINK]

\bmhead{Acknowledgments}

*Thanks to Alex Berke, Alix Winter, Anson Berns, Hari Dandapani, Adin Richards, Martin Gould, and Matt Lerner for comments on an early draft. Thanks to Jacob Peacock, Andrew Jalil, Gregg Sparkman, Joshua Tasoff, Lucius Caviola, and Emma Garnett for help with assembling the database and providing guidance on their studies. We gratefully acknowledge funding from the NIH (grant XXX) and Open Philanthropy (YYY).*

# Declarations {.unnumbered}

\newpage

# Appendix

## Supplementary Methods

### Converting difference in proportions to standardized mean difference

Conventional methods of converting binary outcomes to estimates of standardized mean difference have some notable downsides, e.g. any given odds ratio is compatible with multiple possible effect sizes depending on the rate of occurrence of the dependent variable [@gomila2021].
We address this by treating all binary variables as draws from a Bernoulli distribution with $p(1 - p)$, where p is the proportion of some event's occurrence.
For example, if 50% of the treatment group ate vegetarian meals vs 45% for the control group, then Glass's $\Delta = \frac{0.05}{\sqrt{0.45 * (1-0.45)}} = 0.1$.

### Four deviations from pre-analysis plan

Our pre-analysis plan registered some general principles and hypotheses for our search process, but did not otherwise do much to constrain how or where we searched.
It also included a synthesized dataset and some mock analyses that resemble our final analyses in general form.
However, as the project evolved over time, we made four substantive changes to our paper that we did not anticipate at the pre-analysis stage.

First, our initial draft combined RPM and MAP studies, taking the former as providing face value estaimtes of the latter.
However, we later decided that RPM reduction was fundamentally a separate estimand, and that without firm data on substitution to other kinds of MAP, we could not say anything definite about the net effect on demand for MAP.

Second, and related, we initially included studies that were not necessarily aimed at achieving overall MAP reduction, but rather a generally healthier diet with some amount of chicken and/or fish, for instance the Mediterranean diet.
Many of these studies had otherwise qualifying measurement strategies.
However, we ultimately concluded that these were a separate estimand as well, and we did not want to add noise to the estimate of studies that were aimed more specifically at reducing MAP consumption rather than causing inter-MAP substitution.

```{r alt_model, include=F}
alt_model <- dat |> map_robust(model = 'RMA')
```

Third, our initial analyses used the random effects model from `metafor` to calculate pooled effect sizes.
However, as we assembled our dataset, we noticed that many papers had, across interventions, non-independent observations, typically in the form of multiple treatments compared to a single control group.
Upon discussion, the team's statistician (MBM) suggested that the `CORR` model from the `robumeta` package would be a better fit.

Using our original model from `metafor`, we detect a pooled effect size of `r alt_model$Delta` (SE = `r alt_model$se`), p = `r alt_model$pval`.
In relative terms, this is substantially smaller, but in absolute terms, both this model and our main model produce very small estimates.
Table S2 provides an overview of alternate estimates by our main theoretical approaches.

Fourth, we added many moderators to our dataset that we did not plan on, such as a broad category for delivery method, whether a study was intended to be emotionally activating, or whether a program had multiple components.
We did not end up focusing on these in our main paper but include them in our dataset in case they are of interest.

## Supplementary Figure

This figure displays the relationship between standard error and effect size.
The colors correspond to theoretical approach and the shapes correspond to the venue where results were published.

```{r supplementary_figure, echo=F, message=F, out.width='120%'}
supplementary_figure
```

## Supplementary Tables

Table S1 displays the category of source where we learned of papers in our main dataset.
\captionsetup[table]{labelformat=empty}

```{r supp_table_one}
supplementary_table_one
```

Table S2 displays the pooled effect size by theoretical approach when using standard random effects estimation methods from the `metafor` package (rather than the robust variance estimation methods we ended up using from the `robumeta` package).

```{r supp_table_two}
supplementary_table_two
```

## Supplementary Discussion

### Edge cases for study inclusion

Arguably the hardest decision in meta-analysis is what studies to include or exclude.
By far the most common reason for exclusion was category of dependent variable (e.g. measuring attitudinal or intentional outcomes).
However, many cases were harder and required some discussion.
Here are a few cases we found difficult.

Some studies limit dietary portions or switch what people are served (e.g. children being served more vegetables at lunchtime).
We did not include these studies because they were essentially guaranteed to have effects that were either positive or at least bounded at zero.
However, we did include studies that provided free access to meat alternatives [@acharya2004; @bianchi2022] and measured outcomes after the intervention had concluded.

Other studies induce a form of treatment in their control groups, for instance asking subjects to take a pledge to go vegetarian or encouraging them not to change their diet over the course of study, which could potentially induce changes relative to baseline consumption patterns.
Our main database only includes studies with a pure control group or an unrelated placebo. However, we include these studies in our supplementary robustness check.

A few studies recruit an already motivated population, e.g. people looking to cut back on their MAP consumption. We consider this a separate estimand and do not include these studies in our main dataset, but we do include them in our supplementary robustness check.

Another common design limitation we encountered was treatment assigned at the level of alternating weeks at a cafeteria.
Generally these studies did not have enough weeks to meet our sample size requirements, but we also note that simply alternating weeks is not random assignment, and it is possible that consumption patterns in these studies will not be equivalent between groups in expectation.

Last, we encountered many studies that measured fruit and or/vegetable consumption but not MAP consumption.
In some cases, it might have been possible to add assumptions about substitution and estimate effect sizes, but we decided to focus on studies that reported this information directly.

## Robustness check: including X additional studies that with near-random assignment

[TO FILL IN]

### Is a norm a nudge?

Tallying how many studies pursue a given theory of change requires drawing boundaries between those theories.
There is some scholarly debate about whether norms intervention qualify as nudges [@bicchieri2023].
The canonical definition of nudge [@thaler2009] is "any aspect of the choice architecture that alters people’s behaviour in a predictable way, without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid" (p. 6), while others [@hausman2010] define nudges as "ways of influencing choice without limiting the choice set or making alternatives appreciably more costly in terms of time, trouble, social sanctions, and so forth" (p. 126).

By this definition, an injunctive norm intervention, which implies a threat of social deviance and therefore sanction, clearly does not qualify.
Whether a descriptive norm can be a nudge is trickier.
Nudges are generally designed to address "flaws in individual decision-making" [@hausman2010, p. 126], while others identify "unthinking conformity" as an example of a "human failing" that nudges take as their "starting point" [@mols2015, p. 4].

Our view is that a social norm prompt might engender a rich array of possible reactions, both cognitive and affective, and we do not assume that "unthinking conformity" is the dominant or exclusive response.
Therefore, we do not classify the norms interventions in our database as nudges.

A future project might investigate exactly what reactions are occurring by asking subjects how well they recall a norms message and what it made them think about.
A high prevalence of subjects who are unable to recall the message's specifics but nevertheless cut back on MAP consumption would be evidence that norms are acting through automatic rather than reflective processes.

### A lack of follow-up data in cafeteria-based interventions

One potential source of measurement error that we cannot easily correct for is the possibility that a subject who is guided towards eating less meat at one meal eats more at the next, i.e. "regression to the meat." Most cafeteria-based studies we looked at were not designed to measure this kind of spillover effect, although such designs are possible [@vocski2024].
We encourage researchers in this field to collect follow-up outcomes.

### Notes on prior reviews
TO FILL IN: a few sentences about the reviews that contributed studies to our search. Then a bit about reviews that we found otherwise noteworthy. Then, a few notes on where our conclusions do or do not converge with prior reviews, e.g.
  - A forthcoming meta-analysis of dynamic norms interventions concludes that their overall effects on MAP consumption are negligible [@Weikertova2024].
  - These disappointing results [animal advocacy] conflict with the central conclusions of [@mathur2021effectiveness], but accord with the finding in [@DiGennaro2024] that animal welfare appeals produce a null effect on average.
  - ([@bianchi2018conscious] also found effects on intentions and attitudes but no evidence of effects on behavior.)


\newpage

# References
