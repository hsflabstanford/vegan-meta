---
classoptions: 
  - sn-nature      
  # - referee       # Optional: Use double line spacing 
  # - lineno        # Optional: Add line numbers
  # - iicol         # Optional: Double column layour
title: "Meaningfully reducing consumption of meat and animal products is an unsolved problem: results from a meta-analysis"
titlerunning: MAP-reduction-meta
authors: 
  - firstname: Seth Ariel
    lastname: Green
    email: setgree@stanford.edu
    affiliation: 1
    corresponding: TRUE
  - firstname: Maya 
    lastname: Mathur
    affiliation: 1
  - firstname: Benny
    lastname: Smith 
    affiliation: 2
affiliations:
  - number: 1
    info:
      orgdiv: Humane and Sustainable Food Lab
      orgname: Stanford University
  - number: 2
    info:
      orgname: Allied Scholars for Animal Protection 
keywords:
  - meta-analysis
  - meat
  - plant-based
  - randomized controlled trial
  
abstract: |
  Which theoretical approach leads to the broadest and most enduring reductions in consumptions of meat and animal products (MAP)? We address these questions with a theoretical review and meta-analysis of especially rigorous Randomized Controlled Trials (RCTs). We meta-analyze 33 papers comprising 39 studies,107 interventions, and approximately 90000 subjects. We find that these papers employ either a nudge, norms, or persuasion approach to changing behavior (some papers combine norms and persuasion). The pooled effect of these interventions on MAP consumption outcomes is $\Delta$ = 0.057, suggesting that we face an unsolved problem. Reducing consumption of red and processed meat is an easier target: $\Delta$ = 0.249, but because of missing data on potential substitution to other MAP, we canâ€™t say anything definitive about the consequences of these interventions on animal welfare. We further explore effect size heterogeneity by approach, population, and study features. We conclude that while no theoretical approach provides a proven remedy to MAP consumption, designs and measurement strategies have generally been improving over time, and many promising interventions await rigorous evaluation.
output: 
  rticles::springer_article:
    keep_tex: true
bibliography: "./vegan-refs.bib"
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{comment}
  - \usepackage{anyfontsize}
  - \usepackage{threeparttable}
---

```{r setup, include=FALSE}
# so that knitr labels figures
knitr::opts_chunk$set(
	fig.path = "../results/",
	echo = FALSE,
	out.extra = ""
)
# so we can put the manuscript stuff in its own folder
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
setwd(rprojroot::find_rstudio_root_file())


source('./scripts/libraries.R')
source('./scripts/functions.R')
source('./scripts/load-data.R')
source('./scripts/red-and-processed-meat.R')
source('./scripts/models.R')
source('./scripts/tables.R')
source('./scripts/figures.R')

```

# Introduction {#sec1}

Consumption of meat and animal products (MAP) is increasingly recognized as a major contributor to premature deaths [@willett2019; @landry2023], public health risks [@slingenbergh2004; @graham2008], ecological harms [@greger2010] and climate change [@scarborough2023; @koneswaran2008] as well as an ethical crisis in its own right [@kuruc2023; @singer2023].

Supply-side interventions, such as banning or taxing certain practices or products, risk political backlash if they do not have broad public support.
It is of vital importance, therefore, to assess which strategies and theoretical perspectives lead to consistent reductions in demand for MAP, under which conditions, and for which populations.
We address these questions with a meta-analysis of the most rigorous studies aimed at reducing MAP consumption.

The research on diet and its antecedents and consequences is vast.
By our count, there have been at least 120 previous published dietary reviews in the past two decades, with at least 37 focused specifically on MAP reduction.
However, comparatively few of these are quantitative, and most prior reviews investigated particular approaches, for example choice architecture [@bianchi2018restructuring] or literacy interventions [@DiGennaro2024], rather than comparing approaches to one another.
Moreover, two prior investigations revealed three common gaps in the MAP literature: a dearth of long-term follow-ups, missing consumption outcomes, and inattention to the gap between intentions and behavior [@mathur2021meta; @mathur2021effectiveness].

Our paper addresses these concerns by meta-analyzing randomized controlled trials (RCTs) that

-   were designed to voluntarily reduce MAP consumption, rather than encouraging substitution from red to white meat or fish or removing meat from someone's plate

-   had least 25 subjects each in treatment and control, or, for cluster-randomized trials, at least 10 clusters in total;

-   measured MAP consumption, whether self-reported or observed directly, rather than (or in addition to) attitudes, intentions, beliefs or hypothetical choices;

-   recorded outcomes at least a single day after the start of treatment.

Additionally, studies needed to be publicly circulated by December 2023 and published in English.

```{r useful_constants}
num_papers <- as.numeric(max(dat$unique_paper_id))
num_studies <- as.numeric(max(dat$unique_study_id))
num_interventions <- as.numeric(nrow(dat))

n_total <-  round_to(x = sum(dat$n_c_total_pop) + sum(dat$n_t_total_pop), 1000, floor)

decade_tab <- dat |> group_by(unique_paper_id) |>  slice(1) |>  ungroup() |> count(decade)
```

We coded `r num_papers` papers [@aldoh2023; @allen2002; @alblas2023; @coker2022; @griesoph2021; @piester2020; @sparkman2017; @sparkman2020; @andersson2021; @kanchanachitra2020; @bochmann2017; @bschaden2020; @cooney2016; @feltz2022; @haile2021; @mathur2021effectiveness; @peacock2017; @polanco2022; @sparkman2021; @abrahamse2007; @acharya2004; @berndsen2005; @bertolaso2015; @bianchi2022; @fehrenbach2015; @hatami2018; @jalil2023; @merrill2009; @norris2014; @weingarten2022; @carfora2023; @hennessy2016; @mattson2020] comprising `r num_studies` separate studies, `r num_interventions` interventions, and approximately `r n_total` subjects.
(Some treatments were administered at the level of day or cafeteria and did not record their number of human subjects.) The earliest paper was published in 2002 [@allen2002], and a majority (`r decade_tab$n[3]` of `r num_papers`) have been published since 2020.

```{r red_meat_numbers, include=F}
RPMC_papers <- as.numeric(max(RPMC$unique_paper_id))
RPMC_studies <- as.numeric(max(RPMC$unique_study_id))
RPMC_interventions <- as.numeric(nrow(RPMC))
n_meat_total <-  plyr::round_any(x = sum(RPMC$n_c_total_pop) + sum(RPMC$n_t_total_pop), 1000, floor)

```

We also coded a supplementary dataset of `r RPMC_papers` papers aimed at reducing, and measuring, consumption of red and/or processed meat (RPM) [@carfora2017correlational; @carfora2017randomised; @carfora2019; @carfora2019informational; @delichatsios2001; @dijkstra2022; @emmons2005cancer; @emmons2005project; @jaacks2014; @james2015; @lee2018; @perino2022; @schatzkin2000; @sorensen2005], comprising `r RPMC_studies` studies, `r RPMC_interventions` interventions, and approximately `r n_meat_total` subjects.
Last, we compiled a third dataset of over 780 excluded studies, along with their reason(s) for exclusion.

# Results {#sec2}

## Three theoretical categories: persuasion, choice architecture, and norms {#sec2.1}

Studies in our database pursued three theories of change: norms, nudges, and persuasion, or a combination of norms and persuasion.
Table 1 reports the distribution of studies, interventions, subjects (approximately), and effect sizes per approach.

`r table_one`

**Norms** studies [@aldoh2023; @allen2002; @alblas2023; @coker2022; @griesoph2021; @piester2020; @sparkman2017; @sparkman2020] manipulate the perceived popularity of desired outcomes, e.g. plant-based dishes [@sparkman2017].
Norms might be descriptive ("33% of British people...successfully engaged in one or more...behaviours to eat less meat" [@aldoh2023]), injunctive (a message with a frowning face for subjects who eat more meat than the average person in their country [@alblas2023]), or dynamic, i.e. they tell subjects that the number of people engaging in desired behavior is increasing [@aldoh2023; @coker2022; @sparkman2017; @sparkman2020].
The first norms study meeting our criteria was published in 2017.

**Nudge** studies [@andersson2021; @kanchanachitra2020] manipulate aspects of physical environments to make non-MAP options more salient, such as placing a vegetarian meal at eye level on a billboard menu [@andersson2021] or making it more laborious for people to serve themselves fish sauc [@kanchanachitra2020].

**Persuasion** studies [@kanchanachitra2020; @abrahamse2007; @acharya2004; @berndsen2005; @bertolaso2015; @bianchi2022; @bochmann2017; @bschaden2020; @carfora2023; @cooney2016; @fehrenbach2015; @feltz2022; @haile2021; @hatami2018; @hennessy2016; @mathur2021effectiveness; @norris2014; @peacock2017; @polanco2022; @sparkman2021; @jalil2023; @merrill2009; @weingarten2022] appeal directly to people to eat less MAP.
These studies formed the majority of our database.
Arguments typically focus on health the environment \textemdash usually climate change \textemdash and animal welfare.
Some are designed to be emotionally activating, e.g. presenting upsetting footage of factory farms [@bertolaso2015], while others present facts about, e.g., the relationship between diet and cancer [@hatami2018].
Many persuasion studies combine arguments, such as a lecture on the health and environmental consequences of eating meat [@jalil2023] or a leaflet with information in all three categories [@hennessy2016].

Table 2 displays the distribution of persuasion studies within these categories.

`r table_two`

Finally, a handful of studies combines **norms and persuasion** approaches [@hennessy2016; @carfora2023; @mattson2020; @piester2020].
These interventions typically suggest reasons to eat less meat side by side with information about changing consumer habits in society.

Three papers [@piester2020; @hennessy2016; @kanchanachitra2020] evaluate multiple interventions reflecting contrasting theoretical approaches.

## An overall small effect {#sec2.2}

Our overall meta-analytic effect size is $\Delta$ = `r model$Delta` (SE = `r model$se`), p = `r model$pval`.
The aggregate effect is statistically significant, but does not indicate a meaningful reduction.

Figure 1 displays the distribution of effect sizes, grouped by paper, with each individual point representing an intervention.
The overall effect size is plotted at the bottom.

```{r forest_plot, out.width='120%'}
forest_plot
```

This small effect may surprise readers of previous reviews, which typically found more positive results [@mathur2021meta; @meier2022; @chang2023].
We attribute this difference to our stricter inclusion criteria.
For instance, of the ten largest effect sizes recorded in [@mathur2021effectiveness], nine were non-consumption outcomes and the tenth came from a non-randomized design.
([@bianchi2018conscious] also found effects on intentions and attitudes but no evidence of effects on behavior.)

```{r nulls_and_cis}
effect_size_table <- dat |> sum_tab(neg_null_pos)

significant_CIs <- dat |> select(author, year, d, se_d, neg_null_pos) |>
  mutate(lower_bound = d - (1.96 * se_d),
         upper_bound = d + (1.96 * se_d)) |>
  filter(lower_bound < 0 & upper_bound < 0 |
           lower_bound > 0 & upper_bound > 0) |> 
  mutate(direction = case_when
         (lower_bound > 0 ~ 1,
           upper_bound < 0 ~ 0))

# pap analysis
pap_model <- dat |> filter(public_pre_analysis_plan != 'N') |> map_robust()

## this difference is not statistically significant
dat <- dat |> mutate(pap_yes = if_else(public_pre_analysis_plan == 'N', FALSE, TRUE))

pap_difference <- robumeta::robu(d ~ pap_yes, dat, unique_study_id, var_d)

# open data analysis
open_data_model <- dat |> filter(open_data != 'N') |> map_robust()

dat <- dat |> mutate(open_data_yes = if_else(open_data == 'N', FALSE, TRUE))

open_data_difference <- robumeta::robu(d ~ open_data_yes, dat, unique_study_id, var_d)
```

According to papers' own analyses, `r effect_size_table['0']` of `r num_interventions` interventions had null effects on net MAP consumption.
However, many studies present a wide variety of outcomes, or include MAP reduction as one of many components of a broader program of behavior change, and focus on their significant results.
Using our calculations of effect size and standard error `r (nrow(significant_CIs))` interventions have 95% confidence intervals that do not overlap with zero, `r sum(significant_CIs$direction)` of which are positive effects, out of `r nrow(dat)` interventions.

## Moderate evidence of publication bias {#sec2.3}

We conduct four tests for publication bias.

```{=tex}
\begin{comment} 
introductory remarks about how this puts our main results in one light or another? 
\end{comment}
```
First, in our dataset, $\Delta$ and standard error are positively correlated, though not significantly.

```{=tex}
\begin{comment} 
good place for a figure?
\end{comment}
```
Second, the `r pap_model$N_studies` studies with a pre-analysis plan have a marginally smaller effect: $\Delta$ = `r pap_model$Delta` (SE = `r pap_model$se`), p = `r pap_model$pval`.
This difference is not statistically significant.

Third, the `r open_data_model$N_studies` studies with openly available data also have a marginally smaller effect: $\Delta$ = `r open_data_model$Delta` (SE = `r open_data_model$se`), p = `r open_data_model$pval`.
This difference is also not statistically significant.

Fourth, the pooled effect size of interventions published in peer-reviewed journals is about 9 times larger than the equivalent effect size in student theses (table 3).
Interventions published by advocacy organizations produce a small backlash effect on average.

`r table_three`

```{r pbulication_bias_checks, include=F}

# "$\Delta$ and standard error are positively correlated, though not significantly"
dat |> sum_lm(d, se_d)
# a la the contact hypothesis re-evaluated, the implication here is that an infinitely powered
# study would have an effect size of 0.0008

# within journal articles? 
dat |> filter(pub_status == 'Journal article') |> sum_lm()
```

## Red and Processed Meat is an easier target {#sec2.4}

On average, interventions aimed at reducing consumption of RPM outperform general MAP reduction interventions: $\Delta$ = `r rpmc_model$Delta` \text{(SE = `r rpmc_model$se`)}, p = `r rpmc_model$pval`.
Each of these studies employs persuasion, and a majority (16/19) appeal to personal health.
However, these studies do not collect data on white meat and/or fish consumption, and therefore their impact on MAP consumption overall is unknown.

Red meat is of special concern for its environmental and health consequences [@grummon2023], but eating chicken is arguably worse for animals on a pound-for-pound basis [@mathur2022ethical].
For some plausible patterns of substitution, these interventions are net positive for health and the environment and net negative for animal welfare.

## Norms work sometimes, but it is not clear why or when {#sec2.5}

```{r include=F, echo=F}
norms_overall_model <- dat |> filter(str_detect(theory, "norms")) |> map_robust()

# look at the distribution of effect sizes

norms_studies <- dat |> filter(str_detect(theory, "norms")) |> select(author, year, theory, d, se_d, intervention_condition, neg_null_pos) |> arrange(neg_null_pos, d) |> print(n = 30)

norms_proportions  <- norms_studies |>
  sum_tab(neg_null_pos)
```

The overall effect for intervention with a norms component is $\Delta$ = `r norms_overall_model$Delta` (SE = `r norms_overall_model$se`), p = `r norms_overall_model$pval`.
Of these `r nrow(norms_studies)` interventions, `r norms_proportions['0']` are self-reported nulls.
Moreover, the spread of norms results is unusually large, with a fair number of backlash results [@mattson2020; @griesoph2021], and one paper with four studies, each featuring real-world settings and objectively measured consumption outcomes, finding one significant positive result, two nulls, and one significant backlash [@sparkman2020].
We do not see, in this collection of studies, a clear limiting principle for when norms interventions achieve their goals.

A forthcoming meta-analysis of dynamic norms interventions concludes that their overall effects on MAP consumption are negligible [@Weikertova2024].

## The evidence for nudges on MAP consumption is scant {#sec2.6}

Although nudges are common in the diet literature writ large [@olafsson2024; @cadario2020; @szaszi2018], only two nudge studies met our inclusion criteria [@andersson2021].
Both had moderate effect sizes on the order of a few percentage points of MAP reduction.

## Health studies work better for RPM than for MAP {#sec2.7}

```{r health_studies, echo=F, include=F}
health_studies <- dat |> 
  filter(str_detect(secondary_theory, 'health')) |> 
  select(author, year, secondary_theory, d, se_d, 
         self_report, neg_null_pos) |> 
  arrange(neg_null_pos, d) |> print(n = 30)

health_proportions <- 
  health_studies |> sum_tab(neg_null_pos)

RPM_health_model <- RPMC |>  
  filter(str_detect(secondary_theory, 'health')) |> map_robust()

# health studies form the majority of RPM studies
RPM_health_numbers <- RPMC |> mutate(health_component = if_else(str_detect(secondary_theory, 'health'), T,F)) |> 
  sum_tab(health_component)
```

The pooled effect size for persuasion studies with a health component is $\Delta$ = `r health_model$Delta` (SE = `r health_model$se`), p = `r health_model$pval`.
This is small and not significant, albeit larger than the overall pooled effect.
Health appeals are a component of `r RPM_health_numbers['TRUE']` of `r nrow(RPMC)` interventions aimed at reducing RPM consumption, and are generally more effective there: $\Delta$ = `r RPM_health_model$Delta` (SE = `r RPM_health_model$se`), p = `r RPM_health_model$pval`.
This fits with the broader context where official nutritional guidelines typically encourage consumers to reduce RPM and consume moderate amounts of lean meat and fish.

We also observe that many health studies either seek to induce a sense of fear in subjects [@berndsen2005] or target people who are at risk of cancer [@hatami2018] or cancer survivors [@james2015; @lee2018] with health-based reasons to change their diets, and then ask them to self-report what they have eaten recently.
We judge self-reporting bias to be a potential concern.

## Environmental appeals have modest positive effects {#sec2.8}

```{r environment_studies, include=F}
# exploratory
# do encvironmental studies work better for certain groups?
# my intuition would be they work best for college students, & Jalil study is nice evidence of that,
# but the data don't reflect that
dat |> filter(str_detect(secondary_theory, 'environment')) |> 
  split(~population) |> map(map_robust)

dat |> filter(str_detect(secondary_theory, 'animal')) |>  
  mutate(ad_yes_no = if_else(advocacy_org == 'N', FALSE, TRUE)) |> 
  split(~ad_yes_no) |> map(map_robust)

jalil_data <- dat |> filter(author == 'Jalil')
```

The pooled effect size for persuasion studies with an environmental component is $\Delta$ = `r environment_model$Delta` (SE = `r environment_model$se`), p = `r environment_model$pval`.
The strongest evidence that these appeals produce real-world impacts is [@jalil2023], which substituted an introductory lecture in a first-year economics class for a lecture on the environment and health consequences of meat, focusing mostly on the environment, and then tracked student meal choices in dining halls for three years following.
That study found that treatment led to an overall reduction in MAP consumption of 5.6% $\Delta$ = `r round(jalil_data$d, 3)` (SE = `r jalil_data$se_d`), which is neither especially large nor statistically significant.
However, due to its exceptional commitment to long-term, oblique outcome measurement, we consider this study to be reasonably robust evidence for this intervention's efficacy among students at liberal arts colleges.

## Animal welfare appeals are almost always ineffective {#sec2.9}

```{r animal_welfare_studies, include=F}
animal_proportions  <- dat |> filter(str_detect(secondary_theory, 'animal')) |>
  sum_tab(neg_null_pos)

animal_proportions['0']

neg_animal_results <- dat |> filter(str_detect(secondary_theory, 'animal')) |> filter(d<0) |> count()

advocacy_model <- dat |> filter(advocacy_org != 'N') |> map_robust()
```

The pooled effect size for persuasion studies with an animal welfare component is $\Delta$ = `r animal_model$Delta` (SE = `r animal_model$se`), p = `r animal_model$pval`.
A full `r animal_proportions['0']` of `r animal_model$N_interventions` interventions in this category are self-described nulls.
Slightly more than half (`r neg_animal_results` of `r animal_model$N_interventions`) lead to increases in MAP consumption, though just one of these effects is statistically significant.

The `r advocacy_model$N_interventions` interventions and `r advocacy_model$N_studies` studies using materials from advocacy organizations find an overall effect of `r advocacy_model$Delta` (SE = `r advocacy_model$se`), p = `r advocacy_model$pval`.

These disappointing results conflict with the central conclusions of [@mathur2021effectiveness], but accord with the finding in [@DiGennaro2024] that animal welfare appeals produce a null effect on average.

## Heterogeneity by reporting, cluster assignment, delivery method, and country {#sec2.10}

```{r confounding_check, include=F}
confound_table <- dat |> filter(self_report == 'N') |> 
  group_by(str_detect(population, 'university')) |> 
  summarise(count = n()) |> as_tibble()

```

Our sample of studies is comparatively small, and many differences between studies are confounded (for example, `r confound_table$count[[2]]` of `r confound_table$count[[2]] + confound_table$count[[1]]` interventions with objectively reported outcomes are also studies of university populations).
Nevertheless we offer a few tentative explorations of potential moderators of effect size.

```{r heterogeneity_analyses, include=F}

self_report_table <- dat |> split(~self_report) |> map(map_robust)  |> 
 bind_rows(.id = 'self_report')

cluster_results <- dat |> split(~cluster_assigned) |> map(map_robust)
delivery_table <- table(dat$delivery_method)
```

Contrary to our expectations, self-reported and objectively collected outcomes were not meaningfully different: `r self_report_table[self_report_table$self_report == "N", "Delta"]` for objectively reported vs `r self_report_table[self_report_table$self_report == "Y", "Delta"]` for self-reported.
Likewise, the difference in effect sizes for studies where treatment was assigned to clusters (e.g. cafeteria or day of treatment) vs. individuals is small.

Table 4 displays the effect size associated with the four most common delivery mechanisms in our dataset, which were also the groups with enough clusters for meta-analysis to be viable.

`r table_four`

Table 5 displays effects associated with different regions.

`r table_five`

# Methods {#sec3}

**Search**: We employed a multi-pronged search strategy for assembling our database.
First, we checked the bibliographies of three recent reviews [@mathur2021meta; @bianchi2018conscious; @bianchi2018restructuring] for relevant studies.
Second, we checked any possibly relevant study that either cited or was cited by studies we coded.
Third, we checked the bibliographies of authors whose studies we coded.
Fourth, we contacted leading researchers in the field with our in-progress database to see if we had missed any.
Fifth, we rinsed and repeated with the new studies we had.
Sixth, we conducted searched Google Scholar for certain terms that had come up in studies repeatedly (e.g. "dynamic norms + meat", "MAP reduction", and "plant-based diet + effective").
Sixth, we checked database emerging from a parallel project being conducted by Rethink Priorities.
Seventh, we identified a further 100+ systematic reviews and checked their bibliographies.
Eighth, we used an AI search tool (\url{<https://undermind.ai>}) to check for unfound gray literature.
All three authors contributed to the search.

**Coding:** For quantitative outcomes, we selected the latest possible outcome that still had sufficient subjects to meet our inclusion criteria.
Sample sizes were drawn from the same post-test.
All effect sizes were standardized by the standard deviation of the outcome for the control group at baseline whenever possible (Glass's $\Delta$).
All effect size conversions were conducted by the first author using methods and R code initially developed for previous papers [@paluck2019; @paluck2021; @porat2024] using standard techniques from [@cooper2019], with the exception of a difference in proportion estimator created for [@paluck2021] and detailed in the appendix of [@porat2024].

**Meta-analysis:** Our initial set of analyses was pre-registered on the Open Science Framework in November 2023 (\url{<https://osf.io/j5wbp>}), although the project evolved substantially over time and our final analyses do not match those initial plans.
Our analyses use functions and models from the `robumeta` [@fisher2015] and `tidyverse` [@wickham2019] packages in `R` [@Rlang].

## Discussion

We offer three lenses through which to view our results.

First, one might focus on the small effect sizes and the moderate evidence of publication bias and conclude that what meager effects we do detect are likely overestimates, and therefore conclude that the true effect being estimated in this dataset is a null.

Second, one might argue that our assembled database of studies *is* successfully changing consumption behavior, but in ranges too small for most studies to detect.
By this light, future studies should replicate existing approaches with sufficient power to detect much smaller effects.

Moreover, a change of a few percentage points might be significant in some contexts.
For example, if a college calculates that meat consumption accounts for 20% of its carbon emissions, a 5.4% reduction in meat consumption [@jalil2023] achieves a 1% reduction in carbon output.
Whether or not that is a cost-effective method depends on the alternatives.

```{r largest_eff_sizes, include=F}
large_effs <- dat |> filter(d>=0.5) |> 
  select(author, year, d, se_d, 
         n_t_post, n_c_post,  n_t_total_pop, n_c_total_pop)

average_total_large_effs_n <- round(mean(large_effs$n_t_post)) + round(mean(large_effs$n_c_post))

```

Third, one might look at the largest effect sizes in our dataset, for instance, the seven interventions with an effect size of $\Delta \geq 0.5$ [@bianchi2022; @carfora2023; @merrill2009; @piester2020] and seek to replicate and/or expand their approaches.
However, we'd caution that these seven studies are generally small-n, with an average of `r average_total_large_effs_n` subjects.

We do not have a clear preference between these interpretations.
However, we are generally encouraged by trends in this literature.

First, as previously noted, a majority of studies that meet our inclusion criteria have been published in the past few years, suggesting an overall increase in attention to design and measurement validity.
Second, we applaud researchers in this field for publicizing null results when they find them.
Third, we notice that the universe of possible interventions and settings is much broader than those we analyzed, suggesting that some promising approaches await rigorous evaluation.
e.g. direct contact with animals on an animal sanctuary, price gradations, high-intensity vegan meal planning, door-to-door canvassing, or studies taking place in diverse settings such as retirement homes.

\backmatter

\bmhead{Supplementary information}

All code and data are available on GitHub and Code Ocean: LINKS

\bmhead{Acknowledgments}

*Thanks to Alex Berke, Alix Winter, Anson Berns, Hari Dandapani, Adin Richards, Martin Gould, and Matt Lerner for comments on an early draft. Thanks to Jacob Peacock, Andrew Jalil, Gregg Sparkman, Joshua Tasoff, Lucius Caviola, and Emma Garnett for help with assembling the database and providing guidance on their studies. We gratefully acknowledge funding from the NIH (grant XXX) and Open Philanthropy (YYY).*

# Declarations {.unnumbered}

\newpage

# Appendix

## Supplementary Methods

-   Question for Maya â€“ what else should go here?

    -   e.g. a discussion meta-analytic models and the fact that our results are a bit sensitive to differences

-   difference in proportions calculator briefly

-   deviations from pre-analysis plan

## Supplementary Figure

This figure displays...

```{r supplementary_figure, echo=F, message=F, out.width='120%'}
supplementary_figure
```

## Supplementary Discussion

### Edge cases for study inclusion

-   discuss some studies that we did include but weren't sure about

-   some studies that we didn't include and why

    -    maybe one illustrative example for each inclusion criteria

    -    & some unexpected coding challenges

### Is a norm a nudge?

Tallying which studies pursue which theories of change requires drawing boundaries between those theories.
One potentially tricky boundary is between nudges and norms.
[@thaler2009] define a nudge as "any aspect of the choice architecture that alters peopleâ€™s behaviour in a predictable way, without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid" (p. 6), while [@hausman2010] define nudges as "ways of influencing choice without limiting the choice set or making alternatives appreciably more costly in terms of time, trouble, social sanctions, and so forth" (p. 126).

By this definition, an injunctive norm intervention, which implies a threat of social deviance and therefore sanction, clearly does not qualify.
Whether a descriptive norm can be a nudge is trickier.
[@thaler2009] would likely say yes; they write that if "choice architects want to change behavior, and to do so with a nudge, they might simply inform people about what other people are doing" (p. 71).
However, [@hausman2010] also note that nudges are designed to address "flaws in individual decision-making" (p. 126), which captures the flavor of most of the nudge studies we looked at.
Is a tendency to conform or to preconform [@sparkman2017] a flaw or "behavioral bias" [@kantorowicz2021, p. 362]?

[@mols2015] thread this needle by defining "unthinking conformity" as one of the "human failings" which nudges are intended to address.
They contrast this approach with "persuasion," which "appeals to an individualâ€™s ability to reason, digest new information and engage in systematic information processing" (p. 4).

Our view is that a social norm prompt might engender a rich array of possible reactions, both cognitive and affective, and we do not assume that "unthinking conformity" is the dominant or exclusive response.
Therefore, we do not classify the norms interventions in our database as nudges, though we acknowledge a burgeoning "norms-nudge" literature and that terminology may change over time.
(See [@bicchieri2023] for discussion).

We also think that a future project might investigate exactly what reactions are occurring by asking subjects, e.g., how well they recall the message, what it made them think about, etc.
A high prevalence of subjects' being unable to recall the message's specifics, for instance, but nevertheless showing signs of behavioral change would be evidence that norms are acting through automatic rather than reflective processes.

\newpage

# References

# 
